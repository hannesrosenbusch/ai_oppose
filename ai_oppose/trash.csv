,Author,Title,Publication Year,Abstract Note,SOURCE,Abstract Note Length,file_content,reference
0,Brown,The GRIM test: A simple technique detects numerous anomalies in the reporting of results in psychology,2017,"We present a simple mathematical technique that we call granularity-related inconsistency of means (GRIM) for verifying the summary statistics of research reports in psychology. This technique evaluates whether the reported means of integer data such as Likert-type scales are consistent with the given sample size and number of items. We tested this technique with a sample of 260 recent empirical articles in leading journals. Of the articles that we could test with the GRIM technique (N = 71), around half (N = 36) appeared to contain at least one inconsistent mean, and more than 20% (N = 16) contained multiple such inconsistencies. We requested the data sets corresponding to 21 of these articles, receiving positive responses in 9 cases. We confirmed the presence of at least one reporting error in all cases, with three articles requiring extensive corrections. The implications for the reliability and replicability of empirical psychology are discussed. ",data/metaanalysis.pdf --> https://www.researchgate.net/publication/309275131_The_GRIM_Test_A_Simple_Technique_Detects_Numerous_Anomalies_in_the_Reporting_of_Results_in_Psychology,0,"TITLE: 'The GRIM test: A simple technique detects numerous anomalies in the reporting of results in psychology' ABSTRACT: 'We present a simple mathematical technique that we call granularity-related inconsistency of means (GRIM) for verifying the summary statistics of research reports in psychology. This technique evaluates whether the reported means of integer data such as Likert-type scales are consistent with the given sample size and number of items. We tested this technique with a sample of 260 recent empirical articles in leading journals. Of the articles that we could test with the GRIM technique (N = 71), around half (N = 36) appeared to contain at least one inconsistent mean, and more than 20% (N = 16) contained multiple such inconsistencies. We requested the data sets corresponding to 21 of these articles, receiving positive responses in 9 cases. We confirmed the presence of at least one reporting error in all cases, with three articles requiring extensive corrections. The implications for the reliability and replicability of empirical psychology are discussed. ' SOURCE: (Brown, 2017)","Brown, 2017"
1,kossmeier tran voracek,Power-enhanced funnel plots for meta-analysis: The sunset funnel plot,2020,"Currently, dedicated graphical displays to depict study-level statistical power in the context of meta-analysis are unavailable. Here, we introduce the sunset (power-enhanced) funnel plot to visualize this relevant information for assessing the credibility, or evidential value, of a set of studies. The sunset funnel plot highlights the statistical power of primary studies to detect an underlying true effect of interest in the well-known funnel display with color-coded power regions and a second power axis. This graphical display allows meta-analysts to incorporate power considerations into classic funnel plot assessments of small-study effects. Nominally significant, but low-powered, studies might be seen as less credible and as more likely being affected by selective reporting. We exemplify the application of the sunset funnel plot with two published meta-analyses from medicine and psychology. Software to create this variation of the funnel plot is provided via a tailored R function. In conclusion, the sunset (power-enhanced) funnel plot is a novel and useful graphical display to critically examine and to present study-level power in the context of meta-analysis.",data/metaanalysis.pdf --> https://www.psycharchives.org/handle/20.500.12034/2098,0,"TITLE: 'Power-enhanced funnel plots for meta-analysis: The sunset funnel plot' ABSTRACT: 'Currently, dedicated graphical displays to depict study-level statistical power in the context of meta-analysis are unavailable. Here, we introduce the sunset (power-enhanced) funnel plot to visualize this relevant information for assessing the credibility, or evidential value, of a set of studies. The sunset funnel plot highlights the statistical power of primary studies to detect an underlying true effect of interest in the well-known funnel display with color-coded power regions and a second power axis. This graphical display allows meta-analysts to incorporate power considerations into classic funnel plot assessments of small-study effects. Nominally significant, but low-powered, studies might be seen as less credible and as more likely being affected by selective reporting. We exemplify the application of the sunset funnel plot with two published meta-analyses from medicine and psychology. Software to create this variation of the funnel plot is provided via a tailored R function. In conclusion, the sunset (power-enhanced) funnel plot is a novel and useful graphical display to critically examine and to present study-level power in the context of meta-analysis.' SOURCE: (kossmeier tran voracek, 2020)","kossmeier tran voracek, 2020"
2,Borenstien Hedges Higgins,Introduction to meta-analysis,2021,"As a means to synthesize the results of multiple studies, the chronological development of the meta-analysis method was in parallel to a variety of definitions in the literature. Meta-analysis can be defined in different ways: as a means of summarizing and combining the quantitative results of research or as a method used to reach the quantitative effect size based on individual studies. Meta-analysis uses many quantitative approaches and calculation formulas when compiling multiple research findings. In this sense, no researcher needs to be an expert in all types and calculation formulas for all types of meta-analysis. However, if the researcher lacks familiarity with at least some of the main concepts of meta-analysis, then the correct results may not be obtained. This chapter aims to explain some of the main concepts of meta-analysis.",data/metaanalysis.pdf --> https://onlinelibrary.wiley.com/doi/book/10.1002/9780470743386,0,"TITLE: 'Introduction to meta-analysis' ABSTRACT: 'As a means to synthesize the results of multiple studies, the chronological development of the meta-analysis method was in parallel to a variety of definitions in the literature. Meta-analysis can be defined in different ways: as a means of summarizing and combining the quantitative results of research or as a method used to reach the quantitative effect size based on individual studies. Meta-analysis uses many quantitative approaches and calculation formulas when compiling multiple research findings. In this sense, no researcher needs to be an expert in all types and calculation formulas for all types of meta-analysis. However, if the researcher lacks familiarity with at least some of the main concepts of meta-analysis, then the correct results may not be obtained. This chapter aims to explain some of the main concepts of meta-analysis.' SOURCE: (Borenstien Hedges Higgins, 2021)","Borenstien Hedges Higgins, 2021"
3,ghai,It’s time to reimagine sample diversity and retire the WEIRD dichotomy,2021,"The world’s population does not split neatly into two groups, WEIRD and non-WEIRD people, argues Sakshi Ghai. Because the non-WEIRD brush does not do justice to the complexity of human lives, she calls upon behavioural science to ensure that samples represent human diversity.",data/metaanalysis.pdf --> https://pubmed.ncbi.nlm.nih.gov/34285391/,0,"TITLE: 'It's time to reimagine sample diversity and retire the WEIRD dichotomy' ABSTRACT: 'The world's population does not split neatly into two groups, WEIRD and non-WEIRD people, argues Sakshi Ghai. Because the non-WEIRD brush does not do justice to the complexity of human lives, she calls upon behavioural science to ensure that samples represent human diversity.' SOURCE: (ghai, 2021)","ghai, 2021"
4,Nordahl Hansen Cogo Moreira Panjeh,Redefining effect size interpretations for psychotherapy RCTs in depression,2022,"importance: Effect sizes are used to interpret the size of effects and in power calculations when planning research studies. However, as effect sizes are context-dependent, the rule of thumb proposed by Jacob Cohen might vary across different areas of research, nature of the intervention, and population. Objectives: To determine small, medium, and large effect sizes within the psychotherapy randomized controlled trials (RCTs) in depression by calculating the effect size distribution.Design, Setting, and Participants: Effect sizes of 366 RCTs provided by the systematic review of Cuijpers and colleagues (2020) on psychotherapy for depressive disorders across all age-groups.Main Outcomes and Measures: The 50th percentile effect size, as this represents a medium effect size, and the 25th (small) and 75th (large) percentile effect sizes were calculated.Results: After adjusting for publication bias, 0.27, 0.53, and 0.86 represent small, medium, and large effect sizes, respectively, in psychotherapy for depressive disorders. Conclusions and Relevance: Applying Cohen’s suggested effect size thresholds (0.2, 0.5, and 0.8) underestimate effect sizes when compared to the real-world context of psychotherapy interventions for depression. These results have implications for the interpretation of study effects and the planning of future studies via power analyses, which often use effect size thresholds.",data/metaanalysis.pdf --> https://osf.io/erhmw/download,0,"TITLE: 'Redefining effect size interpretations for psychotherapy RCTs in depression' ABSTRACT: 'importance: Effect sizes are used to interpret the size of effects and in power calculations when planning research studies. However, as effect sizes are context-dependent, the rule of thumb proposed by Jacob Cohen might vary across different areas of research, nature of the intervention, and population. Objectives: To determine small, medium, and large effect sizes within the psychotherapy randomized controlled trials (RCTs) in depression by calculating the effect size distribution.Design, Setting, and Participants: Effect sizes of 366 RCTs provided by the systematic review of Cuijpers and colleagues (2020) on psychotherapy for depressive disorders across all age-groups.Main Outcomes and Measures: The 50th percentile effect size, as this represents a medium effect size, and the 25th (small) and 75th (large) percentile effect sizes were calculated.Results: After adjusting for publication bias, 0.27, 0.53, and 0.86 represent small, medium, and large effect sizes, respectively, in psychotherapy for depressive disorders. Conclusions and Relevance: Applying Cohen's suggested effect size thresholds (0.2, 0.5, and 0.8) underestimate effect sizes when compared to the real-world context of psychotherapy interventions for depression. These results have implications for the interpretation of study effects and the planning of future studies via power analyses, which often use effect size thresholds.' SOURCE: (Nordahl Hansen Cogo Moreira Panjeh, 2022)","Nordahl Hansen Cogo Moreira Panjeh, 2022"
5,quintana,Replication studies for undergraduate theses to improve science and education,2021,"Requiring undergraduate students to perform what is termed original research for their thesis, an investigation that cannot constitute a replication of an existing study, is a failed opportunity for science and education, argues Daniel Quintana. An undergraduate thesis, sometimes referred to as a bachelor or honours thesis, is a common component of psychology degrees. Its goal is ostensibly to provide students with an opportunity to directly experience the research process and to assess their ability to perform and report scientific research. These research projects can also contribute to the scientific literature. However, due to an understandable lack of experience, oversight and resources, only a small number of these research projects are deemed of sufficient breadth and quality to be considered for scientific publication. At present, they represent a substantial lost opportunity for mass data collection and kick-starting research careers. This situation will not improve unless institutions change their narrow requirements for what qualifies as an undergraduate research project—the status quo is the design, execution, analysis and report of a novel research project. One alternative, among others, is to make cumulative replication studies available for student participation. This would provide a valuable contribution to the literature by conducting much-needed replication studies that might not otherwise be completed. Moreover, the replication study framework can teach undergraduate students important skills for conducting transparent and reproducible science, including pre-registration and data sharing. In a large-scale cumulative student replication project, a steering committee carefully selects research findings previously reported in the literature to be replicated by multiple groups. In the psychological sciences, effect sizes are generally too small to be reliably detected by the sample sizes that are available for undergraduate thesis projects, whereas a meta-analytic synthesis of the results from multiple replication studies can achieve the statistical power required to detect or reject small effect sizes. Consequently, undergraduate replication projects can meaningfully contribute to the research literature despite limited resources and can begin a student’s formal research career.",data/metaanalysis.pdf --> https://pubmed.ncbi.nlm.nih.gov/34493847/,0,"TITLE: 'Replication studies for undergraduate theses to improve science and education' ABSTRACT: 'Requiring undergraduate students to perform what is termed original research for their thesis, an investigation that cannot constitute a replication of an existing study, is a failed opportunity for science and education, argues Daniel Quintana. An undergraduate thesis, sometimes referred to as a bachelor or honours thesis, is a common component of psychology degrees. Its goal is ostensibly to provide students with an opportunity to directly experience the research process and to assess their ability to perform and report scientific research. These research projects can also contribute to the scientific literature. However, due to an understandable lack of experience, oversight and resources, only a small number of these research projects are deemed of sufficient breadth and quality to be considered for scientific publication. At present, they represent a substantial lost opportunity for mass data collection and kick-starting research careers. This situation will not improve unless institutions change their narrow requirements for what qualifies as an undergraduate research project--the status quo is the design, execution, analysis and report of a novel research project. One alternative, among others, is to make cumulative replication studies available for student participation. This would provide a valuable contribution to the literature by conducting much-needed replication studies that might not otherwise be completed. Moreover, the replication study framework can teach undergraduate students important skills for conducting transparent and reproducible science, including pre-registration and data sharing. In a large-scale cumulative student replication project, a steering committee carefully selects research findings previously reported in the literature to be replicated by multiple groups. In the psychological sciences, effect sizes are generally too small to be reliably detected by the sample sizes that are available for undergraduate thesis projects, whereas a meta-analytic synthesis of the results from multiple replication studies can achieve the statistical power required to detect or reject small effect sizes. Consequently, undergraduate replication projects can meaningfully contribute to the research literature despite limited resources and can begin a student's formal research career.' SOURCE: (quintana, 2021)","quintana, 2021"
6,fanelli,Negative results are disappearing from most disciplines and countries,2011,"Concerns that the growing competition for funding and citations might distort science are frequently discussed, but have not been verified directly. Of the hypothesized problems, perhaps the most worrying is a worsening of positive-outcome bias. A system that disfavours negative results not only distorts the scientific literature directly, but might also discourage high-risk projects and pressure scientists to fabricate and falsify their data. This study analysed over 4,600 papers published in all disciplines between 1990 and 2007, measuring the frequency of papers that, having declared to have “tested” a hypothesis, reported a positive support for it. The overall frequency of positive supports has grown by over 22% between 1990 and 2007, with significant differences between disciplines and countries. The increase was stronger in the social and some biomedical disciplines. The United States had published, over the years, significantly fewer positive results than Asian countries (and particularly Japan) but more than European countries (and in particular the United Kingdom). Methodological artefacts cannot explain away these patterns, which support the hypotheses that research is becoming less pioneering and/or that the objectivity with which results are produced and published is decreasing.",10.1007/s11192-011-0494-7 --> https://dl.acm.org/doi/10.1007/s11192-011-0494-7,0,"TITLE: 'Negative results are disappearing from most disciplines and countries' ABSTRACT: 'Concerns that the growing competition for funding and citations might distort science are frequently discussed, but have not been verified directly. Of the hypothesized problems, perhaps the most worrying is a worsening of positive-outcome bias. A system that disfavours negative results not only distorts the scientific literature directly, but might also discourage high-risk projects and pressure scientists to fabricate and falsify their data. This study analysed over 4,600 papers published in all disciplines between 1990 and 2007, measuring the frequency of papers that, having declared to have ""tested"" a hypothesis, reported a positive support for it. The overall frequency of positive supports has grown by over 22% between 1990 and 2007, with significant differences between disciplines and countries. The increase was stronger in the social and some biomedical disciplines. The United States had published, over the years, significantly fewer positive results than Asian countries (and particularly Japan) but more than European countries (and in particular the United Kingdom). Methodological artefacts cannot explain away these patterns, which support the hypotheses that research is becoming less pioneering and/or that the objectivity with which results are produced and published is decreasing.' SOURCE: (fanelli, 2011)","fanelli, 2011"
7,Cohen,Statistical power analysis for the behavioural sciences,1988,"The power of a statistical test is the probability that it will yield statis- tically significant results. Since statistical significance is so earnestly sought and devoutly wished for by behavioral scientists, one would think that the a priori probability of its accomplishment would be routinely determined and well understood. Quite surprisingly, this is not the case. Instead, if we take as evidence the research literature, we find evidence that statistical power is frequenty not understood and, in reports of research where it is clearly rele- vant, the issue is not addressed. The purpose of this book is to provide a self-contained comprehensive treatment of statistical power analysis from an ""applied"" viewpoint. The purpose of this chapter is to present the basic conceptual framework of statistical hypothesis testing, giving emphasis to power, followed by the frame- work within which this book is organized",data/metaanalysis.pdf --> https://www.utstat.toronto.edu/~brunner/oldclass/378f16/readings/CohenPower.pdf,0,"TITLE: 'Statistical power analysis for the behavioural sciences' ABSTRACT: 'The power of a statistical test is the probability that it will yield statis- tically significant results. Since statistical significance is so earnestly sought and devoutly wished for by behavioral scientists, one would think that the a priori probability of its accomplishment would be routinely determined and well understood. Quite surprisingly, this is not the case. Instead, if we take as evidence the research literature, we find evidence that statistical power is frequenty not understood and, in reports of research where it is clearly rele- vant, the issue is not addressed. The purpose of this book is to provide a self-contained comprehensive treatment of statistical power analysis from an ""applied"" viewpoint. The purpose of this chapter is to present the basic conceptual framework of statistical hypothesis testing, giving emphasis to power, followed by the frame- work within which this book is organized' SOURCE: (Cohen, 1988)","Cohen, 1988"
9,Harrer Cuijpers Furukawa,Doing meta-analysis in R: A hands-on guide,2021,"M. Harrer, P. Cuijpers, T. A. Furukawa, D. D. Ebert 2021 Doing meta-analysis in R: A hands-on guide",data/metaanalysis.pdf --> https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/,0,"TITLE: 'Doing meta-analysis in R: A hands-on guide' ABSTRACT: 'M. Harrer, P. Cuijpers, T. A. Furukawa, D. D. Ebert 2021 Doing meta-analysis in R: A hands-on guide' SOURCE: (Harrer Cuijpers Furukawa, 2021)","Harrer Cuijpers Furukawa, 2021"
10,borenstein hedges higgins rothstein,Introduction to Meta‐Analysis,2009,"This book provides a clear and thorough introduction to meta-analysis, the process of synthesizing data from a series of separate studies. Meta-analysis has become a critically important tool in fields as diverse as medicine, pharmacology, epidemiology, education, psychology, business, and ecology. Introduction to Meta-Analysis: Outlines the role of meta-analysis in the research process Shows how to compute effects sizes and treatment effects Explains the fixed-effect and random-effects models for synthesizing data Demonstrates how to assess and interpret variation in effect size across studies Clarifies concepts using text and figures, followed by formulas and examples Explains how to avoid common mistakes in meta-analysis Discusses controversies in meta-analysis Features a web site with additional material and exercises",data/metaanalysis.pdf --> https://onlinelibrary.wiley.com/doi/book/10.1002/9780470743386,0,"TITLE: 'Introduction to Meta-Analysis' ABSTRACT: 'This book provides a clear and thorough introduction to meta-analysis, the process of synthesizing data from a series of separate studies. Meta-analysis has become a critically important tool in fields as diverse as medicine, pharmacology, epidemiology, education, psychology, business, and ecology. Introduction to Meta-Analysis: Outlines the role of meta-analysis in the research process Shows how to compute effects sizes and treatment effects Explains the fixed-effect and random-effects models for synthesizing data Demonstrates how to assess and interpret variation in effect size across studies Clarifies concepts using text and figures, followed by formulas and examples Explains how to avoid common mistakes in meta-analysis Discusses controversies in meta-analysis Features a web site with additional material and exercises' SOURCE: (borenstein hedges higgins rothstein, 2009)","borenstein hedges higgins rothstein, 2009"
11,valentine pigott rothstein,How many studies do you need? A primer on statistical power for meta-analysis,2011,"In this article, the authors outline methods for using fixed and random effects power analysis in the context of meta-analysis. Like statistical power analysis for primary studies, power analysis for meta-analysis can be done either prospectively or retrospectively and requires assumptions about parameters that are unknown. The authors provide some suggestions for thinking about these parameters, in particular for the random effects variance component. The authors also show how the typically uninformative retrospective power analysis can be made more informative. The authors then discuss the value of confidence intervals, show how they could be used in addition to or instead of retrospective power analysis, and also demonstrate that confidence intervals can convey information more effectively in some situations than power analyses alone. Finally, the authors take up the question “How many studies do you need to do a meta-analysis?” and show that, given the need for a conclusion, the answer is “two studies,” because all other synthesis techniques are less transparent and/or are less likely to be valid. For systematic reviewers who choose not to conduct a quantitative synthesis, the authors provide suggestions for both highlighting the current limitations in the research base and for displaying the characteristics and results of studies that were found to meet inclusion criteria.",data/metaanalysis.pdf --> https://journals.sagepub.com/doi/10.3102/1076998609346961,0,"TITLE: 'How many studies do you need? A primer on statistical power for meta-analysis' ABSTRACT: 'In this article, the authors outline methods for using fixed and random effects power analysis in the context of meta-analysis. Like statistical power analysis for primary studies, power analysis for meta-analysis can be done either prospectively or retrospectively and requires assumptions about parameters that are unknown. The authors provide some suggestions for thinking about these parameters, in particular for the random effects variance component. The authors also show how the typically uninformative retrospective power analysis can be made more informative. The authors then discuss the value of confidence intervals, show how they could be used in addition to or instead of retrospective power analysis, and also demonstrate that confidence intervals can convey information more effectively in some situations than power analyses alone. Finally, the authors take up the question ""How many studies do you need to do a meta-analysis?"" and show that, given the need for a conclusion, the answer is ""two studies,"" because all other synthesis techniques are less transparent and/or are less likely to be valid. For systematic reviewers who choose not to conduct a quantitative synthesis, the authors provide suggestions for both highlighting the current limitations in the research base and for displaying the characteristics and results of studies that were found to meet inclusion criteria.' SOURCE: (valentine pigott rothstein, 2011)","valentine pigott rothstein, 2011"
12,alexander scozzaro borodkin,Statistical and empirical examination of the chi-square test for homogeneity of correlations in meta-analysis.,1989,"Les auteurs font une analyse statistique de l'utilisation du test du Chi-Square Test (Chi-2), test qui mesure la grandeur des effets lors d'etudes experimentales en psychologie",data/metaanalysis.pdf,176,"TITLE: 'Statistical and empirical examination of the chi-square test for homogeneity of correlations in meta-analysis.' ABSTRACT: 'Les auteurs font une analyse statistique de l'utilisation du test du Chi-Square Test (Chi-2), test qui mesure la grandeur des effets lors d'etudes experimentales en psychologie' SOURCE: (alexander scozzaro borodkin, 1989)","alexander scozzaro borodkin, 1989"
13,wagge brandt lazarević legate christopherson wiggins grahe,Publishing Research With Undergraduate Students via Replication Work: The Collaborative Replications and Education Project,2018,"We discuss the Collaborative Replications and Education Project (CREP), the background and purpose of the CREP, and how students participate in the project. We also discuss how CREP facilitates and promotes dissemination of student work and the benefits of CREP.",data/metaanalysis.pdf,262,"TITLE: 'Publishing Research With Undergraduate Students via Replication Work: The Collaborative Replications and Education Project' ABSTRACT: 'We discuss the Collaborative Replications and Education Project (CREP), the background and purpose of the CREP, and how students participate in the project. We also discuss how CREP facilitates and promotes dissemination of student work and the benefits of CREP.' SOURCE: (wagge brandt lazarevic legate christopherson wiggins grahe, 2018)","wagge brandt lazarevic legate christopherson wiggins grahe, 2018"
14,keech crowe hocking,"Intranasal oxytocin, social cognition and neurodevelopmental disorders: A meta-analysis",2018,"Highlights • 17 studies were included in the meta-analysis. • Oxytocin had no significant effect on emotion recognition. • Oxytocin had a small, significant effect on theory of mind. • The effect did not differ according to the diagnosis, age of participants, dose or dose frequency.",data/metaanalysis.pdf --> https://www.sciencedirect.com/science/article/pii/S0306453017304894,283,"TITLE: 'Intranasal oxytocin, social cognition and neurodevelopmental disorders: A meta-analysis' ABSTRACT: 'Highlights * 17 studies were included in the meta-analysis. * Oxytocin had no significant effect on emotion recognition. * Oxytocin had a small, significant effect on theory of mind. * The effect did not differ according to the diagnosis, age of participants, dose or dose frequency.' SOURCE: (keech crowe hocking, 2018)","keech crowe hocking, 2018"
15,lipsey wilson,Practical Meta-Analysis,2000,"Introduction Problem Specification and Study Retrieval Selecting, Computing and Coding the Effect Size Statistic Developing a Coding Scheme and Coding Study Reports Data Management Analysis Issues and Strategies Computational Techniques for Meta-Analysis Data Interpreting and Using Meta-Analysis Results",data/metaanalysis.pdf,304,"TITLE: 'Practical Meta-Analysis' ABSTRACT: 'Introduction Problem Specification and Study Retrieval Selecting, Computing and Coding the Effect Size Statistic Developing a Coding Scheme and Coding Study Reports Data Management Analysis Issues and Strategies Computational Techniques for Meta-Analysis Data Interpreting and Using Meta-Analysis Results' SOURCE: (lipsey wilson, 2000)","lipsey wilson, 2000"
16,gignac szodorai,Effect size guidelines for individual differences researchers,2016,"Highlights • Cohen's correlation guidelines (0.10, 0.30, 0.50) were evaluated empirically. • A pool of 708 meta-analytically derived correlations were collated. • The 25th, 50th, and 75th percentiles corresponded to r = 0.10, r = 0.20, r = 0.30. • Researchers are recommended to consider these normative guidelines.",data/metaanalysis.pdf --> https://www.sciencedirect.com/science/article/pii/S0191886916308194,315,"TITLE: 'Effect size guidelines for individual differences researchers' ABSTRACT: 'Highlights * Cohen's correlation guidelines (0.10, 0.30, 0.50) were evaluated empirically. * A pool of 708 meta-analytically derived correlations were collated. * The 25th, 50th, and 75th percentiles corresponded to r = 0.10, r = 0.20, r = 0.30. * Researchers are recommended to consider these normative guidelines.' SOURCE: (gignac szodorai, 2016)","gignac szodorai, 2016"
17,quintana,Most oxytocin administration studies are statistically underpowered to reliably detect (or reject) a wide range of effect sizes,2020,Highlights • The mean statistical power across 107 oxytocin administration studies was 12.2%. • The was no noticeable improvement in statistical power over an eight-year period. • None of the 26 non-significant meta-analyses were statistically equivalent. • Most oxytocin studies cannot detect or reject a wide range of effect sizes.,data/metaanalysis.pdf --> https://www.sciencedirect.com/science/article/pii/S266649762030014X,333,"TITLE: 'Most oxytocin administration studies are statistically underpowered to reliably detect (or reject) a wide range of effect sizes' ABSTRACT: 'Highlights * The mean statistical power across 107 oxytocin administration studies was 12.2%. * The was no noticeable improvement in statistical power over an eight-year period. * None of the 26 non-significant meta-analyses were statistically equivalent. * Most oxytocin studies cannot detect or reject a wide range of effect sizes.' SOURCE: (quintana, 2020)","quintana, 2020"
18,quintana,Towards better hypothesis tests in oxytocin research: Evaluating the validity of auxiliary assumptions,2021,"Highlights • Auxiliary assumptions are claims that are not central for testing a hypothesis. • The use of auxiliary assumptions are still critical for testing theories. • This article evaluates key auxiliary assumptions in oxytocin research. • Strong auxiliary assumptions will leave hypotheses vulnerable for falsification. • By falsifying hypotheses, we can advance our understanding of oxytocin.",10.1016/j.psyneuen.2021.105642 --> https://www.sciencedirect.com/science/article/pii/S0306453021005163,398,"TITLE: 'Towards better hypothesis tests in oxytocin research: Evaluating the validity of auxiliary assumptions' ABSTRACT: 'Highlights * Auxiliary assumptions are claims that are not central for testing a hypothesis. * The use of auxiliary assumptions are still critical for testing theories. * This article evaluates key auxiliary assumptions in oxytocin research. * Strong auxiliary assumptions will leave hypotheses vulnerable for falsification. * By falsifying hypotheses, we can advance our understanding of oxytocin.' SOURCE: (quintana, 2021)","quintana, 2021"
19,korbmacher azevedo pennington helena hartmann madeleine pownall kathleen schmidt elsherif nate breznau olly robertson kalandadze shijun yu bradley baker aoife mahony jørgen olsnes john shaw gjoneska yuki yamada röer jennifer murphy shilaan alzahawi sandra grinschgl cátia oliveira tobias wingen yeung meng liu könig nihan albayrak aydemir lecuona micheli evans,"The replication crisis has led to positive structural, procedural, and community changes",2023,"The emergence of large-scale replication projects yielding successful rates substantially lower than expected caused the behavioural, cognitive, and social sciences to experience a so-called ‘replication crisis’. In this Perspective, we reframe this ‘crisis’ through the lens of a credibility revolution, focusing on positive structural, procedural and community-driven changes. Second, we outline a path to expand ongoing advances and improvements. The credibility revolution has been an impetus to several substantive changes which will have a positive, long-term impact on our research environment.",10.1038/s44271-023-00003-2 --> https://www.nature.com/articles/s44271-023-00003-2,601,"TITLE: 'The replication crisis has led to positive structural, procedural, and community changes' ABSTRACT: 'The emergence of large-scale replication projects yielding successful rates substantially lower than expected caused the behavioural, cognitive, and social sciences to experience a so-called 'replication crisis'. In this Perspective, we reframe this 'crisis' through the lens of a credibility revolution, focusing on positive structural, procedural and community-driven changes. Second, we outline a path to expand ongoing advances and improvements. The credibility revolution has been an impetus to several substantive changes which will have a positive, long-term impact on our research environment.' SOURCE: (korbmacher azevedo pennington helena hartmann madeleine pownall kathleen schmidt elsherif nate breznau olly robertson kalandadze shijun yu bradley baker aoife mahony jorgen olsnes john shaw gjoneska yuki yamada roer jennifer murphy shilaan alzahawi sandra grinschgl catia oliveira tobias wingen yeung meng liu konig nihan albayrak aydemir lecuona micheli evans, 2023)","korbmacher azevedo pennington helena hartmann madeleine pownall kathleen schmidt, 2023"
20,button ioannidis mokrysz brian nosek flint robinson munafo,Power failure: why small sample size undermines the reliability of neuroscience,2013,"A study with low statistical power has a reduced chance of detecting a true effect, but it is less well appreciated that low power also reduces the likelihood that a statistically significant result reflects a true effect. Here, we show that the average statistical power of studies in the neurosciences is very low. The consequences of this include overestimates of effect size and low reproducibility of results. There are also ethical dimensions to this problem, as unreliable research is inefficient and wasteful. Improving reproducibility in neuroscience is a key priority and requires attention to well-established but often ignored methodological principles.",10.1038/nrn3475 --> https://www.nature.com/articles/nrn3475,665,"TITLE: 'Power failure: why small sample size undermines the reliability of neuroscience' ABSTRACT: 'A study with low statistical power has a reduced chance of detecting a true effect, but it is less well appreciated that low power also reduces the likelihood that a statistically significant result reflects a true effect. Here, we show that the average statistical power of studies in the neurosciences is very low. The consequences of this include overestimates of effect size and low reproducibility of results. There are also ethical dimensions to this problem, as unreliable research is inefficient and wasteful. Improving reproducibility in neuroscience is a key priority and requires attention to well-established but often ignored methodological principles.' SOURCE: (button ioannidis mokrysz brian nosek flint robinson munafo, 2013)","button ioannidis mokrysz brian nosek flint robinson munafo, 2013"
21,huedo medina sánchez meca marín martínez botella,Assessing heterogeneity in meta-analysis: Q statistic or I2 index?,2006,"In meta-analysis, the usual way of assessing whether a set of single studies is homogeneous is by means of the Q test. However, the Q test only informs meta-analysts about the presence versus the absence of heterogeneity, but it does not report on the extent of such heterogeneity. Recently, the I(2) index has been proposed to quantify the degree of heterogeneity in a meta-analysis. In this article, the performances of the Q test and the confidence interval around the I(2) index are compared by means of a Monte Carlo simulation. The results show the utility of the I(2) index as a complement to the Q test, although it has the same problems of power with a small number of studies.",data/metaanalysis.pdf,686,"TITLE: 'Assessing heterogeneity in meta-analysis: Q statistic or I2 index?' ABSTRACT: 'In meta-analysis, the usual way of assessing whether a set of single studies is homogeneous is by means of the Q test. However, the Q test only informs meta-analysts about the presence versus the absence of heterogeneity, but it does not report on the extent of such heterogeneity. Recently, the I(2) index has been proposed to quantify the degree of heterogeneity in a meta-analysis. In this article, the performances of the Q test and the confidence interval around the I(2) index are compared by means of a Monte Carlo simulation. The results show the utility of the I(2) index as a complement to the Q test, although it has the same problems of power with a small number of studies.' SOURCE: (huedo medina sanchez meca marin martinez botella, 2006)","huedo medina sanchez meca marin martinez botella, 2006"
22,viechtbauer,Conducting Meta-Analyses in R with the metafor Package,2010,"The metafor package provides functions for conducting meta-analyses in R. The package includes functions for fitting the meta-analytic fixed- and random-effects models and allows for the inclusion of moderators variables (study-level covariates) in these models. Meta-regression analyses with continuous and categorical moderators can be conducted in this way. Functions for the Mantel-Haenszel and Peto's one-step method for meta-analyses of 2 x 2 table data are also available. Finally, the package provides various plot functions (for example, for forest, funnel, and radial plots) and functions for assessing the model fit, for obtaining case diagnostics, and for tests of publication bias.",data/metaanalysis.pdf,694,"TITLE: 'Conducting Meta-Analyses in R with the metafor Package' ABSTRACT: 'The metafor package provides functions for conducting meta-analyses in R. The package includes functions for fitting the meta-analytic fixed- and random-effects models and allows for the inclusion of moderators variables (study-level covariates) in these models. Meta-regression analyses with continuous and categorical moderators can be conducted in this way. Functions for the Mantel-Haenszel and Peto's one-step method for meta-analyses of 2 x 2 table data are also available. Finally, the package provides various plot functions (for example, for forest, funnel, and radial plots) and functions for assessing the model fit, for obtaining case diagnostics, and for tests of publication bias.' SOURCE: (viechtbauer, 2010)","viechtbauer, 2010"
23,simonsohn nelson simmons,P-Curve and Effect Size: Correcting for Publication Bias Using Only Significant Results,2014,"Journals tend to publish only statistically significant evidence, creating a scientific record that markedly overstates the size of effects. We provide a new tool that corrects for this bias without requiring access to nonsignificant results. It capitalizes on the fact that the distribution of significant p-values, p-curve, is a function of the true underlying effect. Researchers armed only with sample sizes and test results of the published findings can correct for publication bias. We validate the technique with simulations and by re-analyzing data from the Many-Labs Replication project. We demonstrate that p-curve can arrive at conclusions opposite that of existing tools by re-analyzing the meta-analysis of the “choice overload�? literature.",data/metaanalysis.pdf,754,"TITLE: 'P-Curve and Effect Size: Correcting for Publication Bias Using Only Significant Results' ABSTRACT: 'Journals tend to publish only statistically significant evidence, creating a scientific record that markedly overstates the size of effects. We provide a new tool that corrects for this bias without requiring access to nonsignificant results. It capitalizes on the fact that the distribution of significant p-values, p-curve, is a function of the true underlying effect. Researchers armed only with sample sizes and test results of the published findings can correct for publication bias. We validate the technique with simulations and by re-analyzing data from the Many-Labs Replication project. We demonstrate that p-curve can arrive at conclusions opposite that of existing tools by re-analyzing the meta-analysis of the ""choice overload? literature.' SOURCE: (simonsohn nelson simmons, 2014)","simonsohn nelson simmons, 2014"
24,leng leng,Oxytocin: A citation network analysis of 10 000 papers,2021,"Our understanding of the oxytocin system has been built over the last 70 years by the work of hundreds of scientists, reported in thousands of papers. Here, we construct a map to that literature, using citation network analysis in conjunction with bibliometrics. The map identifies ten major ‘clusters’ of papers on oxytocin that differ in their particular research focus and that densely cite papers from the same cluster. We identify highly cited papers within each cluster and in each decade, not because citations are a good indicator of quality, but as a guide to recognising what questions were of wide interest at particular times. The clusters differ in their temporal profiles and bibliometric features; here, we attempt to understand the origins of these differences.",data/metaanalysis.pdf,777,"TITLE: 'Oxytocin: A citation network analysis of 10 000 papers' ABSTRACT: 'Our understanding of the oxytocin system has been built over the last 70 years by the work of hundreds of scientists, reported in thousands of papers. Here, we construct a map to that literature, using citation network analysis in conjunction with bibliometrics. The map identifies ten major 'clusters' of papers on oxytocin that differ in their particular research focus and that densely cite papers from the same cluster. We identify highly cited papers within each cluster and in each decade, not because citations are a good indicator of quality, but as a guide to recognising what questions were of wide interest at particular times. The clusters differ in their temporal profiles and bibliometric features; here, we attempt to understand the origins of these differences.' SOURCE: (leng leng, 2021)","leng leng, 2021"
25,hedges pigott,The power of statistical tests for moderators in meta-analysis.,2004,"Calculation of the statistical power of statistical tests is important in planning and interpreting the results of research studies, including meta-analyses. It is particularly important in moderator analyses in meta-analysis, which are often used as sensitivity analyses to rule out moderator effects but also may have low statistical power. This article describes how to compute statistical power of both fixed- and mixed-effects moderator tests in meta-analysis that are analogous to the analysis of variance and multiple regression analysis for effect sizes. It also shows how to compute power of tests for goodness of fit associated with these models. Examples from a published meta-analysis demonstrate that power of moderator tests and goodness-of-fit tests is not always high.",data/metaanalysis.pdf,784,"TITLE: 'The power of statistical tests for moderators in meta-analysis.' ABSTRACT: 'Calculation of the statistical power of statistical tests is important in planning and interpreting the results of research studies, including meta-analyses. It is particularly important in moderator analyses in meta-analysis, which are often used as sensitivity analyses to rule out moderator effects but also may have low statistical power. This article describes how to compute statistical power of both fixed- and mixed-effects moderator tests in meta-analysis that are analogous to the analysis of variance and multiple regression analysis for effect sizes. It also shows how to compute power of tests for goodness of fit associated with these models. Examples from a published meta-analysis demonstrate that power of moderator tests and goodness-of-fit tests is not always high.' SOURCE: (hedges pigott, 2004)","hedges pigott, 2004"
26,munafo nosek bishop button chambers sert simonsohn wagenmakers ware ioannidis,A manifesto for reproducible science,2017,"Improving the reliability and efficiency of scientific research will increase the credibility of the published scientific literature and accelerate discovery. Here we argue for the adoption of measures to optimize key elements of the scientific process: methods, reporting and dissemination, reproducibility, evaluation and incentives. There is some evidence from both simulations and empirical studies supporting the likely effectiveness of these measures, but their broad adoption by researchers, institutions, funders and journals will require iterative evaluation and improvement. We discuss the goals of these measures, and how they can be implemented, in the hope that this will facilitate action toward improving the transparency, reproducibility and efficiency of scientific research.",data/metaanalysis.pdf --> https://www.nature.com/articles/s41562-016-0021;,792,"TITLE: 'A manifesto for reproducible science' ABSTRACT: 'Improving the reliability and efficiency of scientific research will increase the credibility of the published scientific literature and accelerate discovery. Here we argue for the adoption of measures to optimize key elements of the scientific process: methods, reporting and dissemination, reproducibility, evaluation and incentives. There is some evidence from both simulations and empirical studies supporting the likely effectiveness of these measures, but their broad adoption by researchers, institutions, funders and journals will require iterative evaluation and improvement. We discuss the goals of these measures, and how they can be implemented, in the hope that this will facilitate action toward improving the transparency, reproducibility and efficiency of scientific research.' SOURCE: (munafo nosek bishop button chambers sert simonsohn wagenmakers ware ioannidis, 2017)","munafo nosek bishop button chambers sert simonsohn wagenmakers ware ioannidis, 2017"
27,duval tweedie,Trim and Fill: A Simple Funnel‐Plot–Based Method of Testing and Adjusting for Publication Bias in Meta‐Analysis,2000,"Summary. We study recently developed nonparametric methods for estimating the number of missing studies that might exist in a meta‐analysis and the effect that these studies might have had on its outcome. These are simple rank‐based data augmentation techniques, which formalize the use of funnel plots. We show that they provide effective and relatively powerful tests for evaluating the existence of such publication bias. After adjusting for missing studies, we find that the point estimate of the overall effect size is approximately correct and coverage of the effect size confidence intervals is substantially improved, in many cases recovering the nominal confidence levels entirely. We illustrate the trim and fill method on existing meta‐analyses of studies in clinical trials and psychometrics.",10.1111/j.0006-341X.2000.00455.x,804,"TITLE: 'Trim and Fill: A Simple Funnel-Plot-Based Method of Testing and Adjusting for Publication Bias in Meta-Analysis' ABSTRACT: 'Summary. We study recently developed nonparametric methods for estimating the number of missing studies that might exist in a meta-analysis and the effect that these studies might have had on its outcome. These are simple rank-based data augmentation techniques, which formalize the use of funnel plots. We show that they provide effective and relatively powerful tests for evaluating the existence of such publication bias. After adjusting for missing studies, we find that the point estimate of the overall effect size is approximately correct and coverage of the effect size confidence intervals is substantially improved, in many cases recovering the nominal confidence levels entirely. We illustrate the trim and fill method on existing meta-analyses of studies in clinical trials and psychometrics.' SOURCE: (duval tweedie, 2000)","duval tweedie, 2000"
28,vevea woods,Publication bias in research synthesis: sensitivity analysis using a priori weight functions.,2005,"Publication bias, sometimes known as the ""file-drawer problem"" or ""funnel-plot asymmetry,"" is common in empirical research. The authors review the implications of publication bias for quantitative research synthesis (meta-analysis) and describe existing techniques for detecting and correcting it. A new approach is proposed that is suitable for application to meta-analytic data sets that are too small for the application of existing methods. The model estimates parameters relevant to fixed-effects, mixed-effects or random-effects meta-analysis contingent on a hypothetical pattern of bias that is fixed independently of the data. The authors illustrate this approach for sensitivity analysis using 3 data sets adapted from a commonly cited reference work on research synthesis (H. M. Cooper & L. V. Hedges, 1994).",data/metaanalysis.pdf,818,"TITLE: 'Publication bias in research synthesis: sensitivity analysis using a priori weight functions.' ABSTRACT: 'Publication bias, sometimes known as the ""file-drawer problem"" or ""funnel-plot asymmetry,"" is common in empirical research. The authors review the implications of publication bias for quantitative research synthesis (meta-analysis) and describe existing techniques for detecting and correcting it. A new approach is proposed that is suitable for application to meta-analytic data sets that are too small for the application of existing methods. The model estimates parameters relevant to fixed-effects, mixed-effects or random-effects meta-analysis contingent on a hypothetical pattern of bias that is fixed independently of the data. The authors illustrate this approach for sensitivity analysis using 3 data sets adapted from a commonly cited reference work on research synthesis (H. M. Cooper & L. V. Hedges, 1994).' SOURCE: (vevea woods, 2005)","vevea woods, 2005"
29,maccallum zhang preacher rucker,On the practice of dichotomization of quantitative variables.,2002,"The authors examine the practice of dichotomization of quantitative measures, wherein relationships among variables are examined after 1 or more variables have been converted to dichotomous variables by splitting the sample at some point on the scale(s) of measurement. A common form of dichotomization is the median split, where the independent variable is split at the median to form high and low groups, which are then compared with respect to their means on the dependent variable. The consequences of dichotomization for measurement and statistical analyses are illustrated and discussed. The use of dichotomization in practice is described, and justifications that are offered for such usage are examined. The authors present the case that dichotomization is rarely defensible and often will yield misleading results.",data/metaanalysis.pdf,823,"TITLE: 'On the practice of dichotomization of quantitative variables.' ABSTRACT: 'The authors examine the practice of dichotomization of quantitative measures, wherein relationships among variables are examined after 1 or more variables have been converted to dichotomous variables by splitting the sample at some point on the scale(s) of measurement. A common form of dichotomization is the median split, where the independent variable is split at the median to form high and low groups, which are then compared with respect to their means on the dependent variable. The consequences of dichotomization for measurement and statistical analyses are illustrated and discussed. The use of dichotomization in practice is described, and justifications that are offered for such usage are examined. The authors present the case that dichotomization is rarely defensible and often will yield misleading results.' SOURCE: (maccallum zhang preacher rucker, 2002)","maccallum zhang preacher rucker, 2002"
30,lakens scheel isager,Equivalence Testing for Psychological Research: A Tutorial,2018,"Psychologists must be able to test both for the presence of an effect and for the absence of an effect. In addition to testing against zero, researchers can use the two one-sided tests (TOST) procedure to test for equivalence and reject the presence of a smallest effect size of interest (SESOI). The TOST procedure can be used to determine if an observed effect is surprisingly small, given that a true effect at least as extreme as the SESOI exists. We explain a range of approaches to determine the SESOI in psychological science and provide detailed examples of how equivalence tests should be performed and reported. Equivalence tests are an important extension of the statistical tools psychologists currently use and enable researchers to falsify predictions about the presence, and declare the absence, of meaningful effects.",data/metaanalysis.pdf,833,"TITLE: 'Equivalence Testing for Psychological Research: A Tutorial' ABSTRACT: 'Psychologists must be able to test both for the presence of an effect and for the absence of an effect. In addition to testing against zero, researchers can use the two one-sided tests (TOST) procedure to test for equivalence and reject the presence of a smallest effect size of interest (SESOI). The TOST procedure can be used to determine if an observed effect is surprisingly small, given that a true effect at least as extreme as the SESOI exists. We explain a range of approaches to determine the SESOI in psychological science and provide detailed examples of how equivalence tests should be performed and reported. Equivalence tests are an important extension of the statistical tools psychologists currently use and enable researchers to falsify predictions about the presence, and declare the absence, of meaningful effects.' SOURCE: (lakens scheel isager, 2018)","lakens scheel isager, 2018"
31,quintana guastella,An Allostatic Theory of Oxytocin,2019,"Oxytocin has garnered considerable interest for its role in social behavior, as well as for the potential of intranasal administration to treat social difficulties. However, current theoretical models for the role of oxytocin in social behavior pay little consideration to its evolutionary and developmental history. This article aims to broaden our understanding of the role of oxytocin in social behavior by adopting an ethological approach through the lens of Nikolaas Tinbergen's 'four questions' - how does oxytocin work; how does the role of oxytocin change during development; how does oxytocin enhance survival; and how did the oxytocin system evolve? We argue that oxytocin is most accurately described as an allostatic hormone that modulates both social and non-social behavior by maintaining stability through changing environments.",data/metaanalysis.pdf --> https://pubmed.ncbi.nlm.nih.gov/32360118/,843,"TITLE: 'An Allostatic Theory of Oxytocin' ABSTRACT: 'Oxytocin has garnered considerable interest for its role in social behavior, as well as for the potential of intranasal administration to treat social difficulties. However, current theoretical models for the role of oxytocin in social behavior pay little consideration to its evolutionary and developmental history. This article aims to broaden our understanding of the role of oxytocin in social behavior by adopting an ethological approach through the lens of Nikolaas Tinbergen's 'four questions' - how does oxytocin work; how does the role of oxytocin change during development; how does oxytocin enhance survival; and how did the oxytocin system evolve? We argue that oxytocin is most accurately described as an allostatic hormone that modulates both social and non-social behavior by maintaining stability through changing environments.' SOURCE: (quintana guastella, 2019)","quintana guastella, 2019"
32,felipe romero sprenger,Scientific self-correction: the Bayesian way,2020,"The enduring replication crisis in many scientific disciplines casts doubt on the ability of science to estimate effect sizes accurately, and in a wider sense, to self-correct its findings and to produce reliable knowledge. We investigate the merits of a particular countermeasure—replacing null hypothesis significance testing (NHST) with Bayesian inference—in the context of the meta-analytic aggregation of effect sizes. In particular, we elaborate on the advantages of this Bayesian reform proposal under conditions of publication bias and other methodological imperfections that are typical of experimental research in the behavioral sciences. Moving to Bayesian statistics would not solve the replication crisis single-handedly. However, the move would eliminate important sources of effect size overestimation for the conditions we study.",10.1007/s11229-020-02697-x --> https://link.springer.com/article/10.1007/s11229-020-02697-x,845,"TITLE: 'Scientific self-correction: the Bayesian way' ABSTRACT: 'The enduring replication crisis in many scientific disciplines casts doubt on the ability of science to estimate effect sizes accurately, and in a wider sense, to self-correct its findings and to produce reliable knowledge. We investigate the merits of a particular countermeasure--replacing null hypothesis significance testing (NHST) with Bayesian inference--in the context of the meta-analytic aggregation of effect sizes. In particular, we elaborate on the advantages of this Bayesian reform proposal under conditions of publication bias and other methodological imperfections that are typical of experimental research in the behavioral sciences. Moving to Bayesian statistics would not solve the replication crisis single-handedly. However, the move would eliminate important sources of effect size overestimation for the conditions we study.' SOURCE: (felipe romero sprenger, 2020)","felipe romero sprenger, 2020"
33,kraft,Interpreting Effect Sizes of Education Interventions,2020,"Researchers commonly interpret effect sizes by applying benchmarks proposed by Jacob Cohen over a half century ago. However, effects that are small by Cohen’s standards are large relative to the impacts of most field-based interventions. These benchmarks also fail to consider important differences in study features, program costs, and scalability. In this article, I present five broad guidelines for interpreting effect sizes that are applicable across the social sciences. I then propose a more structured schema with new empirical benchmarks for interpreting a specific class of studies: causal research on education interventions with standardized achievement outcomes. Together, these tools provide a practical approach for incorporating study features, costs, and scalability into the process of interpreting the policy importance of effect sizes.",data/metaanalysis.pdf,855,"TITLE: 'Interpreting Effect Sizes of Education Interventions' ABSTRACT: 'Researchers commonly interpret effect sizes by applying benchmarks proposed by Jacob Cohen over a half century ago. However, effects that are small by Cohen's standards are large relative to the impacts of most field-based interventions. These benchmarks also fail to consider important differences in study features, program costs, and scalability. In this article, I present five broad guidelines for interpreting effect sizes that are applicable across the social sciences. I then propose a more structured schema with new empirical benchmarks for interpreting a specific class of studies: causal research on education interventions with standardized achievement outcomes. Together, these tools provide a practical approach for incorporating study features, costs, and scalability into the process of interpreting the policy importance of effect sizes.' SOURCE: (kraft, 2020)","kraft, 2020"
34,john loewenstein prelec,Measuring the Prevalence of Questionable Research Practices With Incentives for Truth Telling,2012,"Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm.",10.1177/0956797611430953,857,"TITLE: 'Measuring the Prevalence of Questionable Research Practices With Incentives for Truth Telling' ABSTRACT: 'Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm.' SOURCE: (john loewenstein prelec, 2012)","john loewenstein prelec, 2012"
35,page mckenzie bossuyt boutron hoffmann mulrow shamseer tetzlaff akl brennan chou glanville grimshaw hrõbjartsson lalu li loder mayo wilson mcdonald mcguinness stewart thomas tricco welch whiting moher,The PRISMA 2020 statement: an updated guideline for reporting systematic reviews,2020,"The Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) statement, published in 2009, was designed to help systematic reviewers transparently report why the review was done, what the authors did, and what they found. Over the past decade, advances in systematic review methodology and terminology have necessitated an update to the guideline. The PRISMA 2020 statement replaces the 2009 statement and includes new reporting guidance that reflects advances in methods to identify, select, appraise, and synthesise studies. The structure and presentation of the items have been modified to facilitate implementation. In this article, we present the PRISMA 2020 27-item checklist, an expanded checklist that details reporting recommendations for each item, the PRISMA 2020 abstract checklist, and the revised flow diagrams for original and updated reviews.",data/metaanalysis.pdf,877,"TITLE: 'The PRISMA 2020 statement: an updated guideline for reporting systematic reviews' ABSTRACT: 'The Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) statement, published in 2009, was designed to help systematic reviewers transparently report why the review was done, what the authors did, and what they found. Over the past decade, advances in systematic review methodology and terminology have necessitated an update to the guideline. The PRISMA 2020 statement replaces the 2009 statement and includes new reporting guidance that reflects advances in methods to identify, select, appraise, and synthesise studies. The structure and presentation of the items have been modified to facilitate implementation. In this article, we present the PRISMA 2020 27-item checklist, an expanded checklist that details reporting recommendations for each item, the PRISMA 2020 abstract checklist, and the revised flow diagrams for original and updated reviews.' SOURCE: (page mckenzie bossuyt boutron hoffmann mulrow shamseer tetzlaff akl brennan chou glanville grimshaw hrobjartsson lalu li loder mayo wilson mcdonald mcguinness stewart thomas tricco welch whiting moher, 2020)","page mckenzie bossuyt boutron hoffmann mulrow shamseer tetzlaff akl brennan chou, 2020"
36,guyatt oxman montori vist kunz brożek alonso coello djulbegovic atkins falck ytter williams meerpohl norris akl schünemann,GRADE guidelines: 5. Rating the quality of evidence--publication bias.,2011,"In the GRADE approach, randomized trials start as high-quality evidence and observational studies as low-quality evidence, but both can be rated down if a body of evidence is associated with a high risk of publication bias. Even when individual studies included in best-evidence summaries have a low risk of bias, publication bias can result in substantial overestimates of effect. Authors should suspect publication bias when available evidence comes from a number of small studies, most of which have been commercially funded. A number of approaches based on examination of the pattern of data are available to help assess publication bias. The most popular of these is the funnel plot; all, however, have substantial limitations. Publication bias is likely frequent, and caution in the face of early results, particularly with small sample size and number of events, is warranted.",data/metaanalysis.pdf --> https://www.sciencedirect.com/science/article/pii/S0895435611001818,883,"TITLE: 'GRADE guidelines: 5. Rating the quality of evidence--publication bias.' ABSTRACT: 'In the GRADE approach, randomized trials start as high-quality evidence and observational studies as low-quality evidence, but both can be rated down if a body of evidence is associated with a high risk of publication bias. Even when individual studies included in best-evidence summaries have a low risk of bias, publication bias can result in substantial overestimates of effect. Authors should suspect publication bias when available evidence comes from a number of small studies, most of which have been commercially funded. A number of approaches based on examination of the pattern of data are available to help assess publication bias. The most popular of these is the funnel plot; all, however, have substantial limitations. Publication bias is likely frequent, and caution in the face of early results, particularly with small sample size and number of events, is warranted.' SOURCE: (guyatt oxman montori vist kunz brozek alonso coello djulbegovic atkins falck ytter williams meerpohl norris akl schunemann, 2011)","guyatt oxman montori vist kunz brozek alonso coello djulbegovic atkins falck ytt, 2011"
37,simmons leif nelson simonsohn,False-Positive Psychology,2011,"In this article, we accomplish two things. First, we show that despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.",10.1177/0956797611417632,883,"TITLE: 'False-Positive Psychology' ABSTRACT: 'In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings (<= .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.' SOURCE: (simmons leif nelson simonsohn, 2011)","simmons leif nelson simonsohn, 2011"
38,nuijten polanin,“statcheck”: Automatically detect statistical reporting inconsistencies to increase reproducibility of meta‐analyses,2020,"We present the R package and web app statcheck to automatically detect statistical reporting inconsistencies in primary studies and meta‐analyses. Previous research has shown a high prevalence of reported p‐values that are inconsistent ‐ meaning a re‐calculated p‐value, based on the reported test statistic and degrees of freedom, does not match the author‐reported p‐value. Such inconsistencies affect the reproducibility and evidential value of published findings. The tool statcheck can help researchers to identify statistical inconsistencies so that they may correct them. In this paper, we provide an overview of the prevalence and consequences of statistical reporting inconsistencies. We also discuss the tool statcheck in more detail and give an example of how it can be used in a meta‐analysis. We end with some recommendations concerning the use of statcheck in meta‐analyses and make a case for better reporting standards of statistical results.",data/metaanalysis.pdf,958,"TITLE: '""statcheck"": Automatically detect statistical reporting inconsistencies to increase reproducibility of meta-analyses' ABSTRACT: 'We present the R package and web app statcheck to automatically detect statistical reporting inconsistencies in primary studies and meta-analyses. Previous research has shown a high prevalence of reported p-values that are inconsistent - meaning a re-calculated p-value, based on the reported test statistic and degrees of freedom, does not match the author-reported p-value. Such inconsistencies affect the reproducibility and evidential value of published findings. The tool statcheck can help researchers to identify statistical inconsistencies so that they may correct them. In this paper, we provide an overview of the prevalence and consequences of statistical reporting inconsistencies. We also discuss the tool statcheck in more detail and give an example of how it can be used in a meta-analysis. We end with some recommendations concerning the use of statcheck in meta-analyses and make a case for better reporting standards of statistical results.' SOURCE: (nuijten polanin, 2020)","nuijten polanin, 2020"
39,heathers anaya zee brown,Recovering data from summary statistics: Sample Parameter Reconstruction via Iterative TEchniques (SPRITE),2018,"Scientific publications have not traditionally been accompanied by data, either during the peer review process or when published. Concern has arisen that the literature in many fields may contain inaccuracies or errors that cannot be detected without inspecting the original data. Here, we introduce SPRITE (Sample Parameter Reconstruction via Interative TEchniques), a heuristic method for reconstructing plausible samples from descriptive statistics of granular data, allowing reviewers, editors, readers, and future researchers to gain insights into the possible distributions of item values in the original data set. This paper presents the principles of operation of SPRITE, as well as worked examples of its practical use for error detection in real published work. Full source code for three software implementations of SPRITE (in MATLAB, R, and Python) and two web-based implementations requiring no local installation (1, 2) are available for readers.",data/metaanalysis.pdf,960,"TITLE: 'Recovering data from summary statistics: Sample Parameter Reconstruction via Iterative TEchniques (SPRITE)' ABSTRACT: 'Scientific publications have not traditionally been accompanied by data, either during the peer review process or when published. Concern has arisen that the literature in many fields may contain inaccuracies or errors that cannot be detected without inspecting the original data. Here, we introduce SPRITE (Sample Parameter Reconstruction via Interative TEchniques), a heuristic method for reconstructing plausible samples from descriptive statistics of granular data, allowing reviewers, editors, readers, and future researchers to gain insights into the possible distributions of item values in the original data set. This paper presents the principles of operation of SPRITE, as well as worked examples of its practical use for error detection in real published work. Full source code for three software implementations of SPRITE (in MATLAB, R, and Python) and two web-based implementations requiring no local installation (1, 2) are available for readers.' SOURCE: (heathers anaya zee brown, 2018)","heathers anaya zee brown, 2018"
40,winterton westlye steen andreassen quintana,Improving the precision of intranasal oxytocin research,2020,"The neuropeptide oxytocin has been popularized for its role in social behaviour and nominated as a candidate treatment for several psychiatric illnesses due to promising preclinical results. However, these results so far have failed to reliably translate from animal models to human research. In response, there have been justified calls to improve intranasal oxytocin delivery methodology in terms of verifying that intranasal administration increases central levels of oxytocin. Nonetheless, improved methodology needs to be coupled with a robust theory of the role of oxytocin in behaviour and physiology to ask meaningful research questions. Moreover, stringent methodology based on robust theory may yield interesting results, but such findings will have limited utility if they are not reproducible. We outline how the precision of intranasal oxytocin research can be improved by the complementary consideration of methodology, theory and reproducibility.",10.1038/s41562-020-00996-4 --> https://www.nature.com/articles/s41562-020-00996-4,961,"TITLE: 'Improving the precision of intranasal oxytocin research' ABSTRACT: 'The neuropeptide oxytocin has been popularized for its role in social behaviour and nominated as a candidate treatment for several psychiatric illnesses due to promising preclinical results. However, these results so far have failed to reliably translate from animal models to human research. In response, there have been justified calls to improve intranasal oxytocin delivery methodology in terms of verifying that intranasal administration increases central levels of oxytocin. Nonetheless, improved methodology needs to be coupled with a robust theory of the role of oxytocin in behaviour and physiology to ask meaningful research questions. Moreover, stringent methodology based on robust theory may yield interesting results, but such findings will have limited utility if they are not reproducible. We outline how the precision of intranasal oxytocin research can be improved by the complementary consideration of methodology, theory and reproducibility.' SOURCE: (winterton westlye steen andreassen quintana, 2020)","winterton westlye steen andreassen quintana, 2020"
41,elizabeth murdoch robin lines crane ntoumanis carly brade quested joanne ayers gucciardi,The effectiveness of stress regulation interventions with athletes: A systematic review and multilevel meta-analysis of randomised controlled trials,2021,"s in order of significance from existing articles known to the team as relevant for inclusion based on the screening criteria, and continuously updates the learning algorithm every 50 abstracts screened based on what is deemed as in/eligible by the reviewer. Preliminary evidence supports the utility of Research Screener for semi-automating the screening process (Chai et al., 2021). Briefly, across nine systematic reviews and two scoping reviews, Research Screener delivered a 60-90% workload saving, and estimated a conservative threshold of the need to screen no more than 50% of articles to assure that 100% of eligible articles are identified. EM and RL discussed uncertainty regarding the screening decision for 16 papers with DG, who made the executive decision regarding their suitability for inclusion in the meta-analysis. Reasons for study exclusion were summarised as part of the search and included in the data extraction flow diagram (see Figure 1).",10.1080/1750984x.2021.1977974,965,"TITLE: 'The effectiveness of stress regulation interventions with athletes: A systematic review and multilevel meta-analysis of randomised controlled trials' ABSTRACT: 's in order of significance from existing articles known to the team as relevant for inclusion based on the screening criteria, and continuously updates the learning algorithm every 50 abstracts screened based on what is deemed as in/eligible by the reviewer. Preliminary evidence supports the utility of Research Screener for semi-automating the screening process (Chai et al., 2021). Briefly, across nine systematic reviews and two scoping reviews, Research Screener delivered a 60-90% workload saving, and estimated a conservative threshold of the need to screen no more than 50% of articles to assure that 100% of eligible articles are identified. EM and RL discussed uncertainty regarding the screening decision for 16 papers with DG, who made the executive decision regarding their suitability for inclusion in the meta-analysis. Reasons for study exclusion were summarised as part of the search and included in the data extraction flow diagram (see Figure 1).' SOURCE: (elizabeth murdoch robin lines crane ntoumanis carly brade quested joanne ayers gucciardi, 2021)","elizabeth murdoch robin lines crane ntoumanis carly brade quested joanne ayers g, 2021"
42,kvarven strømland johannesson,Comparing meta-analyses and preregistered multiple-laboratory replication projects,2019,"Many researchers rely on meta-analysis to summarize research evidence. However, there is a concern that publication bias and selective reporting may lead to biased meta-analytic effect sizes. We compare the results of meta-analyses to large-scale preregistered replications in psychology carried out at multiple laboratories. The multiple-laboratory replications provide precisely estimated effect sizes that do not suffer from publication bias or selective reporting. We searched the literature and identified 15 meta-analyses on the same topics as multiple-laboratory replications. We find that meta-analytic effect sizes are significantly different from replication effect sizes for 12 out of the 15 meta-replication pairs. These differences are systematic and, on average, meta-analytic effect sizes are almost three times as large as replication effect sizes. We also implement three methods of correcting meta-analysis for bias, but these methods do not substantively improve the meta-analytic results.",data/metaanalysis.pdf --> https://www.nature.com/articles/s41562-019-0787-z,1008,"TITLE: 'Comparing meta-analyses and preregistered multiple-laboratory replication projects' ABSTRACT: 'Many researchers rely on meta-analysis to summarize research evidence. However, there is a concern that publication bias and selective reporting may lead to biased meta-analytic effect sizes. We compare the results of meta-analyses to large-scale preregistered replications in psychology carried out at multiple laboratories. The multiple-laboratory replications provide precisely estimated effect sizes that do not suffer from publication bias or selective reporting. We searched the literature and identified 15 meta-analyses on the same topics as multiple-laboratory replications. We find that meta-analytic effect sizes are significantly different from replication effect sizes for 12 out of the 15 meta-replication pairs. These differences are systematic and, on average, meta-analytic effect sizes are almost three times as large as replication effect sizes. We also implement three methods of correcting meta-analysis for bias, but these methods do not substantively improve the meta-analytic results.' SOURCE: (kvarven stromland johannesson, 2019)","kvarven stromland johannesson, 2019"
43,charles james bartlett messick coleman alex uzdavines,Researcher Degrees of Freedom in the Psychology of Religion,2019,"ABSTRACT There is a push in psychology toward more transparent practices, stemming partially as a response to the replication crisis. We argue that the psychology of religion should help lead the way toward these new, more transparent practices to ensure a robust and dynamic subfield. One of the major issues that proponents of Open Science practices hope to address is researcher degrees of freedom (RDF). We pre-registered and conducted a systematic review of the 2017 issues from three psychology of religion journals. We aimed to identify the extent to which the psychology of religion has embraced Open Science practices and the role of RDF within the subfield. We found that many of the methodologies that help to increase transparency, such as pre-registration, have yet to be adopted by those in the subfield. In light of these findings, we present recommendations for addressing the issue of transparency in the psychology of religion and outline how to move toward these new Open Science practices.",10.1080/10508619.2019.1660573,1009,"TITLE: 'Researcher Degrees of Freedom in the Psychology of Religion' ABSTRACT: 'ABSTRACT There is a push in psychology toward more transparent practices, stemming partially as a response to the replication crisis. We argue that the psychology of religion should help lead the way toward these new, more transparent practices to ensure a robust and dynamic subfield. One of the major issues that proponents of Open Science practices hope to address is researcher degrees of freedom (RDF). We pre-registered and conducted a systematic review of the 2017 issues from three psychology of religion journals. We aimed to identify the extent to which the psychology of religion has embraced Open Science practices and the role of RDF within the subfield. We found that many of the methodologies that help to increase transparency, such as pre-registration, have yet to be adopted by those in the subfield. In light of these findings, we present recommendations for addressing the issue of transparency in the psychology of religion and outline how to move toward these new Open Science practices.' SOURCE: (charles james bartlett messick coleman alex uzdavines, 2019)","charles james bartlett messick coleman alex uzdavines, 2019"
44,chambers tzavella,"The past, present and future of Registered Reports",2021,"Registered Reports are a form of empirical publication in which study proposals are peer reviewed and pre-accepted before research is undertaken. By deciding which articles are published based on the question, theory and methods, Registered Reports offer a remedy for a range of reporting and publication biases. Here, we reflect on the history, progress and future prospects of the Registered Reports initiative and offer practical guidance for authors, reviewers and editors. We review early evidence that Registered Reports are working as intended, while at the same time acknowledging that they are not a universal solution for irreproducibility. We also consider how the policies and practices surrounding Registered Reports are changing, or must change in the future, to address limitations and adapt to new challenges. We conclude that Registered Reports are promoting reproducibility, transparency and self-correction across disciplines and may help reshape how society evaluates research and researchers.",data/metaanalysis.pdf --> https://pubmed.ncbi.nlm.nih.gov/34782730/,1013,"TITLE: 'The past, present and future of Registered Reports' ABSTRACT: 'Registered Reports are a form of empirical publication in which study proposals are peer reviewed and pre-accepted before research is undertaken. By deciding which articles are published based on the question, theory and methods, Registered Reports offer a remedy for a range of reporting and publication biases. Here, we reflect on the history, progress and future prospects of the Registered Reports initiative and offer practical guidance for authors, reviewers and editors. We review early evidence that Registered Reports are working as intended, while at the same time acknowledging that they are not a universal solution for irreproducibility. We also consider how the policies and practices surrounding Registered Reports are changing, or must change in the future, to address limitations and adapt to new challenges. We conclude that Registered Reports are promoting reproducibility, transparency and self-correction across disciplines and may help reshape how society evaluates research and researchers.' SOURCE: (chambers tzavella, 2021)","chambers tzavella, 2021"
45,bryan tipton yeager,Behavioural science is unlikely to change the world without a heterogeneity revolution,2021,"In the past decade, behavioural science has gained influence in policymaking but suffered a crisis of confidence in the replicability of its findings. Here, we describe a nascent heterogeneity revolution that we believe these twin historical trends have triggered. This revolution will be defined by the recognition that most treatment effects are heterogeneous, so the variation in effect estimates across studies that defines the replication crisis is to be expected as long as heterogeneous effects are studied without a systematic approach to sampling and moderation. When studied systematically, heterogeneity can be leveraged to build more complete theories of causal mechanism that could inform nuanced and dependable guidance to policymakers. We recommend investment in shared research infrastructure to make it feasible to study behavioural interventions in heterogeneous and generalizable samples, and suggest low-cost steps researchers can take immediately to avoid being misled by heterogeneity and begin to learn from it instead.",data/metaanalysis.pdf --> https://pubmed.ncbi.nlm.nih.gov/34294901/,1042,"TITLE: 'Behavioural science is unlikely to change the world without a heterogeneity revolution' ABSTRACT: 'In the past decade, behavioural science has gained influence in policymaking but suffered a crisis of confidence in the replicability of its findings. Here, we describe a nascent heterogeneity revolution that we believe these twin historical trends have triggered. This revolution will be defined by the recognition that most treatment effects are heterogeneous, so the variation in effect estimates across studies that defines the replication crisis is to be expected as long as heterogeneous effects are studied without a systematic approach to sampling and moderation. When studied systematically, heterogeneity can be leveraged to build more complete theories of causal mechanism that could inform nuanced and dependable guidance to policymakers. We recommend investment in shared research infrastructure to make it feasible to study behavioural interventions in heterogeneous and generalizable samples, and suggest low-cost steps researchers can take immediately to avoid being misled by heterogeneity and begin to learn from it instead.' SOURCE: (bryan tipton yeager, 2021)","bryan tipton yeager, 2021"
46,szűcs ioannidis,Empirical assessment of published effect sizes and power in the recent cognitive neuroscience and psychology literature,2017,"We have empirically assessed the distribution of published effect sizes and estimated power by analyzing 26,841 statistical records from 3,801 cognitive neuroscience and psychology papers published recently. The reported median effect size was D = 0.93 (interquartile range: 0.64–1.46) for nominally statistically significant results and D = 0.24 (0.11–0.42) for nonsignificant results. Median power to detect small, medium, and large effects was 0.12, 0.44, and 0.73, reflecting no improvement through the past half-century. This is so because sample sizes have remained small. Assuming similar true effect sizes in both disciplines, power was lower in cognitive neuroscience than in psychology. Journal impact factors negatively correlated with power. Assuming a realistic range of prior probabilities for null hypotheses, false report probability is likely to exceed 50% for the whole literature. In light of our findings, the recently reported low replication success in psychology is realistic, and worse performance may be expected for cognitive neuroscience.",data/metaanalysis.pdf,1065,"TITLE: 'Empirical assessment of published effect sizes and power in the recent cognitive neuroscience and psychology literature' ABSTRACT: 'We have empirically assessed the distribution of published effect sizes and estimated power by analyzing 26,841 statistical records from 3,801 cognitive neuroscience and psychology papers published recently. The reported median effect size was D = 0.93 (interquartile range: 0.64-1.46) for nominally statistically significant results and D = 0.24 (0.11-0.42) for nonsignificant results. Median power to detect small, medium, and large effects was 0.12, 0.44, and 0.73, reflecting no improvement through the past half-century. This is so because sample sizes have remained small. Assuming similar true effect sizes in both disciplines, power was lower in cognitive neuroscience than in psychology. Journal impact factors negatively correlated with power. Assuming a realistic range of prior probabilities for null hypotheses, false report probability is likely to exceed 50% for the whole literature. In light of our findings, the recently reported low replication success in psychology is realistic, and worse performance may be expected for cognitive neuroscience.' SOURCE: (szucs ioannidis, 2017)","szucs ioannidis, 2017"
47,cristian mesquida jennifer murphy lakens warne,Replication concerns in sports and exercise science: a narrative review of selected methodological issues in the field,2022,"Known methodological issues such as publication bias, questionable research practices and studies with underpowered designs are known to decrease the replicability of study findings. The presence of such issues has been widely established across different research fields, especially in psychology. Their presence raised the first concerns that the replicability of study findings could be low and led researchers to conduct large replication projects. These replication projects revealed that a significant portion of original study findings could not be replicated, giving rise to the conceptualization of the replication crisis. Although previous research in the field of sports and exercise science has identified the first warning signs, such as an overwhelming proportion of significant findings, small sample sizes and lack of data availability, their possible consequences for the replicability of our field have been overlooked. We discuss the consequences of the above issues on the replicability of our field and offer potential solutions to improve replicability.",10.1098/rsos.220946,1075,"TITLE: 'Replication concerns in sports and exercise science: a narrative review of selected methodological issues in the field' ABSTRACT: 'Known methodological issues such as publication bias, questionable research practices and studies with underpowered designs are known to decrease the replicability of study findings. The presence of such issues has been widely established across different research fields, especially in psychology. Their presence raised the first concerns that the replicability of study findings could be low and led researchers to conduct large replication projects. These replication projects revealed that a significant portion of original study findings could not be replicated, giving rise to the conceptualization of the replication crisis. Although previous research in the field of sports and exercise science has identified the first warning signs, such as an overwhelming proportion of significant findings, small sample sizes and lack of data availability, their possible consequences for the replicability of our field have been overlooked. We discuss the consequences of the above issues on the replicability of our field and offer potential solutions to improve replicability.' SOURCE: (cristian mesquida jennifer murphy lakens warne, 2022)","cristian mesquida jennifer murphy lakens warne, 2022"
48,stegenga,Is meta-analysis the platinum standard of evidence?,2011,"An astonishing volume and diversity of evidence is available for many hypotheses in the biomedical and social sciences. Some of this evidence—usually from randomized controlled trials (RCTs)—is amalgamated by meta-analysis. Despite the ongoing debate regarding whether or not RCTs are the ‘gold-standard’ of evidence, it is usually meta-analysis which is considered the best source of evidence: meta-analysis is thought by many to be the platinum standard of evidence. However, I argue that meta-analysis falls far short of that standard. Different meta-analyses of the same evidence can reach contradictory conclusions. Meta-analysis fails to provide objective grounds for intersubjective assessments of hypotheses because numerous decisions must be made when performing a meta-analysis which allow wide latitude for subjective idiosyncrasies to influence its outcome. I end by suggesting that an older tradition of evidence in medicine—the plurality of reasoning strategies appealed to by the epidemiologist Sir Bradford Hill—is a superior strategy for assessing a large volume and diversity of evidence.",data/metaanalysis.pdf --> https://www.sciencedirect.com/science/article/pii/S1369848611000665,1106,"TITLE: 'Is meta-analysis the platinum standard of evidence?' ABSTRACT: 'An astonishing volume and diversity of evidence is available for many hypotheses in the biomedical and social sciences. Some of this evidence--usually from randomized controlled trials (RCTs)--is amalgamated by meta-analysis. Despite the ongoing debate regarding whether or not RCTs are the 'gold-standard' of evidence, it is usually meta-analysis which is considered the best source of evidence: meta-analysis is thought by many to be the platinum standard of evidence. However, I argue that meta-analysis falls far short of that standard. Different meta-analyses of the same evidence can reach contradictory conclusions. Meta-analysis fails to provide objective grounds for intersubjective assessments of hypotheses because numerous decisions must be made when performing a meta-analysis which allow wide latitude for subjective idiosyncrasies to influence its outcome. I end by suggesting that an older tradition of evidence in medicine--the plurality of reasoning strategies appealed to by the epidemiologist Sir Bradford Hill--is a superior strategy for assessing a large volume and diversity of evidence.' SOURCE: (stegenga, 2011)","stegenga, 2011"
49,balshem helfand schünemann oxman kunz brożek vist falck ytter meerpohl norris guyatt,GRADE guidelines: 3. Rating the quality of evidence.,2011,"This article introduces the approach of GRADE to rating quality of evidence. GRADE specifies four categories—high, moderate, low, and very low—that are applied to a body of evidence, not to individual studies. In the context of a systematic review, quality reflects our confidence that the estimates of the effect are correct. In the context of recommendations, quality reflects our confidence that the effect estimates are adequate to support a particular recommendation. Randomized trials begin as high-quality evidence, observational studies as low quality. “Quality” as used in GRADE means more than risk of bias and so may also be compromised by imprecision, inconsistency, indirectness of study results, and publication bias. In addition, several factors can increase our confidence in an estimate of effect. GRADE provides a systematic approach for considering and reporting each of these factors. GRADE separates the process of assessing quality of evidence from the process of making recommendations. Judgments about the strength of a recommendation depend on more than just the quality of evidence.",data/metaanalysis.pdf --> https://www.sciencedirect.com/science/article/pii/S089543561000332X,1108,"TITLE: 'GRADE guidelines: 3. Rating the quality of evidence.' ABSTRACT: 'This article introduces the approach of GRADE to rating quality of evidence. GRADE specifies four categories--high, moderate, low, and very low--that are applied to a body of evidence, not to individual studies. In the context of a systematic review, quality reflects our confidence that the estimates of the effect are correct. In the context of recommendations, quality reflects our confidence that the effect estimates are adequate to support a particular recommendation. Randomized trials begin as high-quality evidence, observational studies as low quality. ""Quality"" as used in GRADE means more than risk of bias and so may also be compromised by imprecision, inconsistency, indirectness of study results, and publication bias. In addition, several factors can increase our confidence in an estimate of effect. GRADE provides a systematic approach for considering and reporting each of these factors. GRADE separates the process of assessing quality of evidence from the process of making recommendations. Judgments about the strength of a recommendation depend on more than just the quality of evidence.' SOURCE: (balshem helfand schunemann oxman kunz brozek vist falck ytter meerpohl norris guyatt, 2011)","balshem helfand schunemann oxman kunz brozek vist falck ytter meerpohl norris g, 2011"
50,rad martingano ginges,Toward a psychology of Homo sapiens: Making psychological science more representative of the human population,2018,"Two primary goals of psychological science should be to understand what aspects of human psychology are universal and the way that context and culture produce variability. This requires that we take into account the importance of culture and context in the way that we write our papers and in the types of populations that we sample. However, most research published in our leading journals has relied on sampling WEIRD (Western, educated, industrialized, rich, and democratic) populations. One might expect that our scholarly work and editorial choices would by now reflect the knowledge that Western populations may not be representative of humans generally with respect to any given psychological phenomenon. However, as we show here, almost all research published by one of our leading journals, Psychological Science, relies on Western samples and uses these data in an unreflective way to make inferences about humans in general. To take us forward, we offer a set of concrete proposals for authors, journal editors, and reviewers that may lead to a psychological science that is more representative of the human condition.",data/metaanalysis.pdf,1129,"TITLE: 'Toward a psychology of Homo sapiens: Making psychological science more representative of the human population' ABSTRACT: 'Two primary goals of psychological science should be to understand what aspects of human psychology are universal and the way that context and culture produce variability. This requires that we take into account the importance of culture and context in the way that we write our papers and in the types of populations that we sample. However, most research published in our leading journals has relied on sampling WEIRD (Western, educated, industrialized, rich, and democratic) populations. One might expect that our scholarly work and editorial choices would by now reflect the knowledge that Western populations may not be representative of humans generally with respect to any given psychological phenomenon. However, as we show here, almost all research published by one of our leading journals, Psychological Science, relies on Western samples and uses these data in an unreflective way to make inferences about humans in general. To take us forward, we offer a set of concrete proposals for authors, journal editors, and reviewers that may lead to a psychological science that is more representative of the human condition.' SOURCE: (rad martingano ginges, 2018)","rad martingano ginges, 2018"
51,bartoš maier quintana wagenmakers,"Adjusting for Publication Bias in JASP and R: Selection Models, PET-PEESE, and Robust Bayesian Meta-Analysis",2022,"Meta-analyses are essential for cumulative science, but their validity can be compromised by publication bias. To mitigate the impact of publication bias, one may apply publication-bias-adjustment techniques such as precision-effect test and precision-effect estimate with standard errors (PET-PEESE) and selection models. These methods, implemented in JASP and R, allow researchers without programming experience to conduct state-of-the-art publication-bias-adjusted meta-analysis. In this tutorial, we demonstrate how to conduct a publication-bias-adjusted meta-analysis in JASP and R and interpret the results. First, we explain two frequentist bias-correction methods: PET-PEESE and selection models. Second, we introduce robust Bayesian meta-analysis, a Bayesian approach that simultaneously considers both PET-PEESE and selection models. We illustrate the methodology on an example data set, provide an instructional video (https://bit.ly/pubbias) and an R-markdown script (https://osf.io/uhaew/), and discuss the interpretation of the results. Finally, we include concrete guidance on reporting the meta-analytic results in an academic article.",data/metaanalysis.pdf,1151,"TITLE: 'Adjusting for Publication Bias in JASP and R: Selection Models, PET-PEESE, and Robust Bayesian Meta-Analysis' ABSTRACT: 'Meta-analyses are essential for cumulative science, but their validity can be compromised by publication bias. To mitigate the impact of publication bias, one may apply publication-bias-adjustment techniques such as precision-effect test and precision-effect estimate with standard errors (PET-PEESE) and selection models. These methods, implemented in JASP and R, allow researchers without programming experience to conduct state-of-the-art publication-bias-adjusted meta-analysis. In this tutorial, we demonstrate how to conduct a publication-bias-adjusted meta-analysis in JASP and R and interpret the results. First, we explain two frequentist bias-correction methods: PET-PEESE and selection models. Second, we introduce robust Bayesian meta-analysis, a Bayesian approach that simultaneously considers both PET-PEESE and selection models. We illustrate the methodology on an example data set, provide an instructional video (https://bit.ly/pubbias) and an R-markdown script (https://osf.io/uhaew/), and discuss the interpretation of the results. Finally, we include concrete guidance on reporting the meta-analytic results in an academic article.' SOURCE: (bartos maier quintana wagenmakers, 2022)","bartos maier quintana wagenmakers, 2022"
52,flake fried,Measurement Schmeasurement: Questionable Measurement Practices and How to Avoid Them,2020,"In this article, we define questionable measurement practices (QMPs) as decisions researchers make that raise doubts about the validity of the measures, and ultimately the validity of study conclusions. Doubts arise for a host of reasons, including a lack of transparency, ignorance, negligence, or misrepresentation of the evidence. We describe the scope of the problem and focus on how transparency is a part of the solution. A lack of measurement transparency makes it impossible to evaluate potential threats to internal, external, statistical-conclusion, and construct validity. We demonstrate that psychology is plagued by a measurement schmeasurement attitude: QMPs are common, hide a stunning source of researcher degrees of freedom, and pose a serious threat to cumulative psychological science, but are largely ignored. We address these challenges by providing a set of questions that researchers and consumers of scientific research can consider to identify and avoid QMPs. Transparent answers to these measurement questions promote rigorous research, allow for thorough evaluations of a study’s inferences, and are necessary for meaningful replication studies.",data/metaanalysis.pdf,1172,"TITLE: 'Measurement Schmeasurement: Questionable Measurement Practices and How to Avoid Them' ABSTRACT: 'In this article, we define questionable measurement practices (QMPs) as decisions researchers make that raise doubts about the validity of the measures, and ultimately the validity of study conclusions. Doubts arise for a host of reasons, including a lack of transparency, ignorance, negligence, or misrepresentation of the evidence. We describe the scope of the problem and focus on how transparency is a part of the solution. A lack of measurement transparency makes it impossible to evaluate potential threats to internal, external, statistical-conclusion, and construct validity. We demonstrate that psychology is plagued by a measurement schmeasurement attitude: QMPs are common, hide a stunning source of researcher degrees of freedom, and pose a serious threat to cumulative psychological science, but are largely ignored. We address these challenges by providing a set of questions that researchers and consumers of scientific research can consider to identify and avoid QMPs. Transparent answers to these measurement questions promote rigorous research, allow for thorough evaluations of a study's inferences, and are necessary for meaningful replication studies.' SOURCE: (flake fried, 2020)","flake fried, 2020"
53,ioannidis,Why Most Published Research Findings Are False,2005,"Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.",10.1080/09332480.2005.10722754,1177,"TITLE: 'Why Most Published Research Findings Are False' ABSTRACT: 'Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.' SOURCE: (ioannidis, 2005)","ioannidis, 2005"
54,quintana,From pre-registration to publication: a non-technical primer for conducting a meta-analysis to synthesize correlational data,2015,"Meta-analysis synthesizes a body of research investigating a common research question. Outcomes from meta-analyses provide a more objective and transparent summary of a research area than traditional narrative reviews. Moreover, they are often used to support research grant applications, guide clinical practice, and direct health policy. The aim of this article is to provide a practical and non-technical guide for psychological scientists that outlines the steps involved in planning and performing a meta-analysis of correlational datasets. I provide a supplementary R script to demonstrate each analytical step described in the paper, which is readily adaptable for researchers to use for their analyses. While the worked example is the analysis of a correlational dataset, the general meta-analytic process described in this paper is applicable for all types of effect sizes. I also emphasize the importance of meta-analysis protocols and pre-registration to improve transparency and help avoid unintended duplication. An improved understanding this tool will not only help scientists to conduct their own meta-analyses but also improve their evaluation of published meta-analyses.",data/metaanalysis.pdf,1188,"TITLE: 'From pre-registration to publication: a non-technical primer for conducting a meta-analysis to synthesize correlational data' ABSTRACT: 'Meta-analysis synthesizes a body of research investigating a common research question. Outcomes from meta-analyses provide a more objective and transparent summary of a research area than traditional narrative reviews. Moreover, they are often used to support research grant applications, guide clinical practice, and direct health policy. The aim of this article is to provide a practical and non-technical guide for psychological scientists that outlines the steps involved in planning and performing a meta-analysis of correlational datasets. I provide a supplementary R script to demonstrate each analytical step described in the paper, which is readily adaptable for researchers to use for their analyses. While the worked example is the analysis of a correlational dataset, the general meta-analytic process described in this paper is applicable for all types of effect sizes. I also emphasize the importance of meta-analysis protocols and pre-registration to improve transparency and help avoid unintended duplication. An improved understanding this tool will not only help scientists to conduct their own meta-analyses but also improve their evaluation of published meta-analyses.' SOURCE: (quintana, 2015)","quintana, 2015"
55,nuijten hartgerink assen epskamp wicherts,The prevalence of statistical reporting errors in psychology (1985–2013),2015,"This study documents reporting errors in a sample of over 250,000 p-values reported in eight major psychology journals from 1985 until 2013, using the new R package “statcheck.” statcheck retrieved null-hypothesis significance testing (NHST) results from over half of the articles from this period. In line with earlier research, we found that half of all published psychology papers that use NHST contained at least one p-value that was inconsistent with its test statistic and degrees of freedom. One in eight papers contained a grossly inconsistent p-value that may have affected the statistical conclusion. In contrast to earlier findings, we found that the average prevalence of inconsistent p-values has been stable over the years or has declined. The prevalence of gross inconsistencies was higher in p-values reported as significant than in p-values reported as nonsignificant. This could indicate a systematic bias in favor of significant results. Possible solutions for the high prevalence of reporting inconsistencies could be to encourage sharing data, to let co-authors check results in a so-called “co-pilot model,” and to use statcheck to flag possible inconsistencies in one’s own manuscript or during the review process.",data/metaanalysis.pdf --> https://link.springer.com/article/10.3758/s13428-015-0664-2,1237,"TITLE: 'The prevalence of statistical reporting errors in psychology (1985-2013)' ABSTRACT: 'This study documents reporting errors in a sample of over 250,000 p-values reported in eight major psychology journals from 1985 until 2013, using the new R package ""statcheck."" statcheck retrieved null-hypothesis significance testing (NHST) results from over half of the articles from this period. In line with earlier research, we found that half of all published psychology papers that use NHST contained at least one p-value that was inconsistent with its test statistic and degrees of freedom. One in eight papers contained a grossly inconsistent p-value that may have affected the statistical conclusion. In contrast to earlier findings, we found that the average prevalence of inconsistent p-values has been stable over the years or has declined. The prevalence of gross inconsistencies was higher in p-values reported as significant than in p-values reported as nonsignificant. This could indicate a systematic bias in favor of significant results. Possible solutions for the high prevalence of reporting inconsistencies could be to encourage sharing data, to let co-authors check results in a so-called ""co-pilot model,"" and to use statcheck to flag possible inconsistencies in one's own manuscript or during the review process.' SOURCE: (nuijten hartgerink assen epskamp wicherts, 2015)","nuijten hartgerink assen epskamp wicherts, 2015"
56,maier lakens,Justify Your Alpha: A Primer on Two Practical Approaches,2021,"The default use of an alpha level of .05 is suboptimal for two reasons. First, decisions based on data can be made more efficiently by choosing an alpha level that minimizes the combined Type 1 and Type 2 error rate. Second, it is possible that in studies with very high statistical power, p values lower than the alpha level can be more likely when the null hypothesis is true than when the alternative hypothesis is true (i.e., Lindley’s paradox). In this article, we explain two approaches that can be used to justify a better choice of an alpha level than relying on the default threshold of .05. The first approach is based on the idea to either minimize or balance Type 1 and Type 2 error rates. The second approach lowers the alpha level as a function of the sample size to prevent Lindley’s paradox. An R package and Shiny app are provided to perform the required calculations. Both approaches have their limitations (e.g., the challenge of specifying relative costs and priors) but can offer an improvement to current practices, especially when sample sizes are large. The use of alpha levels that are better justified should improve statistical inferences and can increase the efficiency and informativeness of scientific research.",data/metaanalysis.pdf,1241,"TITLE: 'Justify Your Alpha: A Primer on Two Practical Approaches' ABSTRACT: 'The default use of an alpha level of .05 is suboptimal for two reasons. First, decisions based on data can be made more efficiently by choosing an alpha level that minimizes the combined Type 1 and Type 2 error rate. Second, it is possible that in studies with very high statistical power, p values lower than the alpha level can be more likely when the null hypothesis is true than when the alternative hypothesis is true (i.e., Lindley's paradox). In this article, we explain two approaches that can be used to justify a better choice of an alpha level than relying on the default threshold of .05. The first approach is based on the idea to either minimize or balance Type 1 and Type 2 error rates. The second approach lowers the alpha level as a function of the sample size to prevent Lindley's paradox. An R package and Shiny app are provided to perform the required calculations. Both approaches have their limitations (e.g., the challenge of specifying relative costs and priors) but can offer an improvement to current practices, especially when sample sizes are large. The use of alpha levels that are better justified should improve statistical inferences and can increase the efficiency and informativeness of scientific research.' SOURCE: (maier lakens, 2021)","maier lakens, 2021"
57,randall ellis,"Questionable Research Practices, Low Statistical Power, and Other Obstacles to Replicability: Why Preclinical Neuroscience Research Would Benefit from Registered Reports",2022,"Abstract Replicability, the degree to which a previous scientific finding can be repeated in a distinct set of data, has been considered an integral component of institutionalized scientific practice since its inception several hundred years ago. In the past decade, large-scale replication studies have demonstrated that replicability is far from favorable, across multiple scientific fields. Here, I evaluate this literature and describe contributing factors including the prevalence of questionable research practices (QRPs), misunderstanding of p-values, and low statistical power. I subsequently discuss how these issues manifest specifically in preclinical neuroscience research. I conclude that these problems are multifaceted and difficult to solve, relying on the actions of early and late career researchers, funding sources, academic publishers, and others. I assert that any viable solution to the problem of substandard replicability must include changing academic incentives, with adoption of registered reports being the most immediately impactful and pragmatic strategy. For animal research in particular, comprehensive reporting guidelines that document potential sources of sensitivity for experimental outcomes is an essential addition.",10.1523/ENEURO.0017-22.2022,1255,"TITLE: 'Questionable Research Practices, Low Statistical Power, and Other Obstacles to Replicability: Why Preclinical Neuroscience Research Would Benefit from Registered Reports' ABSTRACT: 'Abstract Replicability, the degree to which a previous scientific finding can be repeated in a distinct set of data, has been considered an integral component of institutionalized scientific practice since its inception several hundred years ago. In the past decade, large-scale replication studies have demonstrated that replicability is far from favorable, across multiple scientific fields. Here, I evaluate this literature and describe contributing factors including the prevalence of questionable research practices (QRPs), misunderstanding of p-values, and low statistical power. I subsequently discuss how these issues manifest specifically in preclinical neuroscience research. I conclude that these problems are multifaceted and difficult to solve, relying on the actions of early and late career researchers, funding sources, academic publishers, and others. I assert that any viable solution to the problem of substandard replicability must include changing academic incentives, with adoption of registered reports being the most immediately impactful and pragmatic strategy. For animal research in particular, comprehensive reporting guidelines that document potential sources of sensitivity for experimental outcomes is an essential addition.' SOURCE: (randall ellis, 2022)","randall ellis, 2022"
58,michèle nuijten van assen hilde augusteijn elise crompvoets wicherts,"Effect Sizes, Power, and Biases in Intelligence Research: A Meta-Meta-Analysis",2020,"In this meta-study, we analyzed 2442 effect sizes from 131 meta-analyses in intelligence research, published from 1984 to 2014, to estimate the average effect size, median power, and evidence for bias. We found that the average effect size in intelligence research was a Pearson’s correlation of 0.26, and the median sample size was 60. Furthermore, across primary studies, we found a median power of 11.9% to detect a small effect, 54.5% to detect a medium effect, and 93.9% to detect a large effect. We documented differences in average effect size and median estimated power between different types of intelligence studies (correlational studies, studies of group differences, experiments, toxicology, and behavior genetics). On average, across all meta-analyses (but not in every meta-analysis), we found evidence for small-study effects, potentially indicating publication bias and overestimated effects. We found no differences in small-study effects between different study types. We also found no convincing evidence for the decline effect, US effect, or citation bias across meta-analyses. We concluded that intelligence research does show signs of low power and publication bias, but that these problems seem less severe than in many other scientific fields.",10.3390/jintelligence8040036,1268,"TITLE: 'Effect Sizes, Power, and Biases in Intelligence Research: A Meta-Meta-Analysis' ABSTRACT: 'In this meta-study, we analyzed 2442 effect sizes from 131 meta-analyses in intelligence research, published from 1984 to 2014, to estimate the average effect size, median power, and evidence for bias. We found that the average effect size in intelligence research was a Pearson's correlation of 0.26, and the median sample size was 60. Furthermore, across primary studies, we found a median power of 11.9% to detect a small effect, 54.5% to detect a medium effect, and 93.9% to detect a large effect. We documented differences in average effect size and median estimated power between different types of intelligence studies (correlational studies, studies of group differences, experiments, toxicology, and behavior genetics). On average, across all meta-analyses (but not in every meta-analysis), we found evidence for small-study effects, potentially indicating publication bias and overestimated effects. We found no differences in small-study effects between different study types. We also found no convincing evidence for the decline effect, US effect, or citation bias across meta-analyses. We concluded that intelligence research does show signs of low power and publication bias, but that these problems seem less severe than in many other scientific fields.' SOURCE: (michele nuijten van assen hilde augusteijn elise crompvoets wicherts, 2020)","michele nuijten van assen hilde augusteijn elise crompvoets wicherts, 2020"
59,jacobs viechtbauer,Estimation of the biserial correlation and its sampling variance for use in meta‐analysis,2017,"Meta‐analyses are often used to synthesize the findings of studies examining the correlational relationship between two continuous variables. When only dichotomous measurements are available for one of the two variables, the biserial correlation coefficient can be used to estimate the product–moment correlation between the two underlying continuous variables. Unlike the point‐biserial correlation coefficient, biserial correlation coefficients can therefore be integrated with product–moment correlation coefficients in the same meta‐analysis. The present article describes the estimation of the biserial correlation coefficient for meta‐analytic purposes and reports simulation results comparing different methods for estimating the coefficient's sampling variance. The findings indicate that commonly employed methods yield inconsistent estimates of the sampling variance across a broad range of research situations. In contrast, consistent estimates can be obtained using two methods that appear to be unknown in the meta‐analytic literature. A variance‐stabilizing transformation for the biserial correlation coefficient is described that allows for the construction of confidence intervals for individual coefficients with close to nominal coverage probabilities in most of the examined conditions. Copyright © 2016 John Wiley & Sons, Ltd.",data/metaanalysis.pdf,1347,"TITLE: 'Estimation of the biserial correlation and its sampling variance for use in meta-analysis' ABSTRACT: 'Meta-analyses are often used to synthesize the findings of studies examining the correlational relationship between two continuous variables. When only dichotomous measurements are available for one of the two variables, the biserial correlation coefficient can be used to estimate the product-moment correlation between the two underlying continuous variables. Unlike the point-biserial correlation coefficient, biserial correlation coefficients can therefore be integrated with product-moment correlation coefficients in the same meta-analysis. The present article describes the estimation of the biserial correlation coefficient for meta-analytic purposes and reports simulation results comparing different methods for estimating the coefficient's sampling variance. The findings indicate that commonly employed methods yield inconsistent estimates of the sampling variance across a broad range of research situations. In contrast, consistent estimates can be obtained using two methods that appear to be unknown in the meta-analytic literature. A variance-stabilizing transformation for the biserial correlation coefficient is described that allows for the construction of confidence intervals for individual coefficients with close to nominal coverage probabilities in most of the examined conditions. Copyright (c) 2016 John Wiley & Sons, Ltd.' SOURCE: (jacobs viechtbauer, 2017)","jacobs viechtbauer, 2017"
60,bakermans kranenburg ijzendoorn,Sniffing around oxytocin: review and meta-analyses of trials in healthy and clinical groups with implications for pharmacotherapy,2013,"The popularity of oxytocin (OT) has grown exponentially during the past decade, and so has the number of OT trials in healthy and clinical groups. We take stock of the evidence from these studies to explore potentials and limitations of pharmacotherapeutic applications. In healthy participants, intranasally administered OT leads to better emotion recognition and more trust in conspecifics, but the effects appear to be moderated by context (perceived threat of the ‘out-group’), personality and childhood experiences. In individuals with untoward childhood experiences, positive behavioral or neurobiological effects seem lowered or absent. In 19 clinical trials, covering autism, social anxiety, postnatal depression, obsessive-compulsive problems, schizophrenia, borderline personality disorder and post-traumatic stress, the effects of OT administration were tested, with doses ranging from 15 IU to more than 7000 IU. The combined effect size was d=0.32 (N=304; 95% confidence interval (CI): 0.18–0.47; P<0.01). However, of all disorders, only studies on autism spectrum disorder showed a significant combined effect size (d=0.57; N=68; 95% CI: 0.15–0.99; P<0.01). We hypothesize that for some of the other disorders, etiological factors rooted in negative childhood experiences may also have a role in the diminished effectiveness of treatment with OT.",data/metaanalysis.pdf --> https://www.nature.com/articles/tp201334,1360,"TITLE: 'Sniffing around oxytocin: review and meta-analyses of trials in healthy and clinical groups with implications for pharmacotherapy' ABSTRACT: 'The popularity of oxytocin (OT) has grown exponentially during the past decade, and so has the number of OT trials in healthy and clinical groups. We take stock of the evidence from these studies to explore potentials and limitations of pharmacotherapeutic applications. In healthy participants, intranasally administered OT leads to better emotion recognition and more trust in conspecifics, but the effects appear to be moderated by context (perceived threat of the 'out-group'), personality and childhood experiences. In individuals with untoward childhood experiences, positive behavioral or neurobiological effects seem lowered or absent. In 19 clinical trials, covering autism, social anxiety, postnatal depression, obsessive-compulsive problems, schizophrenia, borderline personality disorder and post-traumatic stress, the effects of OT administration were tested, with doses ranging from 15 IU to more than 7000 IU. The combined effect size was d=0.32 (N=304; 95% confidence interval (CI): 0.18-0.47; P<0.01). However, of all disorders, only studies on autism spectrum disorder showed a significant combined effect size (d=0.57; N=68; 95% CI: 0.15-0.99; P<0.01). We hypothesize that for some of the other disorders, etiological factors rooted in negative childhood experiences may also have a role in the diminished effectiveness of treatment with OT.' SOURCE: (bakermans kranenburg ijzendoorn, 2013)","bakermans kranenburg ijzendoorn, 2013"
61,schäfer schwarz,The Meaningfulness of Effect Sizes in Psychological Research: Differences Between Sub-Disciplines and the Impact of Potential Biases,2019,"Effect sizes are the currency of psychological research. They quantify the results of a study to answer the research question and are used to calculate statistical power. The interpretation of effect sizes—when is an effect small, medium, or large?—has been guided by the recommendations Jacob Cohen gave in his pioneering writings starting in 1962: Either compare an effect with the effects found in past research or use certain conventional benchmarks. The present analysis shows that neither of these recommendations is currently applicable. From past publications without pre-registration, 900 effects were randomly drawn and compared with 93 effects from publications with pre-registration, revealing a large difference: Effects from the former (median r = 0.36) were much larger than effects from the latter (median r = 0.16). That is, certain biases, such as publication bias or questionable research practices, have caused a dramatic inflation in published effects, making it difficult to compare an actual effect with the real population effects (as these are unknown). In addition, there were very large differences in the mean effects between psychological sub-disciplines and between different study designs, making it impossible to apply any global benchmarks. Many more pre-registered studies are needed in the future to derive a reliable picture of real population effects.",data/metaanalysis.pdf,1388,"TITLE: 'The Meaningfulness of Effect Sizes in Psychological Research: Differences Between Sub-Disciplines and the Impact of Potential Biases' ABSTRACT: 'Effect sizes are the currency of psychological research. They quantify the results of a study to answer the research question and are used to calculate statistical power. The interpretation of effect sizes--when is an effect small, medium, or large?--has been guided by the recommendations Jacob Cohen gave in his pioneering writings starting in 1962: Either compare an effect with the effects found in past research or use certain conventional benchmarks. The present analysis shows that neither of these recommendations is currently applicable. From past publications without pre-registration, 900 effects were randomly drawn and compared with 93 effects from publications with pre-registration, revealing a large difference: Effects from the former (median r = 0.36) were much larger than effects from the latter (median r = 0.16). That is, certain biases, such as publication bias or questionable research practices, have caused a dramatic inflation in published effects, making it difficult to compare an actual effect with the real population effects (as these are unknown). In addition, there were very large differences in the mean effects between psychological sub-disciplines and between different study designs, making it impossible to apply any global benchmarks. Many more pre-registered studies are needed in the future to derive a reliable picture of real population effects.' SOURCE: (schafer schwarz, 2019)","schafer schwarz, 2019"
62,walum waldman young,Statistical and Methodological Considerations for the Interpretation of Intranasal Oxytocin Studies,2016,"Over the last decade, oxytocin (OT) has received focus in numerous studies associating intranasal administration of this peptide with various aspects of human social behavior. These studies in humans are inspired by animal research, especially in rodents, showing that central manipulations of the OT system affect behavioral phenotypes related to social cognition, including parental behavior, social bonding, and individual recognition. Taken together, these studies in humans appear to provide compelling, but sometimes bewildering, evidence for the role of OT in influencing a vast array of complex social cognitive processes in humans. In this article, we investigate to what extent the human intranasal OT literature lends support to the hypothesis that intranasal OT consistently influences a wide spectrum of social behavior in humans. We do this by considering statistical features of studies within this field, including factors like statistical power, prestudy odds, and bias. Our conclusion is that intranasal OT studies are generally underpowered and that there is a high probability that most of the published intranasal OT findings do not represent true effects. Thus, the remarkable reports that intranasal OT influences a large number of human social behaviors should be viewed with healthy skepticism, and we make recommendations to improve the reliability of human OT studies in the future.",data/metaanalysis.pdf --> https://www.sciencedirect.com/science/article/pii/S0006322315005223,1409,"TITLE: 'Statistical and Methodological Considerations for the Interpretation of Intranasal Oxytocin Studies' ABSTRACT: 'Over the last decade, oxytocin (OT) has received focus in numerous studies associating intranasal administration of this peptide with various aspects of human social behavior. These studies in humans are inspired by animal research, especially in rodents, showing that central manipulations of the OT system affect behavioral phenotypes related to social cognition, including parental behavior, social bonding, and individual recognition. Taken together, these studies in humans appear to provide compelling, but sometimes bewildering, evidence for the role of OT in influencing a vast array of complex social cognitive processes in humans. In this article, we investigate to what extent the human intranasal OT literature lends support to the hypothesis that intranasal OT consistently influences a wide spectrum of social behavior in humans. We do this by considering statistical features of studies within this field, including factors like statistical power, prestudy odds, and bias. Our conclusion is that intranasal OT studies are generally underpowered and that there is a high probability that most of the published intranasal OT findings do not represent true effects. Thus, the remarkable reports that intranasal OT influences a large number of human social behaviors should be viewed with healthy skepticism, and we make recommendations to improve the reliability of human OT studies in the future.' SOURCE: (walum waldman young, 2016)","walum waldman young, 2016"
63,brunner schimmack,Estimating Population Mean Power Under Conditions of Heterogeneity and Selection for Significance,2020,"In scientific fields that use significance tests, statistical power is important for successful replications of significant results because it is the long-run success rate in a series of exact replication studies. For any population of published results, there is a population of power values of the statistical tests on which conclusions are based. We give exact theoretical results showing how selection for significance affects the distribution of statistical power in a heterogeneous population of significance tests. In a set of large-scale simulation studies, we compare four methods for estimating population mean power of a set of studies selected for significance (a maximum likelihood model, extensions of p-curve and p-uniform, & z-curve). The p-uniform and p-curve methods performed well with a fixed effects size and varying sample sizes. However, when there was substantial variability in effect sizes as well as sample sizes, both methods systematically overestimate mean power. When the assumptions of the maximum likelihood were satisfied, it produced the most accurate estimates for heterogeneity in effect sizes, but z-curve produced more accurate estimates when the assumptions of the maximum likelihood model were not met. We recommend the use of zcurve to estimate the typical power of significant results, which has implications for the replicability of significant results in psychology journals.",data/metaanalysis.pdf,1420,"TITLE: 'Estimating Population Mean Power Under Conditions of Heterogeneity and Selection for Significance' ABSTRACT: 'In scientific fields that use significance tests, statistical power is important for successful replications of significant results because it is the long-run success rate in a series of exact replication studies. For any population of published results, there is a population of power values of the statistical tests on which conclusions are based. We give exact theoretical results showing how selection for significance affects the distribution of statistical power in a heterogeneous population of significance tests. In a set of large-scale simulation studies, we compare four methods for estimating population mean power of a set of studies selected for significance (a maximum likelihood model, extensions of p-curve and p-uniform, & z-curve). The p-uniform and p-curve methods performed well with a fixed effects size and varying sample sizes. However, when there was substantial variability in effect sizes as well as sample sizes, both methods systematically overestimate mean power. When the assumptions of the maximum likelihood were satisfied, it produced the most accurate estimates for heterogeneity in effect sizes, but z-curve produced more accurate estimates when the assumptions of the maximum likelihood model were not met. We recommend the use of zcurve to estimate the typical power of significant results, which has implications for the replicability of significant results in psychology journals.' SOURCE: (brunner schimmack, 2020)","brunner schimmack, 2020"
64,van assen robbie van aert wicherts,Meta-analysis using effect size distributions of only statistically significant studies.,2015,"Publication bias threatens the validity of meta-analytic results and leads to overestimation of the effect size in traditional meta-analysis. This particularly applies to meta-analyses that feature small studies, which are ubiquitous in psychology. Here we develop a new method for meta-analysis that deals with publication bias. This method, p-uniform, enables (a) testing of publication bias, (b) effect size estimation, and (c) testing of the null-hypothesis of no effect. No current method for meta-analysis possesses all 3 qualities. Application of p-uniform is straightforward because no additional data on missing studies are needed and no sophisticated assumptions or choices need to be made before applying it. Simulations show that p-uniform generally outperforms the trim-and-fill method and the test of excess significance (TES; Ioannidis & Trikalinos, 2007b) if publication bias exists and population effect size is homogenous or heterogeneity is slight. For illustration, p-uniform and other publication bias analyses are applied to the meta-analysis of McCall and Carriger (1993) examining the association between infants' habituation to a stimulus and their later cognitive ability (IQ). We conclude that p-uniform is a valuable technique for examining publication bias and estimating population effects in fixed-effect meta-analyses, and as sensitivity analysis to draw inferences about publication bias.",10.1037/met0000025,1421,"TITLE: 'Meta-analysis using effect size distributions of only statistically significant studies.' ABSTRACT: 'Publication bias threatens the validity of meta-analytic results and leads to overestimation of the effect size in traditional meta-analysis. This particularly applies to meta-analyses that feature small studies, which are ubiquitous in psychology. Here we develop a new method for meta-analysis that deals with publication bias. This method, p-uniform, enables (a) testing of publication bias, (b) effect size estimation, and (c) testing of the null-hypothesis of no effect. No current method for meta-analysis possesses all 3 qualities. Application of p-uniform is straightforward because no additional data on missing studies are needed and no sophisticated assumptions or choices need to be made before applying it. Simulations show that p-uniform generally outperforms the trim-and-fill method and the test of excess significance (TES; Ioannidis & Trikalinos, 2007b) if publication bias exists and population effect size is homogenous or heterogeneity is slight. For illustration, p-uniform and other publication bias analyses are applied to the meta-analysis of McCall and Carriger (1993) examining the association between infants' habituation to a stimulus and their later cognitive ability (IQ). We conclude that p-uniform is a valuable technique for examining publication bias and estimating population effects in fixed-effect meta-analyses, and as sensitivity analysis to draw inferences about publication bias.' SOURCE: (van assen robbie van aert wicherts, 2015)","van assen robbie van aert wicherts, 2015"
65,higgins thompson,Quantifying heterogeneity in a meta‐analysis,2002,"The extent of heterogeneity in a meta‐analysis partly determines the difficulty in drawing overall conclusions. This extent may be measured by estimating a between‐study variance, but interpretation is then specific to a particular treatment effect metric. A test for the existence of heterogeneity exists, but depends on the number of studies in the meta‐analysis. We develop measures of the impact of heterogeneity on a meta‐analysis, from mathematical criteria, that are independent of the number of studies and the treatment effect metric. We derive and propose three suitable statistics: H is the square root of the χ2 heterogeneity statistic divided by its degrees of freedom; R is the ratio of the standard error of the underlying mean from a random effects meta‐analysis to the standard error of a fixed effect meta‐analytic estimate, and I2 is a transformation of H that describes the proportion of total variation in study estimates that is due to heterogeneity. We discuss interpretation, interval estimates and other properties of these measures and examine them in five example data sets showing different amounts of heterogeneity. We conclude that H and I2, which can usually be calculated for published meta‐analyses, are particularly useful summaries of the impact of heterogeneity. One or both should be presented in published meta‐analyses in preference to the test for heterogeneity. Copyright © 2002 John Wiley & Sons, Ltd.",data/metaanalysis.pdf,1443,"TITLE: 'Quantifying heterogeneity in a meta-analysis' ABSTRACT: 'The extent of heterogeneity in a meta-analysis partly determines the difficulty in drawing overall conclusions. This extent may be measured by estimating a between-study variance, but interpretation is then specific to a particular treatment effect metric. A test for the existence of heterogeneity exists, but depends on the number of studies in the meta-analysis. We develop measures of the impact of heterogeneity on a meta-analysis, from mathematical criteria, that are independent of the number of studies and the treatment effect metric. We derive and propose three suitable statistics: H is the square root of the kh2 heterogeneity statistic divided by its degrees of freedom; R is the ratio of the standard error of the underlying mean from a random effects meta-analysis to the standard error of a fixed effect meta-analytic estimate, and I2 is a transformation of H that describes the proportion of total variation in study estimates that is due to heterogeneity. We discuss interpretation, interval estimates and other properties of these measures and examine them in five example data sets showing different amounts of heterogeneity. We conclude that H and I2, which can usually be calculated for published meta-analyses, are particularly useful summaries of the impact of heterogeneity. One or both should be presented in published meta-analyses in preference to the test for heterogeneity. Copyright (c) 2002 John Wiley & Sons, Ltd.' SOURCE: (higgins thompson, 2002)","higgins thompson, 2002"
66,hagger,Meta-analysis,2022,"ABSTRACT The sheer volume of available research and shifts toward evidence-based practice has led researchers and practitioners in sport and exercise psychology to rely increasingly on meta-analyses to summarize current knowledge, provide future research directions, and inform policy and practice. These issues highlight the imperative of precision and integrity in the conduct of meta-analyses in the discipline. This review provides a summary of meta-analytic methods relevant to sport and exercise psychology, identifies important issues and advances in meta-analytic methods, and provides best practice guidelines for meta-analysts to consider when synthesizing research in the discipline. In Part I, I provide an overview of the basic principles of meta-analysis and direct readers to accessible, non-technical treatments of the topic. In Part II, I introduce several key issues in meta-analysis and summarize the latest advances in each: effective assessment of heterogeneity; testing for moderators; dealing with dependency; evaluating publication bias and tracking down ‘fugitive literature’; and assessing sample size in meta-analysis. I also cover two emerging topics: testing theories using meta-analysis and open science and transparency practices in meta-analysis. I conclude the discussion of each issue by providing best practice guidelines, and refer the reader to further accessible texts to augment knowledge and understanding.",10.1080/1750984X.2021.1966824,1446,"TITLE: 'Meta-analysis' ABSTRACT: 'ABSTRACT The sheer volume of available research and shifts toward evidence-based practice has led researchers and practitioners in sport and exercise psychology to rely increasingly on meta-analyses to summarize current knowledge, provide future research directions, and inform policy and practice. These issues highlight the imperative of precision and integrity in the conduct of meta-analyses in the discipline. This review provides a summary of meta-analytic methods relevant to sport and exercise psychology, identifies important issues and advances in meta-analytic methods, and provides best practice guidelines for meta-analysts to consider when synthesizing research in the discipline. In Part I, I provide an overview of the basic principles of meta-analysis and direct readers to accessible, non-technical treatments of the topic. In Part II, I introduce several key issues in meta-analysis and summarize the latest advances in each: effective assessment of heterogeneity; testing for moderators; dealing with dependency; evaluating publication bias and tracking down 'fugitive literature'; and assessing sample size in meta-analysis. I also cover two emerging topics: testing theories using meta-analysis and open science and transparency practices in meta-analysis. I conclude the discussion of each issue by providing best practice guidelines, and refer the reader to further accessible texts to augment knowledge and understanding.' SOURCE: (hagger, 2022)","hagger, 2022"
67,linden hönekopp,Heterogeneity of Research Results: A New Perspective From Which to Assess and Promote Progress in Psychological Science,2021,"Heterogeneity emerges when multiple close or conceptual replications on the same subject produce results that vary more than expected from the sampling error. Here we argue that unexplained heterogeneity reflects a lack of coherence between the concepts applied and data observed and therefore a lack of understanding of the subject matter. Typical levels of heterogeneity thus offer a useful but neglected perspective on the levels of understanding achieved in psychological science. Focusing on continuous outcome variables, we surveyed heterogeneity in 150 meta-analyses from cognitive, organizational, and social psychology and 57 multiple close replications. Heterogeneity proved to be very high in meta-analyses, with powerful moderators being conspicuously absent. Population effects in the average meta-analysis vary from small to very large for reasons that are typically not understood. In contrast, heterogeneity was moderate in close replications. A newly identified relationship between heterogeneity and effect size allowed us to make predictions about expected heterogeneity levels. We discuss important implications for the formulation and evaluation of theories in psychology. On the basis of insights from the history and philosophy of science, we argue that the reduction of heterogeneity is important for progress in psychology and its practical applications, and we suggest changes to our collective research practice toward this end.",data/metaanalysis.pdf,1455,"TITLE: 'Heterogeneity of Research Results: A New Perspective From Which to Assess and Promote Progress in Psychological Science' ABSTRACT: 'Heterogeneity emerges when multiple close or conceptual replications on the same subject produce results that vary more than expected from the sampling error. Here we argue that unexplained heterogeneity reflects a lack of coherence between the concepts applied and data observed and therefore a lack of understanding of the subject matter. Typical levels of heterogeneity thus offer a useful but neglected perspective on the levels of understanding achieved in psychological science. Focusing on continuous outcome variables, we surveyed heterogeneity in 150 meta-analyses from cognitive, organizational, and social psychology and 57 multiple close replications. Heterogeneity proved to be very high in meta-analyses, with powerful moderators being conspicuously absent. Population effects in the average meta-analysis vary from small to very large for reasons that are typically not understood. In contrast, heterogeneity was moderate in close replications. A newly identified relationship between heterogeneity and effect size allowed us to make predictions about expected heterogeneity levels. We discuss important implications for the formulation and evaluation of theories in psychology. On the basis of insights from the history and philosophy of science, we argue that the reduction of heterogeneity is important for progress in psychology and its practical applications, and we suggest changes to our collective research practice toward this end.' SOURCE: (linden honekopp, 2021)","linden honekopp, 2021"
68,evan carter felix schönbrodt will gervais hilgard,Correcting for Bias in Psychology: A Comparison of Meta-Analytic Methods,2019,"Publication bias and questionable research practices in primary research can lead to badly overestimated effects in meta-analysis. Methodologists have proposed a variety of statistical approaches to correct for such overestimation. However, it is not clear which methods work best for data typically seen in psychology. Here, we present a comprehensive simulation study in which we examined how some of the most promising meta-analytic methods perform on data that might realistically be produced by research in psychology. We simulated several levels of questionable research practices, publication bias, and heterogeneity, and used study sample sizes empirically derived from the literature. Our results clearly indicated that no single meta-analytic method consistently outperformed all the others. Therefore, we recommend that meta-analysts in psychology focus on sensitivity analyses—that is, report on a variety of methods, consider the conditions under which these methods fail (as indicated by simulation studies such as ours), and then report how conclusions might change depending on which conditions are most plausible. Moreover, given the dependence of meta-analytic methods on untestable assumptions, we strongly recommend that researchers in psychology continue their efforts to improve the primary literature and conduct large-scale, preregistered replications. We provide detailed results and simulation code at https://osf.io/rf3ys and interactive figures at http://www.shinyapps.org/apps/metaExplorer/.",10.1177/2515245919847196,1520,"TITLE: 'Correcting for Bias in Psychology: A Comparison of Meta-Analytic Methods' ABSTRACT: 'Publication bias and questionable research practices in primary research can lead to badly overestimated effects in meta-analysis. Methodologists have proposed a variety of statistical approaches to correct for such overestimation. However, it is not clear which methods work best for data typically seen in psychology. Here, we present a comprehensive simulation study in which we examined how some of the most promising meta-analytic methods perform on data that might realistically be produced by research in psychology. We simulated several levels of questionable research practices, publication bias, and heterogeneity, and used study sample sizes empirically derived from the literature. Our results clearly indicated that no single meta-analytic method consistently outperformed all the others. Therefore, we recommend that meta-analysts in psychology focus on sensitivity analyses--that is, report on a variety of methods, consider the conditions under which these methods fail (as indicated by simulation studies such as ours), and then report how conclusions might change depending on which conditions are most plausible. Moreover, given the dependence of meta-analytic methods on untestable assumptions, we strongly recommend that researchers in psychology continue their efforts to improve the primary literature and conduct large-scale, preregistered replications. We provide detailed results and simulation code at https://osf.io/rf3ys and interactive figures at http://www.shinyapps.org/apps/metaExplorer/.' SOURCE: (evan carter felix schonbrodt will gervais hilgard, 2019)","evan carter felix schonbrodt will gervais hilgard, 2019"
69,daniel quintana,A Guide for Calculating Study-Level Statistical Power for Meta-Analyses,2023,"Meta-analysis is a popular approach in the psychological sciences for synthesizing data across studies. However, the credibility of meta-analysis outcomes depends on the evidential value of studies included in the body of evidence used for data synthesis. One important consideration for determining a study’s evidential value is the statistical power of the study’s design/statistical test combination for detecting hypothetical effect sizes of interest. Studies with a design/test combination that cannot reliably detect a wide range of effect sizes are more susceptible to questionable research practices and exaggerated effect sizes. Therefore, determining the statistical power for design/test combinations for studies included in meta-analyses can help researchers make decisions regarding confidence in the body of evidence. Because the one true population effect size is unknown when hypothesis testing, an alternative approach is to determine statistical power for a range of hypothetical effect sizes. This tutorial introduces the metameta R package and web app, which facilitates the straightforward calculation and visualization of study-level statistical power in meta-analyses for a range of hypothetical effect sizes. Readers will be shown how to reanalyze data using information typically presented in meta-analysis forest plots or tables and how to integrate the metameta package when reporting novel meta-analyses. A step-by-step companion screencast video tutorial is also provided to assist readers using the R package.",10.1177/25152459221147260,1539,"TITLE: 'A Guide for Calculating Study-Level Statistical Power for Meta-Analyses' ABSTRACT: 'Meta-analysis is a popular approach in the psychological sciences for synthesizing data across studies. However, the credibility of meta-analysis outcomes depends on the evidential value of studies included in the body of evidence used for data synthesis. One important consideration for determining a study's evidential value is the statistical power of the study's design/statistical test combination for detecting hypothetical effect sizes of interest. Studies with a design/test combination that cannot reliably detect a wide range of effect sizes are more susceptible to questionable research practices and exaggerated effect sizes. Therefore, determining the statistical power for design/test combinations for studies included in meta-analyses can help researchers make decisions regarding confidence in the body of evidence. Because the one true population effect size is unknown when hypothesis testing, an alternative approach is to determine statistical power for a range of hypothetical effect sizes. This tutorial introduces the metameta R package and web app, which facilitates the straightforward calculation and visualization of study-level statistical power in meta-analyses for a range of hypothetical effect sizes. Readers will be shown how to reanalyze data using information typically presented in meta-analysis forest plots or tables and how to integrate the metameta package when reporting novel meta-analyses. A step-by-step companion screencast video tutorial is also provided to assist readers using the R package.' SOURCE: (daniel quintana, 2023)","daniel quintana, 2023"
70,brydges,"Effect Size Guidelines, Sample Size Calculations, and Statistical Power in Gerontology",2019,"Abstract Background and Objectives Researchers typically use Cohen’s guidelines of Pearson’s r = .10, .30, and .50, and Cohen’s d = 0.20, 0.50, and 0.80 to interpret observed effect sizes as small, medium, or large, respectively. However, these guidelines were not based on quantitative estimates and are only recommended if field-specific estimates are unknown. This study investigated the distribution of effect sizes in both individual differences research and group differences research in gerontology to provide estimates of effect sizes in the field. Research Design and Methods Effect sizes (Pearson’s r, Cohen’s d, and Hedges’ g) were extracted from meta-analyses published in 10 top-ranked gerontology journals. The 25th, 50th, and 75th percentile ranks were calculated for Pearson’s r (individual differences) and Cohen’s d or Hedges’ g (group differences) values as indicators of small, medium, and large effects. A priori power analyses were conducted for sample size calculations given the observed effect size estimates. Results Effect sizes of Pearson’s r = .12, .20, and .32 for individual differences research and Hedges’ g = 0.16, 0.38, and 0.76 for group differences research were interpreted as small, medium, and large effects in gerontology. Discussion and Implications Cohen’s guidelines appear to overestimate effect sizes in gerontology. Researchers are encouraged to use Pearson’s r = .10, .20, and .30, and Cohen’s d or Hedges’ g = 0.15, 0.40, and 0.75 to interpret small, medium, and large effects in gerontology, and recruit larger samples.",10.1093/geroni/igz036,1569,"TITLE: 'Effect Size Guidelines, Sample Size Calculations, and Statistical Power in Gerontology' ABSTRACT: 'Abstract Background and Objectives Researchers typically use Cohen's guidelines of Pearson's r = .10, .30, and .50, and Cohen's d = 0.20, 0.50, and 0.80 to interpret observed effect sizes as small, medium, or large, respectively. However, these guidelines were not based on quantitative estimates and are only recommended if field-specific estimates are unknown. This study investigated the distribution of effect sizes in both individual differences research and group differences research in gerontology to provide estimates of effect sizes in the field. Research Design and Methods Effect sizes (Pearson's r, Cohen's d, and Hedges' g) were extracted from meta-analyses published in 10 top-ranked gerontology journals. The 25th, 50th, and 75th percentile ranks were calculated for Pearson's r (individual differences) and Cohen's d or Hedges' g (group differences) values as indicators of small, medium, and large effects. A priori power analyses were conducted for sample size calculations given the observed effect size estimates. Results Effect sizes of Pearson's r = .12, .20, and .32 for individual differences research and Hedges' g = 0.16, 0.38, and 0.76 for group differences research were interpreted as small, medium, and large effects in gerontology. Discussion and Implications Cohen's guidelines appear to overestimate effect sizes in gerontology. Researchers are encouraged to use Pearson's r = .10, .20, and .30, and Cohen's d or Hedges' g = 0.15, 0.40, and 0.75 to interpret small, medium, and large effects in gerontology, and recruit larger samples.' SOURCE: (brydges, 2019)","brydges, 2019"
71,lakens,Sample Size Justification,2021,"An important step when designing a study is to justify the sample size that will be collected. The key aim of a sample size justification is to explain how the collected data is expected to provide valuable information given the inferential goals of the researcher. In this overview article six approaches are discussed to justify the sample size in a quantitative empirical study: 1) collecting data from (an)almost) the entire population, 2) choosing a sample size based on resource constraints, 3) performing an a-priori power analysis, 4) planning for a desired accuracy, 5) using heuristics, or 6) explicitly acknowledging the absence of a justification. An important question to consider when justifying sample sizes is which effect sizes are deemed interesting, and the extent to which the data that is collected informs inferences about these effect sizes. Depending on the sample size justification chosen, researchers could consider 1) what the smallest effect size of interest is, 2) which minimal effect size will be statistically significant, 3) which effect sizes they expect (and what they base these expectations on), 4) which effect sizes would be rejected based on a confidence interval around the effect size, 5) which ranges of effects a study has sufficient power to detect based on a sensitivity power analysis, and 6) which effect sizes are plausible in a specific research area. Researchers can use the guidelines presented in this article to improve their sample size justification, and hopefully, align the informational value of a study with their inferential goals.",10.31234/osf.io/9d3yf,1593,"TITLE: 'Sample Size Justification' ABSTRACT: 'An important step when designing a study is to justify the sample size that will be collected. The key aim of a sample size justification is to explain how the collected data is expected to provide valuable information given the inferential goals of the researcher. In this overview article six approaches are discussed to justify the sample size in a quantitative empirical study: 1) collecting data from (an)almost) the entire population, 2) choosing a sample size based on resource constraints, 3) performing an a-priori power analysis, 4) planning for a desired accuracy, 5) using heuristics, or 6) explicitly acknowledging the absence of a justification. An important question to consider when justifying sample sizes is which effect sizes are deemed interesting, and the extent to which the data that is collected informs inferences about these effect sizes. Depending on the sample size justification chosen, researchers could consider 1) what the smallest effect size of interest is, 2) which minimal effect size will be statistically significant, 3) which effect sizes they expect (and what they base these expectations on), 4) which effect sizes would be rejected based on a confidence interval around the effect size, 5) which ranges of effects a study has sufficient power to detect based on a sensitivity power analysis, and 6) which effect sizes are plausible in a specific research area. Researchers can use the guidelines presented in this article to improve their sample size justification, and hopefully, align the informational value of a study with their inferential goals.' SOURCE: (lakens, 2021)","lakens, 2021"
72,maier vanderweele mathur,Using selection models to assess sensitivity to publication bias: A tutorial and call for more routine use,2022,"Abstract In meta‐analyses, it is critical to assess the extent to which publication bias might have compromised the results. Classical methods based on the funnel plot, including Egger's test and Trim‐and‐Fill, have become the de facto default methods to do so, with a large majority of recent meta‐analyses in top medical journals (85%) assessing for publication bias exclusively using these methods. However, these classical funnel plot methods have important limitations when used as the sole means of assessing publication bias: they essentially assume that the publication process favors large point estimates for small studies and does not affect the largest studies, and they can perform poorly when effects are heterogeneous. In light of these limitations, we recommend that meta‐analyses routinely apply other publication bias methods in addition to or instead of classical funnel plot methods. To this end, we describe how to use and interpret selection models. These methods make the often more realistic assumption that publication bias favors “statistically significant” results, and the methods also directly accommodate effect heterogeneity. Selection models have been established for decades in the statistics literature and are supported by user‐friendly software, yet remain rarely reported in many disciplines. We use a previously published meta‐analysis to demonstrate that selection models can yield insights that extend beyond those provided by funnel plot methods, suggesting the importance of establishing more comprehensive reporting practices for publication bias assessment.",data/metaanalysis.pdf,1601,"TITLE: 'Using selection models to assess sensitivity to publication bias: A tutorial and call for more routine use' ABSTRACT: 'Abstract In meta-analyses, it is critical to assess the extent to which publication bias might have compromised the results. Classical methods based on the funnel plot, including Egger's test and Trim-and-Fill, have become the de facto default methods to do so, with a large majority of recent meta-analyses in top medical journals (85%) assessing for publication bias exclusively using these methods. However, these classical funnel plot methods have important limitations when used as the sole means of assessing publication bias: they essentially assume that the publication process favors large point estimates for small studies and does not affect the largest studies, and they can perform poorly when effects are heterogeneous. In light of these limitations, we recommend that meta-analyses routinely apply other publication bias methods in addition to or instead of classical funnel plot methods. To this end, we describe how to use and interpret selection models. These methods make the often more realistic assumption that publication bias favors ""statistically significant"" results, and the methods also directly accommodate effect heterogeneity. Selection models have been established for decades in the statistics literature and are supported by user-friendly software, yet remain rarely reported in many disciplines. We use a previously published meta-analysis to demonstrate that selection models can yield insights that extend beyond those provided by funnel plot methods, suggesting the importance of establishing more comprehensive reporting practices for publication bias assessment.' SOURCE: (maier vanderweele mathur, 2022)","maier vanderweele mathur, 2022"
73,boen quintana ladouceur tamnes,Age‐related differences in the error‐related negativity and error positivity in children and adolescents are moderated by sample and methodological characteristics: A meta‐analysis,2021,"Abstract The error‐related negativity (ERN) and the error positivity (Pe) are electrophysiological components associated with error processing that are thought to exhibit distinctive developmental trajectories from childhood to adulthood. To investigate the age and age moderation effects on the ERN and the Pe strength during development, we conducted a preregistered three‐level meta‐analysis synthesizing 120 and 41 effect sizes across 18 group comparison studies and 19 correlational studies, respectively. The meta‐analysis included studies with mean age between 3.6 and 28.7 (min‐max age range: 3.5 and 49.8) years for age‐group comparisons and 6.1 to 18.7 (min‐max age range: 4.0–35.7) years for age correlations. Results showed that age was associated with a more negative ERN (SMD = −.433, r = −.230). No statistically significant association between age and the Pe was found (SMD = .059, r = −.091), except for in a group comparison between younger and older adolescents. The age effects were not significantly moderated by whether a Flanker or a Go/No‐Go task was used, whereas a probabilistic learning task moderated the age effect on the Pe. Moreover, the Fz and Cz electrode sites yielded stronger negative associations between age and the ERN and the Pe, respectively. The results confirm that the ERN and the Pe show differential development courses and suggest that sample and methodological characteristics influence the age effects, and lay the foundation for investigations of developmental patterns of the ERN and the Pe in relation to psychopathology and early genetic and environmental risk factors.",data/metaanalysis.pdf,1622,"TITLE: 'Age-related differences in the error-related negativity and error positivity in children and adolescents are moderated by sample and methodological characteristics: A meta-analysis' ABSTRACT: 'Abstract The error-related negativity (ERN) and the error positivity (Pe) are electrophysiological components associated with error processing that are thought to exhibit distinctive developmental trajectories from childhood to adulthood. To investigate the age and age moderation effects on the ERN and the Pe strength during development, we conducted a preregistered three-level meta-analysis synthesizing 120 and 41 effect sizes across 18 group comparison studies and 19 correlational studies, respectively. The meta-analysis included studies with mean age between 3.6 and 28.7 (min-max age range: 3.5 and 49.8) years for age-group comparisons and 6.1 to 18.7 (min-max age range: 4.0-35.7) years for age correlations. Results showed that age was associated with a more negative ERN (SMD = -.433, r = -.230). No statistically significant association between age and the Pe was found (SMD = .059, r = -.091), except for in a group comparison between younger and older adolescents. The age effects were not significantly moderated by whether a Flanker or a Go/No-Go task was used, whereas a probabilistic learning task moderated the age effect on the Pe. Moreover, the Fz and Cz electrode sites yielded stronger negative associations between age and the ERN and the Pe, respectively. The results confirm that the ERN and the Pe show differential development courses and suggest that sample and methodological characteristics influence the age effects, and lay the foundation for investigations of developmental patterns of the ERN and the Pe in relation to psychopathology and early genetic and environmental risk factors.' SOURCE: (boen quintana ladouceur tamnes, 2021)","boen quintana ladouceur tamnes, 2021"
74,quintana,Statistical considerations for reporting and planning heart rate variability case-control studies.,2017,"The calculation of heart rate variability (HRV) is a popular tool used to investigate differences in cardiac autonomic control between population samples. When interpreting effect sizes to quantify the magnitude of group differences, researchers typically use Cohen's guidelines of small (0.2), medium (0.5), and large (0.8) effects. However, these guidelines were originally proposed as a fallback for when the effect size distribution (ESD) was unknown. Despite the availability of effect sizes from hundreds of HRV studies, researchers still largely rely on Cohen's guidelines to interpret effect sizes and to perform power analyses to calculate required sample sizes for future research. This article describes an ESD analysis of 297 HRV effect sizes from between-group/case-control studies, revealing that the 25th, 50th, and 75th effect size percentiles correspond with effect sizes of 0.26, 0.51, and 0.88, respectively. The analyses suggest that Cohen's guidelines may underestimate the magnitude of small and large effect sizes and that HRV studies are generally underpowered. Therefore, to better reflect the observed ESD, effect sizes of 0.25, 0.5, and 0.9 should be interpreted as small, medium, and large effects (after rounding to the closest 0.05). Based on power calculations using the ESD, suggested sample sizes are also provided for planning suitably powered studies that are more likely to replicate. Researchers are encouraged to use the ESD data set or their own collected data sets in tandem with the provided analysis script to perform custom ESD and power analyses relevant to their specific research area.",data/metaanalysis.pdf,1631,"TITLE: 'Statistical considerations for reporting and planning heart rate variability case-control studies.' ABSTRACT: 'The calculation of heart rate variability (HRV) is a popular tool used to investigate differences in cardiac autonomic control between population samples. When interpreting effect sizes to quantify the magnitude of group differences, researchers typically use Cohen's guidelines of small (0.2), medium (0.5), and large (0.8) effects. However, these guidelines were originally proposed as a fallback for when the effect size distribution (ESD) was unknown. Despite the availability of effect sizes from hundreds of HRV studies, researchers still largely rely on Cohen's guidelines to interpret effect sizes and to perform power analyses to calculate required sample sizes for future research. This article describes an ESD analysis of 297 HRV effect sizes from between-group/case-control studies, revealing that the 25th, 50th, and 75th effect size percentiles correspond with effect sizes of 0.26, 0.51, and 0.88, respectively. The analyses suggest that Cohen's guidelines may underestimate the magnitude of small and large effect sizes and that HRV studies are generally underpowered. Therefore, to better reflect the observed ESD, effect sizes of 0.25, 0.5, and 0.9 should be interpreted as small, medium, and large effects (after rounding to the closest 0.05). Based on power calculations using the ESD, suggested sample sizes are also provided for planning suitably powered studies that are more likely to replicate. Researchers are encouraged to use the ESD data set or their own collected data sets in tandem with the provided analysis script to perform custom ESD and power analyses relevant to their specific research area.' SOURCE: (quintana, 2017)","quintana, 2017"
75,rochefort maranda,Inflated effect sizes and underpowered tests: how the severity measure of evidence is affected by the winner’s curse,2020,"My aim in this paper is to show how the problem of inflated effect sizes (the Winner’s Curse) corrupts the severity measure of evidence. This has never been done. In fact, the Winner’s Curse is barely mentioned in the philosophical literature. Since the severity score is the predominant measure of evidence for frequentist tests in the philosophical literature, it is important to underscore its flaws. It is also crucial to bring the philosophical literature up to speed with the limits of classical testing. The Winner’s Curse is one of them. The problem is that when a significant result is obtained by using an underpowered test, the severity score becomes particularly high for large discrepancies from the null-hypothesis. This means that such discrepancies are very well supported by the evidence according to that measure. However, it is now well documented that significant tests with low power display inflated effect sizes. They systematically show departures from the null hypothesis H0 that are much greater than they really are. From an epistemological point of view this means that a significant result produced by an underpowered test does not provide evidence for large discrepancies from H0. Therefore, the severity score is an inadequate measure of evidence. Given that we are now aware of the phenomenon of inflated effect sizes, it would be irresponsible to rely on the severity score to measure the strength of the evidence against the null. Instead, one must take appropriate measures to try and avoid using underpowered tests by setting a threshold for the sample size or by replicating the results of the experiment.",data/metaanalysis.pdf --> https://link.springer.com/article/10.1007/s11098-020-01424-z,1642,"TITLE: 'Inflated effect sizes and underpowered tests: how the severity measure of evidence is affected by the winner's curse' ABSTRACT: 'My aim in this paper is to show how the problem of inflated effect sizes (the Winner's Curse) corrupts the severity measure of evidence. This has never been done. In fact, the Winner's Curse is barely mentioned in the philosophical literature. Since the severity score is the predominant measure of evidence for frequentist tests in the philosophical literature, it is important to underscore its flaws. It is also crucial to bring the philosophical literature up to speed with the limits of classical testing. The Winner's Curse is one of them. The problem is that when a significant result is obtained by using an underpowered test, the severity score becomes particularly high for large discrepancies from the null-hypothesis. This means that such discrepancies are very well supported by the evidence according to that measure. However, it is now well documented that significant tests with low power display inflated effect sizes. They systematically show departures from the null hypothesis H0 that are much greater than they really are. From an epistemological point of view this means that a significant result produced by an underpowered test does not provide evidence for large discrepancies from H0. Therefore, the severity score is an inadequate measure of evidence. Given that we are now aware of the phenomenon of inflated effect sizes, it would be irresponsible to rely on the severity score to measure the strength of the evidence against the null. Instead, one must take appropriate measures to try and avoid using underpowered tests by setting a threshold for the sample size or by replicating the results of the experiment.' SOURCE: (rochefort maranda, 2020)","rochefort maranda, 2020"
76,alvares quintana whitehouse,Beyond the hype and hope: Critical considerations for intranasal oxytocin research in autism spectrum disorder,2017,"Extensive research efforts in the last decade have been expended into understanding whether intranasal oxytocin may be an effective therapeutic in treating social communication impairments in individuals with autism spectrum disorder (ASD). After much hyped early findings, subsequent clinical trials of longer‐term administration have yielded more conservative and mixed evidence. However, it is still unclear at this stage whether these more disappointing findings reflect a true null effect or are mitigated by methodological differences masking true effects. In this review, we comprehensively evaluate the rationale for oxytocin as a therapeutic, evaluating evidence from randomized controlled trials, case reports, and open‐label studies of oxytocin administration in individuals with ASD. The evidence to date, including reviews of preregistered trials, suggests a number of critical considerations for the design and interpretation of research in this area. These include considering the choice of ASD outcome measures, dosing and nasal spray device issues, and participant selection. Despite these limitations in the field to date, there remains significant potential for oxytocin to ameliorate aspects of the persistent and debilitating social impairments in individuals with ASD. Given the considerable media hype around new treatments for ASD, as well as the needs of eager families, there is an urgent need for researchers to prioritise considering such factors when conducting well‐designed and controlled studies to further advance this field. Autism Res 2017, 10: 25–41. © 2016 International Society for Autism Research, Wiley Periodicals, Inc.",data/metaanalysis.pdf,1660,"TITLE: 'Beyond the hype and hope: Critical considerations for intranasal oxytocin research in autism spectrum disorder' ABSTRACT: 'Extensive research efforts in the last decade have been expended into understanding whether intranasal oxytocin may be an effective therapeutic in treating social communication impairments in individuals with autism spectrum disorder (ASD). After much hyped early findings, subsequent clinical trials of longer-term administration have yielded more conservative and mixed evidence. However, it is still unclear at this stage whether these more disappointing findings reflect a true null effect or are mitigated by methodological differences masking true effects. In this review, we comprehensively evaluate the rationale for oxytocin as a therapeutic, evaluating evidence from randomized controlled trials, case reports, and open-label studies of oxytocin administration in individuals with ASD. The evidence to date, including reviews of preregistered trials, suggests a number of critical considerations for the design and interpretation of research in this area. These include considering the choice of ASD outcome measures, dosing and nasal spray device issues, and participant selection. Despite these limitations in the field to date, there remains significant potential for oxytocin to ameliorate aspects of the persistent and debilitating social impairments in individuals with ASD. Given the considerable media hype around new treatments for ASD, as well as the needs of eager families, there is an urgent need for researchers to prioritise considering such factors when conducting well-designed and controlled studies to further advance this field. Autism Res 2017, 10: 25-41. (c) 2016 International Society for Autism Research, Wiley Periodicals, Inc.' SOURCE: (alvares quintana whitehouse, 2017)","alvares quintana whitehouse, 2017"
77,ioannidis,Why Most Discovered True Associations Are Inflated,2008,"Newly discovered true (non-null) associations often have inflated effects compared with the true effect sizes. I discuss here the main reasons for this inflation. First, theoretical considerations prove that when true discovery is claimed based on crossing a threshold of statistical significance and the discovery study is underpowered, the observed effects are expected to be inflated. This has been demonstrated in various fields ranging from early stopped clinical trials to genome-wide associations. Second, flexible analyses coupled with selective reporting may inflate the published discovered effects. The vibration ratio (the ratio of the largest vs. smallest effect on the same association approached with different analytic choices) can be very large. Third, effects may be inflated at the stage of interpretation due to diverse conflicts of interest. Discovered effects are not always inflated, and under some circumstances may be deflated—for example, in the setting of late discovery of associations in sequentially accumulated overpowered evidence, in some types of misclassification from measurement error, and in conflicts causing reverse biases. Finally, I discuss potential approaches to this problem. These include being cautious about newly discovered effect sizes, considering some rational down-adjustment, using analytical methods that correct for the anticipated inflation, ignoring the magnitude of the effect (if not necessary), conducting large studies in the discovery phase, using strict protocols for analyses, pursuing complete and transparent reporting of all results, placing emphasis on replication, and being fair with interpretation of results.",10.1097/EDE.0b013e31818131e7,1681,"TITLE: 'Why Most Discovered True Associations Are Inflated' ABSTRACT: 'Newly discovered true (non-null) associations often have inflated effects compared with the true effect sizes. I discuss here the main reasons for this inflation. First, theoretical considerations prove that when true discovery is claimed based on crossing a threshold of statistical significance and the discovery study is underpowered, the observed effects are expected to be inflated. This has been demonstrated in various fields ranging from early stopped clinical trials to genome-wide associations. Second, flexible analyses coupled with selective reporting may inflate the published discovered effects. The vibration ratio (the ratio of the largest vs. smallest effect on the same association approached with different analytic choices) can be very large. Third, effects may be inflated at the stage of interpretation due to diverse conflicts of interest. Discovered effects are not always inflated, and under some circumstances may be deflated--for example, in the setting of late discovery of associations in sequentially accumulated overpowered evidence, in some types of misclassification from measurement error, and in conflicts causing reverse biases. Finally, I discuss potential approaches to this problem. These include being cautious about newly discovered effect sizes, considering some rational down-adjustment, using analytical methods that correct for the anticipated inflation, ignoring the magnitude of the effect (if not necessary), conducting large studies in the discovery phase, using strict protocols for analyses, pursuing complete and transparent reporting of all results, placing emphasis on replication, and being fair with interpretation of results.' SOURCE: (ioannidis, 2008)","ioannidis, 2008"
78,halpern karlawish berlin,The continuing unethical conduct of underpowered clinical trials.,2002,"Despite long-standing critiques of the conduct of underpowered clinical trials, the practice not only remains widespread, but also has garnered increasing support. Patients and healthy volunteers continue to participate in research that may be of limited clinical value, and authors recently have offered 2 related arguments to support the validity and value of underpowered clinical trials: that meta-analysis may ""save"" small studies by providing a means to combine the results with those of other similar studies to enable estimates of an intervention's efficacy, and that although small studies may not provide a good basis for testing hypotheses, they may provide valuable estimates of treatment effects using confidence intervals. In this article, we examine these arguments in light of the distinctive moral issues associated with the conduct of underpowered trials, the disclosures that are owed to potential participants in underpowered trials so they may make autonomous enrollment decisions, and the circumstances in which the prospects for future meta-analyses may justify individually underpowered trials. We conclude that underpowered trials are ethical in only 2 situations: small trials of interventions for rare diseases in which investigators document explicit plans for including their results with those of similar trials in a prospective meta-analysis, and early-phase trials in the development of drugs or devices, provided they are adequately powered for defined purposes other than randomized treatment comparisons. In both cases, investigators must inform prospective subjects that their participation may only indirectly contribute to future health care benefits.",data/metaanalysis.pdf,1689,"TITLE: 'The continuing unethical conduct of underpowered clinical trials.' ABSTRACT: 'Despite long-standing critiques of the conduct of underpowered clinical trials, the practice not only remains widespread, but also has garnered increasing support. Patients and healthy volunteers continue to participate in research that may be of limited clinical value, and authors recently have offered 2 related arguments to support the validity and value of underpowered clinical trials: that meta-analysis may ""save"" small studies by providing a means to combine the results with those of other similar studies to enable estimates of an intervention's efficacy, and that although small studies may not provide a good basis for testing hypotheses, they may provide valuable estimates of treatment effects using confidence intervals. In this article, we examine these arguments in light of the distinctive moral issues associated with the conduct of underpowered trials, the disclosures that are owed to potential participants in underpowered trials so they may make autonomous enrollment decisions, and the circumstances in which the prospects for future meta-analyses may justify individually underpowered trials. We conclude that underpowered trials are ethical in only 2 situations: small trials of interventions for rare diseases in which investigators document explicit plans for including their results with those of similar trials in a prospective meta-analysis, and early-phase trials in the development of drugs or devices, provided they are adequately powered for defined purposes other than randomized treatment comparisons. In both cases, investigators must inform prospective subjects that their participation may only indirectly contribute to future health care benefits.' SOURCE: (halpern karlawish berlin, 2002)","halpern karlawish berlin, 2002"
79,gallyer dougherty burani albanese joiner hajcak,"Suicidal thoughts, behaviors, and event-related potentials: A systematic review and meta-analysis.",2021,"Suicidal thoughts and behaviors (STBs) are thought to result from, at least in part, abnormalities in various neural systems. Event-related potentials (ERPs) are a useful method for studying neural activity and can be leveraged to study neural deficits related to STBs; however, it is unknown how effective ERPs are at differentiating various STB groups. The present meta-analysis examined how well ERPs can differentiate (a) those with and without suicidal ideation, (b) those with and without suicide attempts, (c) those with different levels of suicide risk, and (d) differences between those with suicide attempts versus those with suicidal ideation only. This meta-analysis included 208 effect sizes from 2,517 participants from 27 studies. We used a random-effects meta-analysis using a restricted maximum likelihood estimator with robust variance estimation. We meta-analyzed ERP-STB combinations that had at least three effect sizes across two or more studies. A qualitative review found that for each ERP and STB combination, the literature is highly mixed. Our meta-analyses largely did not find significant relationships between STBs and ERPs. We also found that the literature is likely severely underpowered, with most studies only being sufficiently powered to detect unrealistically large effect sizes. Our results provided little-to-no support for a reliable relationship between the ERPs assessed and STBs. However, the current literature is severely underpowered, and there are many methodological weaknesses that must be resolved before making this determination. We recommend large-scale collaboration and improvements in measurement practices to combat the issues in this literature.",data/metaanalysis.pdf,1704,"TITLE: 'Suicidal thoughts, behaviors, and event-related potentials: A systematic review and meta-analysis.' ABSTRACT: 'Suicidal thoughts and behaviors (STBs) are thought to result from, at least in part, abnormalities in various neural systems. Event-related potentials (ERPs) are a useful method for studying neural activity and can be leveraged to study neural deficits related to STBs; however, it is unknown how effective ERPs are at differentiating various STB groups. The present meta-analysis examined how well ERPs can differentiate (a) those with and without suicidal ideation, (b) those with and without suicide attempts, (c) those with different levels of suicide risk, and (d) differences between those with suicide attempts versus those with suicidal ideation only. This meta-analysis included 208 effect sizes from 2,517 participants from 27 studies. We used a random-effects meta-analysis using a restricted maximum likelihood estimator with robust variance estimation. We meta-analyzed ERP-STB combinations that had at least three effect sizes across two or more studies. A qualitative review found that for each ERP and STB combination, the literature is highly mixed. Our meta-analyses largely did not find significant relationships between STBs and ERPs. We also found that the literature is likely severely underpowered, with most studies only being sufficiently powered to detect unrealistically large effect sizes. Our results provided little-to-no support for a reliable relationship between the ERPs assessed and STBs. However, the current literature is severely underpowered, and there are many methodological weaknesses that must be resolved before making this determination. We recommend large-scale collaboration and improvements in measurement practices to combat the issues in this literature.' SOURCE: (gallyer dougherty burani albanese joiner hajcak, 2021)","gallyer dougherty burani albanese joiner hajcak, 2021"
80,keefe kraemer epstein frank haynes laughren mcnulty reed sanchez leon,Defining a clinically meaningful effect for the design and interpretation of randomized controlled trials.,2013,"OBJECTIVE
This article captures the proceedings of a meeting aimed at defining clinically meaningful effects for use in randomized controlled trials for psychopharmacological agents.


DESIGN
Experts from a variety of disciplines defined clinically meaningful effects from their perspectives along with viewpoints about how to design and interpret randomized controlled trials.


SETTING
The article offers relevant, practical, and sometimes anecdotal information about clinically meaningful effects and how to interpret them.


PARTICIPANTS
The concept for this session was the work of co-chairs Richard Keefe and the late Andy Leon. Faculty included Richard Keefe, PhD; James McNulty, AbScB; Robert S. Epstein, MD, MS; Shelby D. Reed, PhD; Juan Sanchez, MD; Ginger Haynes, PhD; Andrew C. Leon, PhD; Helena Chmura Kraemer, PhD; Ellen Frank, PhD, and Kenneth L. Davis, MD.


RESULTS
The term clinically meaningful effect is an important aspect of designing and interpreting randomized controlled trials but can be particularly difficult in the setting of psychopharmacology where effect size may be modest, particularly over the short term, because of a strong response to placebo. Payers, regulators, patients, and clinicians have different concerns about clinically meaningful effects and may describe these terms differently. The use of moderators in success rate differences may help better delineate clinically meaningful effects.


CONCLUSION
There is no clear consensus on a single definition for clinically meaningful differences in randomized controlled trials, and investigators must be sensitive to specific concerns of stakeholders in psychopharmacology in order to design and execute appropriate clinical trials.",data/metaanalysis.pdf,1725,"TITLE: 'Defining a clinically meaningful effect for the design and interpretation of randomized controlled trials.' ABSTRACT: 'OBJECTIVE
This article captures the proceedings of a meeting aimed at defining clinically meaningful effects for use in randomized controlled trials for psychopharmacological agents.


DESIGN
Experts from a variety of disciplines defined clinically meaningful effects from their perspectives along with viewpoints about how to design and interpret randomized controlled trials.


SETTING
The article offers relevant, practical, and sometimes anecdotal information about clinically meaningful effects and how to interpret them.


PARTICIPANTS
The concept for this session was the work of co-chairs Richard Keefe and the late Andy Leon. Faculty included Richard Keefe, PhD; James McNulty, AbScB; Robert S. Epstein, MD, MS; Shelby D. Reed, PhD; Juan Sanchez, MD; Ginger Haynes, PhD; Andrew C. Leon, PhD; Helena Chmura Kraemer, PhD; Ellen Frank, PhD, and Kenneth L. Davis, MD.


RESULTS
The term clinically meaningful effect is an important aspect of designing and interpreting randomized controlled trials but can be particularly difficult in the setting of psychopharmacology where effect size may be modest, particularly over the short term, because of a strong response to placebo. Payers, regulators, patients, and clinicians have different concerns about clinically meaningful effects and may describe these terms differently. The use of moderators in success rate differences may help better delineate clinically meaningful effects.


CONCLUSION
There is no clear consensus on a single definition for clinically meaningful differences in randomized controlled trials, and investigators must be sensitive to specific concerns of stakeholders in psychopharmacology in order to design and execute appropriate clinical trials.' SOURCE: (keefe kraemer epstein frank haynes laughren mcnulty reed sanchez leon, 2013)","keefe kraemer epstein frank haynes laughren mcnulty reed sanchez leon, 2013"
81,kadlec sainani nimphius,With Great Power Comes Great Responsibility: Common Errors in Meta-Analyses and Meta-Regressions in Strength & Conditioning Research,2022,"Background and Objective Meta-analysis and meta-regression are often highly cited and may influence practice. Unfortunately, statistical errors in meta-analyses are widespread and can lead to flawed conclusions. The purpose of this article was to review common statistical errors in meta-analyses and to document their frequency in highly cited meta-analyses from strength and conditioning research. Methods We identified five errors in one highly cited meta-regression from strength and conditioning research: implausible outliers; overestimated effect sizes that arise from confusing standard deviation with standard error; failure to account for correlated observations; failure to account for within-study variance; and a focus on within-group rather than between-group results. We then quantified the frequency of these errors in 20 of the most highly cited meta-analyses in the field of strength and conditioning research from the past 20 years. Results We found that 85% of the 20 most highly cited meta-analyses in strength and conditioning research contained statistical errors. Almost half (45%) contained at least one effect size that was mistakenly calculated using standard error rather than standard deviation. In several cases, this resulted in obviously wrong effect sizes, for example, effect sizes of 11 or 14 standard deviations. Additionally, 45% failed to account for correlated observations despite including numerous effect sizes from the same study and often from the same group within the same study. Conclusions Statistical errors in meta-analysis and meta-regression are common in strength and conditioning research. We highlight five errors that authors, editors, and readers should check for when preparing or critically reviewing meta-analyses.",data/metaanalysis.pdf --> https://link.springer.com/article/10.1007/s40279-022-01766-0,1774,"TITLE: 'With Great Power Comes Great Responsibility: Common Errors in Meta-Analyses and Meta-Regressions in Strength & Conditioning Research' ABSTRACT: 'Background and Objective Meta-analysis and meta-regression are often highly cited and may influence practice. Unfortunately, statistical errors in meta-analyses are widespread and can lead to flawed conclusions. The purpose of this article was to review common statistical errors in meta-analyses and to document their frequency in highly cited meta-analyses from strength and conditioning research. Methods We identified five errors in one highly cited meta-regression from strength and conditioning research: implausible outliers; overestimated effect sizes that arise from confusing standard deviation with standard error; failure to account for correlated observations; failure to account for within-study variance; and a focus on within-group rather than between-group results. We then quantified the frequency of these errors in 20 of the most highly cited meta-analyses in the field of strength and conditioning research from the past 20 years. Results We found that 85% of the 20 most highly cited meta-analyses in strength and conditioning research contained statistical errors. Almost half (45%) contained at least one effect size that was mistakenly calculated using standard error rather than standard deviation. In several cases, this resulted in obviously wrong effect sizes, for example, effect sizes of 11 or 14 standard deviations. Additionally, 45% failed to account for correlated observations despite including numerous effect sizes from the same study and often from the same group within the same study. Conclusions Statistical errors in meta-analysis and meta-regression are common in strength and conditioning research. We highlight five errors that authors, editors, and readers should check for when preparing or critically reviewing meta-analyses.' SOURCE: (kadlec sainani nimphius, 2022)","kadlec sainani nimphius, 2022"
82,cherubini macdonald,Statistical Inferences Using Effect Sizes in Human Endothelial Function Research,2021,"Introduction: Magnitudes of change in endothelial function research can be articulated using effect size statistics. Effect sizes are commonly used in reference to Cohen's seminal guidelines of small (d = 0.2), medium (d = 0.5), and large (d = 0.8). Quantitative analyses of effect size distributions across various research disciplines have revealed values differing from Cohen's original recommendations. Here we examine effect size distributions in human endothelial function research, and the magnitude of small, medium, and large effects for macro and microvascular endothelial function. Methods: Effect sizes reported as standardized mean differences were extracted from meta research available for endothelial function. A frequency distribution was constructed to sort effect sizes. The 25th, 50th, and 75th percentiles were used to derive small, medium, and large effects. Group sample sizes and publication year from primary studies were also extracted to observe any potential trends, related to these factors, in effect size reporting in endothelial function research. Results: Seven hundred fifty-two effect sizes were extracted from eligible meta-analyses. We determined small (d = 0.28), medium (d = 0.69), and large (d = 1.21) effects for endothelial function that corresponded to the 25th, 50th, and 75th percentile of the data distribution. Conclusion: Our data indicate that direct application of Cohen's guidelines would underestimate the magnitude of effects in human endothelial function research. This investigation facilitates future a priori power analyses, provides a practical guiding benchmark for the contextualization of an effect when no other information is available, and further encourages the reporting of effect sizes in endothelial function research.",data/metaanalysis.pdf --> https://pubmed.ncbi.nlm.nih.gov/34966462/,1786,"TITLE: 'Statistical Inferences Using Effect Sizes in Human Endothelial Function Research' ABSTRACT: 'Introduction: Magnitudes of change in endothelial function research can be articulated using effect size statistics. Effect sizes are commonly used in reference to Cohen's seminal guidelines of small (d = 0.2), medium (d = 0.5), and large (d = 0.8). Quantitative analyses of effect size distributions across various research disciplines have revealed values differing from Cohen's original recommendations. Here we examine effect size distributions in human endothelial function research, and the magnitude of small, medium, and large effects for macro and microvascular endothelial function. Methods: Effect sizes reported as standardized mean differences were extracted from meta research available for endothelial function. A frequency distribution was constructed to sort effect sizes. The 25th, 50th, and 75th percentiles were used to derive small, medium, and large effects. Group sample sizes and publication year from primary studies were also extracted to observe any potential trends, related to these factors, in effect size reporting in endothelial function research. Results: Seven hundred fifty-two effect sizes were extracted from eligible meta-analyses. We determined small (d = 0.28), medium (d = 0.69), and large (d = 1.21) effects for endothelial function that corresponded to the 25th, 50th, and 75th percentile of the data distribution. Conclusion: Our data indicate that direct application of Cohen's guidelines would underestimate the magnitude of effects in human endothelial function research. This investigation facilitates future a priori power analyses, provides a practical guiding benchmark for the contextualization of an effect when no other information is available, and further encourages the reporting of effect sizes in endothelial function research.' SOURCE: (cherubini macdonald, 2021)","cherubini macdonald, 2021"
83,campbell,"If journals embraced conditional equivalence testing, would research be better?",2019,"We consider the reliability of published science: the probability that scientific claims put forth are true. Low reliability within many scientific fields is of major concern for researchers, scientific journals and the public at large. In the first part of this thesis, we introduce a publication policy that incorporates “conditional equivalence testing” (CET), a two-stage testing scheme in which standard nullhypothesis significance testing is followed, if the null hypothesis is not rejected, by testing for equivalence. The idea of CET has the potential to address recent concerns about reproducibility and the limited publication of null results. We detail the implementation of CET, investigate similarities with a Bayesian testing scheme, and outline the basis for how a scientific journal could proceed to reduce publication bias while remaining relevant. In the second part of this thesis, we consider proposals to adopt measures of “greater statistical stringency,” including suggestions to require larger sample sizes and to lower the highly criticized “p < 0.05” significance threshold. While pros and cons are vigorously debated, there has been little to no modeling of how adopting these measures might affect what type of science is published. We develop a novel model that, given current incentives to publish, predicts a researcher’s most rational use of resources in terms of the number of studies to undertake, the statistical power to devote to each study, and the desirable pre-study odds to pursue. Using this model, we investigate the merits of adopting measures of “greater statistical stringency” with the goal of informing the ongoing debate. We also use this model to investigate the merits of alternative publication policies, including the registered reports policy and our novel CET publication policy.",10.14288/1.0379722,1834,"TITLE: 'If journals embraced conditional equivalence testing, would research be better?' ABSTRACT: 'We consider the reliability of published science: the probability that scientific claims put forth are true. Low reliability within many scientific fields is of major concern for researchers, scientific journals and the public at large. In the first part of this thesis, we introduce a publication policy that incorporates ""conditional equivalence testing"" (CET), a two-stage testing scheme in which standard nullhypothesis significance testing is followed, if the null hypothesis is not rejected, by testing for equivalence. The idea of CET has the potential to address recent concerns about reproducibility and the limited publication of null results. We detail the implementation of CET, investigate similarities with a Bayesian testing scheme, and outline the basis for how a scientific journal could proceed to reduce publication bias while remaining relevant. In the second part of this thesis, we consider proposals to adopt measures of ""greater statistical stringency,"" including suggestions to require larger sample sizes and to lower the highly criticized ""p < 0.05"" significance threshold. While pros and cons are vigorously debated, there has been little to no modeling of how adopting these measures might affect what type of science is published. We develop a novel model that, given current incentives to publish, predicts a researcher's most rational use of resources in terms of the number of studies to undertake, the statistical power to devote to each study, and the desirable pre-study odds to pursue. Using this model, we investigate the merits of adopting measures of ""greater statistical stringency"" with the goal of informing the ongoing debate. We also use this model to investigate the merits of alternative publication policies, including the registered reports policy and our novel CET publication policy.' SOURCE: (campbell, 2019)","campbell, 2019"
84,jurek neumann,The Oxytocin Receptor: From Intracellular Signaling to Behavior.,2018,"The many facets of the oxytocin (OXT) system of the brain and periphery elicited nearly 25,000 publications since 1930 (see FIGURE 1 , as listed in PubMed), which revealed central roles for OXT and its receptor (OXTR) in reproduction, and social and emotional behaviors in animal and human studies focusing on mental and physical health and disease. In this review, we discuss the mechanisms of OXT expression and release, expression and binding of the OXTR in brain and periphery, OXTR-coupled signaling cascades, and their involvement in behavioral outcomes to assemble a comprehensive picture of the central and peripheral OXT system. Traditionally known for its role in milk let-down and uterine contraction during labor, OXT also has implications in physiological, and also behavioral, aspects of reproduction, such as sexual and maternal behaviors and pair bonding, but also anxiety, trust, sociability, food intake, or even drug abuse. The many facets of OXT are, on a molecular basis, brought about by a single receptor. The OXTR, a 7-transmembrane G protein-coupled receptor capable of binding to either Gαi or Gαq proteins, activates a set of signaling cascades, such as the MAPK, PKC, PLC, or CaMK pathways, which converge on transcription factors like CREB or MEF-2. The cellular response to OXT includes regulation of neurite outgrowth, cellular viability, and increased survival. OXTergic projections in the brain represent anxiety and stress-regulating circuits connecting the paraventricular nucleus of the hypothalamus, amygdala, bed nucleus of the stria terminalis, or the medial prefrontal cortex. Which OXT-induced patterns finally alter the behavior of an animal or a human being is still poorly understood, and studying those OXTR-coupled signaling cascades is one initial step toward a better understanding of the molecular background of those behavioral effects.",data/metaanalysis.pdf,1886,"TITLE: 'The Oxytocin Receptor: From Intracellular Signaling to Behavior.' ABSTRACT: 'The many facets of the oxytocin (OXT) system of the brain and periphery elicited nearly 25,000 publications since 1930 (see FIGURE 1 , as listed in PubMed), which revealed central roles for OXT and its receptor (OXTR) in reproduction, and social and emotional behaviors in animal and human studies focusing on mental and physical health and disease. In this review, we discuss the mechanisms of OXT expression and release, expression and binding of the OXTR in brain and periphery, OXTR-coupled signaling cascades, and their involvement in behavioral outcomes to assemble a comprehensive picture of the central and peripheral OXT system. Traditionally known for its role in milk let-down and uterine contraction during labor, OXT also has implications in physiological, and also behavioral, aspects of reproduction, such as sexual and maternal behaviors and pair bonding, but also anxiety, trust, sociability, food intake, or even drug abuse. The many facets of OXT are, on a molecular basis, brought about by a single receptor. The OXTR, a 7-transmembrane G protein-coupled receptor capable of binding to either Gai or Gaq proteins, activates a set of signaling cascades, such as the MAPK, PKC, PLC, or CaMK pathways, which converge on transcription factors like CREB or MEF-2. The cellular response to OXT includes regulation of neurite outgrowth, cellular viability, and increased survival. OXTergic projections in the brain represent anxiety and stress-regulating circuits connecting the paraventricular nucleus of the hypothalamus, amygdala, bed nucleus of the stria terminalis, or the medial prefrontal cortex. Which OXT-induced patterns finally alter the behavior of an animal or a human being is still poorly understood, and studying those OXTR-coupled signaling cascades is one initial step toward a better understanding of the molecular background of those behavioral effects.' SOURCE: (jurek neumann, 2018)","jurek neumann, 2018"
85,dwan altman arnaiz bloom chan cronin decullier easterbrook elm gamble ghersi ioannidis simes williamson,Systematic Review of the Empirical Evidence of Study Publication Bias and Outcome Reporting Bias,2008,"Background The increased use of meta-analysis in systematic reviews of healthcare interventions has highlighted several types of bias that can arise during the completion of a randomised controlled trial. Study publication bias has been recognised as a potential threat to the validity of meta-analysis and can make the readily available evidence unreliable for decision making. Until recently, outcome reporting bias has received less attention. Methodology/Principal Findings We review and summarise the evidence from a series of cohort studies that have assessed study publication bias and outcome reporting bias in randomised controlled trials. Sixteen studies were eligible of which only two followed the cohort all the way through from protocol approval to information regarding publication of outcomes. Eleven of the studies investigated study publication bias and five investigated outcome reporting bias. Three studies have found that statistically significant outcomes had a higher odds of being fully reported compared to non-significant outcomes (range of odds ratios: 2.2 to 4.7). In comparing trial publications to protocols, we found that 40–62% of studies had at least one primary outcome that was changed, introduced, or omitted. We decided not to undertake meta-analysis due to the differences between studies. Conclusions Recent work provides direct empirical evidence for the existence of study publication bias and outcome reporting bias. There is strong evidence of an association between significant results and publication; studies that report positive or significant results are more likely to be published and outcomes that are statistically significant have higher odds of being fully reported. Publications have been found to be inconsistent with their protocols. Researchers need to be aware of the problems of both types of bias and efforts should be concentrated on improving the reporting of trials.",data/metaanalysis.pdf,1931,"TITLE: 'Systematic Review of the Empirical Evidence of Study Publication Bias and Outcome Reporting Bias' ABSTRACT: 'Background The increased use of meta-analysis in systematic reviews of healthcare interventions has highlighted several types of bias that can arise during the completion of a randomised controlled trial. Study publication bias has been recognised as a potential threat to the validity of meta-analysis and can make the readily available evidence unreliable for decision making. Until recently, outcome reporting bias has received less attention. Methodology/Principal Findings We review and summarise the evidence from a series of cohort studies that have assessed study publication bias and outcome reporting bias in randomised controlled trials. Sixteen studies were eligible of which only two followed the cohort all the way through from protocol approval to information regarding publication of outcomes. Eleven of the studies investigated study publication bias and five investigated outcome reporting bias. Three studies have found that statistically significant outcomes had a higher odds of being fully reported compared to non-significant outcomes (range of odds ratios: 2.2 to 4.7). In comparing trial publications to protocols, we found that 40-62% of studies had at least one primary outcome that was changed, introduced, or omitted. We decided not to undertake meta-analysis due to the differences between studies. Conclusions Recent work provides direct empirical evidence for the existence of study publication bias and outcome reporting bias. There is strong evidence of an association between significant results and publication; studies that report positive or significant results are more likely to be published and outcomes that are statistically significant have higher odds of being fully reported. Publications have been found to be inconsistent with their protocols. Researchers need to be aware of the problems of both types of bias and efforts should be concentrated on improving the reporting of trials.' SOURCE: (dwan altman arnaiz bloom chan cronin decullier easterbrook elm gamble ghersi ioannidis simes williamson, 2008)","dwan altman arnaiz bloom chan cronin decullier easterbrook elm gamble ghersi ioa, 2008"
86,simes,Prospective meta-analysis of cholesterol-lowering studies: the Prospective Pravastatin Pooling (PPP) Project and the Cholesterol Treatment Trialists (CTT) Collaboration.,1995,"Meta-analyses of randomized trials evaluating cholesterol-lowering therapy have demonstrated clear reductions in coronary events and coronary mortality. However, the treatment impact on total mortality has been less certain. With the variable selection of trials and treatment questions, results of metaanalyses have sometimes given conflicting conclusions regarding the magnitude of treatment effects and the populations to whom benefits might accrue. Prospective meta-analysis can avoid these problems by clearly specifying the research questions, eligible studies, analysis plans, and outcome definitions in advance of trial results publication. This approach has been adopted in 2 major prospective meta-analyses of cholesterol-lowering treatments: the Prospective Pravastatin Pooling (PPP) project and the Cholesterol Treatment Trialists (CTT) collaboration. The PPP project is a prospectively planned combined analysis of 3 large-scale pravastatin trials comparing pravastatin against placebo over a minimum 5-year period. The analysis will contain data for > 19,500 patients and should have the power to examine the effects of treatment on total mortality, coronary mortality, and incidence of cancers as well as the ability to look at total coronary events in important subgroups underrepresented in previous trials. The CTT collaboration is a planned prospective meta-analysis of 12 major ongoing or planned randomized trials evaluating therapy with 3-hydroxy-3-methylglutaryl coenzyme A reductase inhibitors, a fibrate, or dietary modification. The trials were prospectively registered and the CTT protocols became final in November 1994. By the year 2000, the CTT collaboration is projected to have information on about 65,000 patients. This enormous data set will provide more reliable estimates of the effects of cholesterol reduction on cause-specific mortality and of effects on coronary mortality within important subgroups.",data/metaanalysis.pdf --> https://www.sciencedirect.com/science/article/pii/S0002914999804822,1940,"TITLE: 'Prospective meta-analysis of cholesterol-lowering studies: the Prospective Pravastatin Pooling (PPP) Project and the Cholesterol Treatment Trialists (CTT) Collaboration.' ABSTRACT: 'Meta-analyses of randomized trials evaluating cholesterol-lowering therapy have demonstrated clear reductions in coronary events and coronary mortality. However, the treatment impact on total mortality has been less certain. With the variable selection of trials and treatment questions, results of metaanalyses have sometimes given conflicting conclusions regarding the magnitude of treatment effects and the populations to whom benefits might accrue. Prospective meta-analysis can avoid these problems by clearly specifying the research questions, eligible studies, analysis plans, and outcome definitions in advance of trial results publication. This approach has been adopted in 2 major prospective meta-analyses of cholesterol-lowering treatments: the Prospective Pravastatin Pooling (PPP) project and the Cholesterol Treatment Trialists (CTT) collaboration. The PPP project is a prospectively planned combined analysis of 3 large-scale pravastatin trials comparing pravastatin against placebo over a minimum 5-year period. The analysis will contain data for > 19,500 patients and should have the power to examine the effects of treatment on total mortality, coronary mortality, and incidence of cancers as well as the ability to look at total coronary events in important subgroups underrepresented in previous trials. The CTT collaboration is a planned prospective meta-analysis of 12 major ongoing or planned randomized trials evaluating therapy with 3-hydroxy-3-methylglutaryl coenzyme A reductase inhibitors, a fibrate, or dietary modification. The trials were prospectively registered and the CTT protocols became final in November 1994. By the year 2000, the CTT collaboration is projected to have information on about 65,000 patients. This enormous data set will provide more reliable estimates of the effects of cholesterol reduction on cause-specific mortality and of effects on coronary mortality within important subgroups.' SOURCE: (simes, 1995)","simes, 1995"
87,aert wicherts assen,Publication bias examined in meta-analyses from psychology and medicine: A meta-meta-analysis,2019,"Publication bias is a substantial problem for the credibility of research in general and of meta-analyses in particular, as it yields overestimated effects and may suggest the existence of non-existing effects. Although there is consensus that publication bias exists, how strongly it affects different scientific literatures is currently less well-known. We examined evidence of publication bias in a large-scale data set of primary studies that were included in 83 meta-analyses published in Psychological Bulletin (representing meta-analyses from psychology) and 499 systematic reviews from the Cochrane Database of Systematic Reviews (CDSR; representing meta-analyses from medicine). Publication bias was assessed on all homogeneous subsets (3.8% of all subsets of meta-analyses published in Psychological Bulletin) of primary studies included in meta-analyses, because publication bias methods do not have good statistical properties if the true effect size is heterogeneous. Publication bias tests did not reveal evidence for bias in the homogeneous subsets. Overestimation was minimal but statistically significant, providing evidence of publication bias that appeared to be similar in both fields. However, a Monte-Carlo simulation study revealed that the creation of homogeneous subsets resulted in challenging conditions for publication bias methods since the number of effect sizes in a subset was rather small (median number of effect sizes equaled 6). Our findings are in line with, in its most extreme case, publication bias ranging from no bias until only 5% statistically nonsignificant effect sizes being published. These and other findings, in combination with the small percentages of statistically significant primary effect sizes (28.9% and 18.9% for subsets published in Psychological Bulletin and CDSR), led to the conclusion that evidence for publication bias in the studied homogeneous subsets is weak, but suggestive of mild publication bias in both psychology and medicine.",data/metaanalysis.pdf,2000,"TITLE: 'Publication bias examined in meta-analyses from psychology and medicine: A meta-meta-analysis' ABSTRACT: 'Publication bias is a substantial problem for the credibility of research in general and of meta-analyses in particular, as it yields overestimated effects and may suggest the existence of non-existing effects. Although there is consensus that publication bias exists, how strongly it affects different scientific literatures is currently less well-known. We examined evidence of publication bias in a large-scale data set of primary studies that were included in 83 meta-analyses published in Psychological Bulletin (representing meta-analyses from psychology) and 499 systematic reviews from the Cochrane Database of Systematic Reviews (CDSR; representing meta-analyses from medicine). Publication bias was assessed on all homogeneous subsets (3.8% of all subsets of meta-analyses published in Psychological Bulletin) of primary studies included in meta-analyses, because publication bias methods do not have good statistical properties if the true effect size is heterogeneous. Publication bias tests did not reveal evidence for bias in the homogeneous subsets. Overestimation was minimal but statistically significant, providing evidence of publication bias that appeared to be similar in both fields. However, a Monte-Carlo simulation study revealed that the creation of homogeneous subsets resulted in challenging conditions for publication bias methods since the number of effect sizes in a subset was rather small (median number of effect sizes equaled 6). Our findings are in line with, in its most extreme case, publication bias ranging from no bias until only 5% statistically nonsignificant effect sizes being published. These and other findings, in combination with the small percentages of statistically significant primary effect sizes (28.9% and 18.9% for subsets published in Psychological Bulletin and CDSR), led to the conclusion that evidence for publication bias in the studied homogeneous subsets is weak, but suggestive of mild publication bias in both psychology and medicine.' SOURCE: (aert wicherts assen, 2019)","aert wicherts assen, 2019"
88,egger smith martin schneider minder,"Bias in meta-analysis detected by a simple, graphical test",1997,"Abstract Objective: Funnel plots (plots of effect estimates against sample size) may be useful to detect bias in meta-analyses that were later contradicted by large trials. We examined whether a simple test of asymmetry of funnel plots predicts discordance of results when meta-analyses are compared to large trials, and we assessed the prevalence of bias in published meta-analyses. Design: Medline search to identify pairs consisting of a meta-analysis and a single large trial (concordance of results was assumed if effects were in the same direction and the meta-analytic estimate was within 30% of the trial); analysis of funnel plots from 37 meta-analyses identified from a hand search of four leading general medicine journals 1993-6 and 38 meta-analyses from the second 1996 issue of the Cochrane Database of Systematic Reviews. Main outcome measure: Degree of funnel plot asymmetry as measured by the intercept from regression of standard normal deviates against precision. Results: In the eight pairs of meta-analysis and large trial that were identified (five from cardiovascular medicine, one from diabetic medicine, one from geriatric medicine, one from perinatal medicine) there were four concordant and four discordant pairs. In all cases discordance was due to meta-analyses showing larger effects. Funnel plot asymmetry was present in three out of four discordant pairs but in none of concordant pairs. In 14 (38%) journal meta-analyses and 5 (13%) Cochrane reviews, funnel plot asymmetry indicated that there was bias. Conclusions: A simple analysis of funnel plots provides a useful test for the likely presence of bias in meta-analyses, but as the capacity to detect bias will be limited when meta-analyses are based on a limited number of small trials the results from such analyses should be treated with considerable caution. Key messages Systematic reviews of randomised trials are the best strategy for appraising evidence; however, the findings of some meta-analyses were later contradicted by large trials Funnel plots, plots of the trials' effect estimates against sample size, are skewed and asymmetrical in the presence of publication bias and other biases Funnel plot asymmetry, measured by regression analysis, predicts discordance of results when meta-analyses are compared with single large trials Funnel plot asymmetry was found in 38% of meta-analyses published in leading general medicine journals and in 13% of reviews from the Cochrane Database of Systematic Reviews Critical examination of systematic reviews for publication and related biases should be considered a routine procedure",10.1136/bmj.315.7109.629,2624,"TITLE: 'Bias in meta-analysis detected by a simple, graphical test' ABSTRACT: 'Abstract Objective: Funnel plots (plots of effect estimates against sample size) may be useful to detect bias in meta-analyses that were later contradicted by large trials. We examined whether a simple test of asymmetry of funnel plots predicts discordance of results when meta-analyses are compared to large trials, and we assessed the prevalence of bias in published meta-analyses. Design: Medline search to identify pairs consisting of a meta-analysis and a single large trial (concordance of results was assumed if effects were in the same direction and the meta-analytic estimate was within 30% of the trial); analysis of funnel plots from 37 meta-analyses identified from a hand search of four leading general medicine journals 1993-6 and 38 meta-analyses from the second 1996 issue of the Cochrane Database of Systematic Reviews. Main outcome measure: Degree of funnel plot asymmetry as measured by the intercept from regression of standard normal deviates against precision. Results: In the eight pairs of meta-analysis and large trial that were identified (five from cardiovascular medicine, one from diabetic medicine, one from geriatric medicine, one from perinatal medicine) there were four concordant and four discordant pairs. In all cases discordance was due to meta-analyses showing larger effects. Funnel plot asymmetry was present in three out of four discordant pairs but in none of concordant pairs. In 14 (38%) journal meta-analyses and 5 (13%) Cochrane reviews, funnel plot asymmetry indicated that there was bias. Conclusions: A simple analysis of funnel plots provides a useful test for the likely presence of bias in meta-analyses, but as the capacity to detect bias will be limited when meta-analyses are based on a limited number of small trials the results from such analyses should be treated with considerable caution. Key messages Systematic reviews of randomised trials are the best strategy for appraising evidence; however, the findings of some meta-analyses were later contradicted by large trials Funnel plots, plots of the trials' effect estimates against sample size, are skewed and asymmetrical in the presence of publication bias and other biases Funnel plot asymmetry, measured by regression analysis, predicts discordance of results when meta-analyses are compared with single large trials Funnel plot asymmetry was found in 38% of meta-analyses published in leading general medicine journals and in 13% of reviews from the Cochrane Database of Systematic Reviews Critical examination of systematic reviews for publication and related biases should be considered a routine procedure' SOURCE: (egger smith martin schneider minder, 1997)","egger smith martin schneider minder, 1997"
89,alexander aarts joanna anderson christopher anderson peter raymond attridge attwood jordan axt molly babel bahník baranski barnett cowan bartmess jennifer beer raoul bell heather bentley leah beyan grace binion borsboom annick bosch frank bosco sara bowman brandt braswell hilmar brohmer brown kristina brown jovita brüning ann calhoun sauls shannon callahan chagnon jesse chandler christopher chartier cheung cody daniel christopherson linda cillessen clay hayley cleary cloud michael conn cohoon simon columbus cordes costantini leslie cramblet alvarez cremata crusius decoster degaetano nicolás delia penna bobby den bezemer deserno olivia devitt dewitte david dobolyi geneva dodson donnellan ryan donohue rebecca dore dorrough anna dreber dugas elizabeth dunn kayleigh easey sylvia eboigbe eggleston jo embley epskamp timothy errington vivien estel frank farach feather fedor fernández castilla fiedler james field stanka fitneva flagan amanda forest eskil forsell foster michael frank rebecca frazier heather fuchs gable jeff galak galliani anup gampa sara garcia douglas gazarian gilbert roger giner sorolla glöckner lars goellner jin goh goldberg patrick goodbourn gordon mckeon bryan gorges jessie gorges goss graham grange gray hartgerink joshua hartshorne hasselman timothy hayes emma heikensten felix henninger hodsoll taylor holubar hoogendoorn humphries hung immelman vanessa irsik jahn jäkel marc jekel johannesson johnson david johnson kate johnson william johnston jonas jennifer joy gaba kappes kim kelso mallory kidwell seung kim kirkhart bennett kleinberg knežević kolorz kossakowski krause krijnen kuhlmann kunkels megan kyc calvin lai aamir laique lakens kristin lane bethany lassetter lazarević bel key jung lee minha lee lemm levitan melissa lewis lin lin stephanie lin matthias lippold darren loureiro ilse luteijn mackinnon heather mainard denise marigold martin tylar martinez masicampo joshua matacotta maya mathur michael may nicole mechin mehta johannes meixner alissa melinger jeremy miller mallorie miller moore marcus möschl matt motyl muller munafo neijenhuijs taylor nervi gandalf nicolas nilsonne brian nosek michèle nuijten catherine olsson osborne lutz ostkamp pavel penton voak perna pernet perugini pipitone pitts plessow prenoveau rahal kate ratliff david reinhard frank renkewitz ashley ricker rigney andrew rivers mark roebke abraham rutchick robert ryan şahin anondah saide gillian sandstrom david santos saxe schlegelmilch kathleen schmidt sabine scholz seibel dylan selterman shaki simpson sinclair jeanine skorinko slowik snyder soderberg carina sonnleitner spencer jeffrey spies steegen stieger nina strohminger sullivan talhelm tapia dorsthorst thomae sarah thomas pia tio frits traets steve tsang tuerlinckx paul turchan milan valášek veer aert assen bork mathijs van de ven bergh hulst dooren doorn renswoude rijn vanpaemel alejandro echeverría melissa vazquez natalia vélez marieke vermue verschoor vianello voracek gina vuu wagenmakers joanneke weerdmeester welsh erin westgate joeri wissink wood woods emily wright sining wu zeelenberg kellylynn zuni,Estimating the reproducibility of psychological science,2015,"Empirically analyzing empirical evidence One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study. Science, this issue 10.1126/science.aac4716 A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired. INTRODUCTION Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error. RATIONALE There is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science. RESULTS We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P < .05). Thirty-six percent of replications had significant results; 47% of original effect sizes were in the 95% confidence interval of the replication effect size; 39% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams. CONCLUSION No single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here. Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that “we already know this” belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know. Original study effect size versus replication effect size (correlation coefficients). Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects. Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47% of original effect sizes were in the 95% confidence interval of the replication effect size; 39% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.",10.1126/science.aac4716,6182,"TITLE: 'Estimating the reproducibility of psychological science' ABSTRACT: 'Empirically analyzing empirical evidence One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study. Science, this issue 10.1126/science.aac4716 A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired. INTRODUCTION Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error. RATIONALE There is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science. RESULTS We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P < .05). Thirty-six percent of replications had significant results; 47% of original effect sizes were in the 95% confidence interval of the replication effect size; 39% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams. CONCLUSION No single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here. Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that ""we already know this"" belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know. Original study effect size versus replication effect size (correlation coefficients). Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects. Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47% of original effect sizes were in the 95% confidence interval of the replication effect size; 39% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.' SOURCE: (alexander aarts joanna anderson christopher anderson peter raymond attridge attwood jordan axt molly babel bahnik baranski barnett cowan bartmess jennifer beer raoul bell heather bentley leah beyan grace binion borsboom annick bosch frank bosco sara bowman brandt braswell hilmar brohmer brown kristina brown jovita bruning ann calhoun sauls shannon callahan chagnon jesse chandler christopher chartier cheung cody daniel christopherson linda cillessen clay hayley cleary cloud michael conn cohoon simon columbus cordes costantini leslie cramblet alvarez cremata crusius decoster degaetano nicolas delia penna bobby den bezemer deserno olivia devitt dewitte david dobolyi geneva dodson donnellan ryan donohue rebecca dore dorrough anna dreber dugas elizabeth dunn kayleigh easey sylvia eboigbe eggleston jo embley epskamp timothy errington vivien estel frank farach feather fedor fernandez castilla fiedler james field stanka fitneva flagan amanda forest eskil forsell foster michael frank rebecca frazier heather fuchs gable jeff galak galliani anup gampa sara garcia douglas gazarian gilbert roger giner sorolla glockner lars goellner jin goh goldberg patrick goodbourn gordon mckeon bryan gorges jessie gorges goss graham grange gray hartgerink joshua hartshorne hasselman timothy hayes emma heikensten felix henninger hodsoll taylor holubar hoogendoorn humphries hung immelman vanessa irsik jahn jakel marc jekel johannesson johnson david johnson kate johnson william johnston jonas jennifer joy gaba kappes kim kelso mallory kidwell seung kim kirkhart bennett kleinberg knezevic kolorz kossakowski krause krijnen kuhlmann kunkels megan kyc calvin lai aamir laique lakens kristin lane bethany lassetter lazarevic bel key jung lee minha lee lemm levitan melissa lewis lin lin stephanie lin matthias lippold darren loureiro ilse luteijn mackinnon heather mainard denise marigold martin tylar martinez masicampo joshua matacotta maya mathur michael may nicole mechin mehta johannes meixner alissa melinger jeremy miller mallorie miller moore marcus moschl matt motyl muller munafo neijenhuijs taylor nervi gandalf nicolas nilsonne brian nosek michele nuijten catherine olsson osborne lutz ostkamp pavel penton voak perna pernet perugini pipitone pitts plessow prenoveau rahal kate ratliff david reinhard frank renkewitz ashley ricker rigney andrew rivers mark roebke abraham rutchick robert ryan sahin anondah saide gillian sandstrom david santos saxe schlegelmilch kathleen schmidt sabine scholz seibel dylan selterman shaki simpson sinclair jeanine skorinko slowik snyder soderberg carina sonnleitner spencer jeffrey spies steegen stieger nina strohminger sullivan talhelm tapia dorsthorst thomae sarah thomas pia tio frits traets steve tsang tuerlinckx paul turchan milan valasek veer aert assen bork mathijs van de ven bergh hulst dooren doorn renswoude rijn vanpaemel alejandro echeverria melissa vazquez natalia velez marieke vermue verschoor vianello voracek gina vuu wagenmakers joanneke weerdmeester welsh erin westgate joeri wissink wood woods emily wright sining wu zeelenberg kellylynn zuni, 2015)","alexander aarts joanna anderson christopher anderson peter raymond attridge attw, 2015"
