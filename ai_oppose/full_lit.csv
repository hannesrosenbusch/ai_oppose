Author,Title,Publication Year,Abstract Note,SOURCE
"d. quintana,",Most oxytocin administration studies are statistically underpowered to reliably detect (or reject) a wide range of effect sizes,2020,,data/metaanalysis.pdf
d. quintana,Towards better hypothesis tests in oxytocin research: Evaluating the validity of auxiliary assumptions,2021,,10.1016/j.psyneuen.2021.105642
"M. Harrer, P. Cuijpers, T. A. Furukawa, D. D. Ebert",Doing meta-analysis in R: A hands-on guide,2021,,data/metaanalysis.pdf
m. korbmacher f. azevedo c. pennington helena hartmann madeleine pownall kathleen schmidt m. elsherif nate breznau olly robertson t. kalandadze shijun yu bradley j. baker aoife o'mahony jørgen ø. -s. olsnes john j. shaw b. gjoneska yuki yamada j. p. röer jennifer murphy shilaan alzahawi sandra grinschgl cátia m. oliveira tobias wingen s. yeung meng liu l. m. könig nihan albayrak-aydemir ó. lecuona l. micheli t. evans,"The replication crisis has led to positive structural, procedural, and community changes",2023,,10.1038/s44271-023-00003-2
"r. simes,",Prospective meta-analysis of cholesterol-lowering studies: the Prospective Pravastatin Pooling (PPP) Project and the Cholesterol Treatment Trialists (CTT) Collaboration.,1995,,data/metaanalysis.pdf
k. button j. ioannidis c. mokrysz brian a. nosek j. flint e. robinson m. munafo,Power failure: why small sample size undermines the reliability of neuroscience,2013,,10.1038/nrn3475
"daniel s. quintana,",Replication studies for undergraduate theses to improve science and education,2021,,data/metaanalysis.pdf
"michèle b. nuijten, c.h.j. hartgerink, m. v. van assen, s. epskamp, j. wicherts,",The prevalence of statistical reporting errors in psychology (1985–2013),2015,,data/metaanalysis.pdf
felipe romero j. sprenger,Scientific self-correction: the Bayesian way,2020,,10.1007/s11229-020-02697-x
"h. walum, i. waldman, l. young,",Statistical and Methodological Considerations for the Interpretation of Intranasal Oxytocin Studies,2016,,data/metaanalysis.pdf
"A. Nordahl-Hansen, H. Cogo-Moreira, S. Panjeh, D. Quintana",Redefining effect size interpretations for psychotherapy RCTs in depression,2022,,data/metaanalysis.pdf
"d. kadlec, kristin l. sainani, sophia nimphius,",With Great Power Comes Great Responsibility: Common Errors in Meta-Analyses and Meta-Regressions in Strength & Conditioning Research,2022,,data/metaanalysis.pdf
"b. keech, s. crowe, d. hocking,","Intranasal oxytocin, social cognition and neurodevelopmental disorders: A meta-analysis",2018,,data/metaanalysis.pdf
"m. munafo, brian a. nosek, d. bishop, k. button, christopher d. chambers, n. p. sert, u. simonsohn, e. wagenmakers, j. ware, j. ioannidis,",A manifesto for reproducible science,2017,,data/metaanalysis.pdf
"m. kossmeier, u. tran, m. voracek,",Power-enhanced funnel plots for meta-analysis: The sunset funnel plot,2020,,data/metaanalysis.pdf
"j. stegenga,",Is meta-analysis the platinum standard of evidence?,2011,,data/metaanalysis.pdf
"g. guyatt, a. oxman, v. montori, g. vist, r. kunz, j. brożek, p. alonso-coello, b. djulbegovic, d. atkins, y. falck-ytter, john w. williams, j. meerpohl, s. norris, e. akl, h. schünemann,",GRADE guidelines: 5. Rating the quality of evidence--publication bias.,2011,,data/metaanalysis.pdf
"gilles e. gignac, eva t. szodorai,",Effect size guidelines for individual differences researchers,2016,,data/metaanalysis.pdf
"sakshi ghai,",It’s time to reimagine sample diversity and retire the WEIRD dichotomy,2021,,data/metaanalysis.pdf
"guillaume rochefort-maranda,",Inflated effect sizes and underpowered tests: how the severity measure of evidence is affected by the winner’s curse,2020,,data/metaanalysis.pdf
d. fanelli,Negative results are disappearing from most disciplines and countries,2011,,10.1007/s11192-011-0494-7
"m. bakermans-kranenburg, marinus h. van ijzendoorn,",Sniffing around oxytocin: review and meta-analyses of trials in healthy and clinical groups with implications for pharmacotherapy,2013,,data/metaanalysis.pdf
"howard balshem, m. helfand, h. schünemann, a. oxman, r. kunz, j. brożek, g. vist, y. falck-ytter, j. meerpohl, s. norris, g. guyatt,",GRADE guidelines: 3. Rating the quality of evidence.,2011,,data/metaanalysis.pdf
a. winterton l. westlye n. steen o. andreassen d. quintana,Improving the precision of intranasal oxytocin research,2020,,10.1038/s41562-020-00996-4
p. lachenbruch,Statistical Power Analysis for the Behavioral Sciences (2nd ed.),1989,,10.2307/2290095
"m. borenstein, l. hedges, j. higgins, h. rothstein,",Introduction to Meta‐Analysis,2009,,data/metaanalysis.pdf
"M. Borenstien, L. V. Hedges, J. P. T. Higgins, H. R. Rothstein",Introduction to meta-analysis,2021,,data/metaanalysis.pdf
"amanda kvarven, eirik strømland, m. johannesson,",Comparing meta-analyses and preregistered multiple-laboratory replication projects,2019,,data/metaanalysis.pdf
"N. J. L. Brown, J. A. J. Heathers",The GRIM test: A simple technique detects numerous anomalies in the reporting of results in psychology,2017,,data/metaanalysis.pdf
"christopher j. bryan, elizabeth tipton, d. yeager,",Behavioural science is unlikely to change the world without a heterogeneity revolution,2021,,data/metaanalysis.pdf
"c. chambers, l. tzavella,","The past, present and future of Registered Reports",2021,,data/metaanalysis.pdf
"joshua m. cherubini, m. macdonald,",Statistical Inferences Using Effect Sizes in Human Endothelial Function Research,2021,,data/metaanalysis.pdf
J. Cohen,Statistical power analysis for the behavioural sciences,1988,,data/metaanalysis.pdf
"d. quintana, a. guastella,",An Allostatic Theory of Oxytocin,2019,,data/metaanalysis.pdf
"j. valentine, t. pigott, h. rothstein,",How many studies do you need? A primer on statistical power for meta-analysis,2011,Abstract,data/metaanalysis.pdf
"r. alexander, michael j. scozzaro, lawrence j. borodkin,",Statistical and empirical examination of the chi-square test for homogeneity of correlations in meta-analysis.,1989,"Les auteurs font une analyse statistique de l'utilisation du test du Chi-Square Test (Chi-2), test qui mesure la grandeur des effets lors d'etudes experimentales en psychologie",data/metaanalysis.pdf
"j. wagge, m. brandt, l. lazarević, n. legate, cody daniel christopherson, brady wiggins, jon e. grahe,",Publishing Research With Undergraduate Students via Replication Work: The Collaborative Replications and Education Project,2018,"We discuss the Collaborative Replications and Education Project (CREP), the background and purpose of the CREP, and how students participate in the project. We also discuss how CREP facilitates and promotes dissemination of student work and the benefits of CREP.",data/metaanalysis.pdf
"m. lipsey, david b. wilson,",Practical Meta-Analysis,2000,"Introduction Problem Specification and Study Retrieval Selecting, Computing and Coding the Effect Size Statistic Developing a Coding Scheme and Coding Study Reports Data Management Analysis Issues and Strategies Computational Techniques for Meta-Analysis Data Interpreting and Using Meta-Analysis Results",data/metaanalysis.pdf
"tania b. huedo-medina, j. sánchez-meca, fulgencio marín-martínez, j. botella,",Assessing heterogeneity in meta-analysis: Q statistic or I2 index?,2006,"In meta-analysis, the usual way of assessing whether a set of single studies is homogeneous is by means of the Q test. However, the Q test only informs meta-analysts about the presence versus the absence of heterogeneity, but it does not report on the extent of such heterogeneity. Recently, the I(2) index has been proposed to quantify the degree of heterogeneity in a meta-analysis. In this article, the performances of the Q test and the confidence interval around the I(2) index are compared by means of a Monte Carlo simulation. The results show the utility of the I(2) index as a complement to the Q test, although it has the same problems of power with a small number of studies.",data/metaanalysis.pdf
"w. viechtbauer,",Conducting Meta-Analyses in R with the metafor Package,2010,"The metafor package provides functions for conducting meta-analyses in R. The package includes functions for fitting the meta-analytic fixed- and random-effects models and allows for the inclusion of moderators variables (study-level covariates) in these models. Meta-regression analyses with continuous and categorical moderators can be conducted in this way. Functions for the Mantel-Haenszel and Peto's one-step method for meta-analyses of 2 x 2 table data are also available. Finally, the package provides various plot functions (for example, for forest, funnel, and radial plots) and functions for assessing the model fit, for obtaining case diagnostics, and for tests of publication bias.",data/metaanalysis.pdf
"u. simonsohn, leif d. nelson, j. simmons,",P-Curve and Effect Size: Correcting for Publication Bias Using Only Significant Results,2014,"Journals tend to publish only statistically significant evidence, creating a scientific record that markedly overstates the size of effects. We provide a new tool that corrects for this bias without requiring access to nonsignificant results. It capitalizes on the fact that the distribution of significant p-values, p-curve, is a function of the true underlying effect. Researchers armed only with sample sizes and test results of the published findings can correct for publication bias. We validate the technique with simulations and by re-analyzing data from the Many-Labs Replication project. We demonstrate that p-curve can arrive at conclusions opposite that of existing tools by re-analyzing the meta-analysis of the “choice overload�? literature.",data/metaanalysis.pdf
"g. leng, r. leng,",Oxytocin: A citation network analysis of 10 000 papers,2021,"Our understanding of the oxytocin system has been built over the last 70 years by the work of hundreds of scientists, reported in thousands of papers. Here, we construct a map to that literature, using citation network analysis in conjunction with bibliometrics. The map identifies ten major ‘clusters’ of papers on oxytocin that differ in their particular research focus and that densely cite papers from the same cluster. We identify highly cited papers within each cluster and in each decade, not because citations are a good indicator of quality, but as a guide to recognising what questions were of wide interest at particular times. The clusters differ in their temporal profiles and bibliometric features; here, we attempt to understand the origins of these differences.",data/metaanalysis.pdf
"l. hedges, t. pigott,",The power of statistical tests for moderators in meta-analysis.,2004,"Calculation of the statistical power of statistical tests is important in planning and interpreting the results of research studies, including meta-analyses. It is particularly important in moderator analyses in meta-analysis, which are often used as sensitivity analyses to rule out moderator effects but also may have low statistical power. This article describes how to compute statistical power of both fixed- and mixed-effects moderator tests in meta-analysis that are analogous to the analysis of variance and multiple regression analysis for effect sizes. It also shows how to compute power of tests for goodness of fit associated with these models. Examples from a published meta-analysis demonstrate that power of moderator tests and goodness-of-fit tests is not always high.",data/metaanalysis.pdf
s. duval r. tweedie,Trim and Fill: A Simple Funnel‐Plot–Based Method of Testing and Adjusting for Publication Bias in Meta‐Analysis,2000,"Summary. We study recently developed nonparametric methods for estimating the number of missing studies that might exist in a meta‐analysis and the effect that these studies might have had on its outcome. These are simple rank‐based data augmentation techniques, which formalize the use of funnel plots. We show that they provide effective and relatively powerful tests for evaluating the existence of such publication bias. After adjusting for missing studies, we find that the point estimate of the overall effect size is approximately correct and coverage of the effect size confidence intervals is substantially improved, in many cases recovering the nominal confidence levels entirely. We illustrate the trim and fill method on existing meta‐analyses of studies in clinical trials and psychometrics.",10.1111/j.0006-341X.2000.00455.x
"j. vevea, carol m. woods,",Publication bias in research synthesis: sensitivity analysis using a priori weight functions.,2005,"Publication bias, sometimes known as the ""file-drawer problem"" or ""funnel-plot asymmetry,"" is common in empirical research. The authors review the implications of publication bias for quantitative research synthesis (meta-analysis) and describe existing techniques for detecting and correcting it. A new approach is proposed that is suitable for application to meta-analytic data sets that are too small for the application of existing methods. The model estimates parameters relevant to fixed-effects, mixed-effects or random-effects meta-analysis contingent on a hypothetical pattern of bias that is fixed independently of the data. The authors illustrate this approach for sensitivity analysis using 3 data sets adapted from a commonly cited reference work on research synthesis (H. M. Cooper & L. V. Hedges, 1994).",data/metaanalysis.pdf
"r. maccallum, shaobo zhang, kristopher j preacher, derek d. rucker,",On the practice of dichotomization of quantitative variables.,2002,"The authors examine the practice of dichotomization of quantitative measures, wherein relationships among variables are examined after 1 or more variables have been converted to dichotomous variables by splitting the sample at some point on the scale(s) of measurement. A common form of dichotomization is the median split, where the independent variable is split at the median to form high and low groups, which are then compared with respect to their means on the dependent variable. The consequences of dichotomization for measurement and statistical analyses are illustrated and discussed. The use of dichotomization in practice is described, and justifications that are offered for such usage are examined. The authors present the case that dichotomization is rarely defensible and often will yield misleading results.",data/metaanalysis.pdf
"d. lakens, anne m. scheel, p. isager,",Equivalence Testing for Psychological Research: A Tutorial,2018,"Psychologists must be able to test both for the presence of an effect and for the absence of an effect. In addition to testing against zero, researchers can use the two one-sided tests (TOST) procedure to test for equivalence and reject the presence of a smallest effect size of interest (SESOI). The TOST procedure can be used to determine if an observed effect is surprisingly small, given that a true effect at least as extreme as the SESOI exists. We explain a range of approaches to determine the SESOI in psychological science and provide detailed examples of how equivalence tests should be performed and reported. Equivalence tests are an important extension of the statistical tools psychologists currently use and enable researchers to falsify predictions about the presence, and declare the absence, of meaningful effects.",data/metaanalysis.pdf
"m. kraft,",Interpreting Effect Sizes of Education Interventions,2020,"Researchers commonly interpret effect sizes by applying benchmarks proposed by Jacob Cohen over a half century ago. However, effects that are small by Cohen’s standards are large relative to the impacts of most field-based interventions. These benchmarks also fail to consider important differences in study features, program costs, and scalability. In this article, I present five broad guidelines for interpreting effect sizes that are applicable across the social sciences. I then propose a more structured schema with new empirical benchmarks for interpreting a specific class of studies: causal research on education interventions with standardized achievement outcomes. Together, these tools provide a practical approach for incorporating study features, costs, and scalability into the process of interpreting the policy importance of effect sizes.",data/metaanalysis.pdf
l. john g. loewenstein d. prelec,Measuring the Prevalence of Questionable Research Practices With Incentives for Truth Telling,2012,"Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm.",10.1177/0956797611430953
"m. page, j. mckenzie, p. bossuyt, i. boutron, t. hoffmann, c. mulrow, larissa shamseer, j. tetzlaff, e. akl, s. brennan, r. chou, julie may glanville, j. grimshaw, a. hrõbjartsson, m. lalu, tianjing li, e. loder, e. mayo-wilson, steve mcdonald, l. mcguinness, l. stewart, james thomas, a. tricco, v. welch, p. whiting, d. moher,",The PRISMA 2020 statement: an updated guideline for reporting systematic reviews,2020,"The Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) statement, published in 2009, was designed to help systematic reviewers transparently report why the review was done, what the authors did, and what they found. Over the past decade, advances in systematic review methodology and terminology have necessitated an update to the guideline. The PRISMA 2020 statement replaces the 2009 statement and includes new reporting guidance that reflects advances in methods to identify, select, appraise, and synthesise studies. The structure and presentation of the items have been modified to facilitate implementation. In this article, we present the PRISMA 2020 27-item checklist, an expanded checklist that details reporting recommendations for each item, the PRISMA 2020 abstract checklist, and the revised flow diagrams for original and updated reviews.",data/metaanalysis.pdf
j. simmons leif d. nelson u. simonsohn,False-Positive Psychology,2011,"In this article, we accomplish two things. First, we show that despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.",10.1177/0956797611417632
"michèle b. nuijten, j. polanin,",“statcheck”: Automatically detect statistical reporting inconsistencies to increase reproducibility of meta‐analyses,2020,"We present the R package and web app statcheck to automatically detect statistical reporting inconsistencies in primary studies and meta‐analyses. Previous research has shown a high prevalence of reported p‐values that are inconsistent ‐ meaning a re‐calculated p‐value, based on the reported test statistic and degrees of freedom, does not match the author‐reported p‐value. Such inconsistencies affect the reproducibility and evidential value of published findings. The tool statcheck can help researchers to identify statistical inconsistencies so that they may correct them. In this paper, we provide an overview of the prevalence and consequences of statistical reporting inconsistencies. We also discuss the tool statcheck in more detail and give an example of how it can be used in a meta‐analysis. We end with some recommendations concerning the use of statcheck in meta‐analyses and make a case for better reporting standards of statistical results.",data/metaanalysis.pdf
"j. heathers, jordan anaya, tim j. van der zee, nicholas j. l. brown,",Recovering data from summary statistics: Sample Parameter Reconstruction via Iterative TEchniques (SPRITE),2018,"Scientific publications have not traditionally been accompanied by data, either during the peer review process or when published. Concern has arisen that the literature in many fields may contain inaccuracies or errors that cannot be detected without inspecting the original data. Here, we introduce SPRITE (Sample Parameter Reconstruction via Interative TEchniques), a heuristic method for reconstructing plausible samples from descriptive statistics of granular data, allowing reviewers, editors, readers, and future researchers to gain insights into the possible distributions of item values in the original data set. This paper presents the principles of operation of SPRITE, as well as worked examples of its practical use for error detection in real published work. Full source code for three software implementations of SPRITE (in MATLAB, R, and Python) and two web-based implementations requiring no local installation (1, 2) are available for readers.",data/metaanalysis.pdf
elizabeth m. murdoch robin l. j. lines m. crane n. ntoumanis carly j. brade e. quested joanne ayers d. gucciardi,The effectiveness of stress regulation interventions with athletes: A systematic review and multilevel meta-analysis of randomised controlled trials,2021,"s in order of significance from existing articles known to the team as relevant for inclusion based on the screening criteria, and continuously updates the learning algorithm every 50 abstracts screened based on what is deemed as in/eligible by the reviewer. Preliminary evidence supports the utility of Research Screener for semi-automating the screening process (Chai et al., 2021). Briefly, across nine systematic reviews and two scoping reviews, Research Screener delivered a 60-90% workload saving, and estimated a conservative threshold of the need to screen no more than 50% of articles to assure that 100% of eligible articles are identified. EM and RL discussed uncertainty regarding the screening decision for 16 papers with DG, who made the executive decision regarding their suitability for inclusion in the meta-analysis. Reasons for study exclusion were summarised as part of the search and included in the data extraction flow diagram (see Figure 1).",10.1080/1750984x.2021.1977974
s. charles james e. bartlett k. messick t. j. coleman alex uzdavines,Researcher Degrees of Freedom in the Psychology of Religion,2019,"ABSTRACT There is a push in psychology toward more transparent practices, stemming partially as a response to the replication crisis. We argue that the psychology of religion should help lead the way toward these new, more transparent practices to ensure a robust and dynamic subfield. One of the major issues that proponents of Open Science practices hope to address is researcher degrees of freedom (RDF). We pre-registered and conducted a systematic review of the 2017 issues from three psychology of religion journals. We aimed to identify the extent to which the psychology of religion has embraced Open Science practices and the role of RDF within the subfield. We found that many of the methodologies that help to increase transparency, such as pre-registration, have yet to be adopted by those in the subfield. In light of these findings, we present recommendations for addressing the issue of transparency in the psychology of religion and outline how to move toward these new Open Science practices.",10.1080/10508619.2019.1660573
"d. szűcs, j. ioannidis,",Empirical assessment of published effect sizes and power in the recent cognitive neuroscience and psychology literature,2017,"We have empirically assessed the distribution of published effect sizes and estimated power by analyzing 26,841 statistical records from 3,801 cognitive neuroscience and psychology papers published recently. The reported median effect size was D = 0.93 (interquartile range: 0.64–1.46) for nominally statistically significant results and D = 0.24 (0.11–0.42) for nonsignificant results. Median power to detect small, medium, and large effects was 0.12, 0.44, and 0.73, reflecting no improvement through the past half-century. This is so because sample sizes have remained small. Assuming similar true effect sizes in both disciplines, power was lower in cognitive neuroscience than in psychology. Journal impact factors negatively correlated with power. Assuming a realistic range of prior probabilities for null hypotheses, false report probability is likely to exceed 50% for the whole literature. In light of our findings, the recently reported low replication success in psychology is realistic, and worse performance may be expected for cognitive neuroscience.",data/metaanalysis.pdf
cristian mesquida jennifer murphy d. lakens j. warne,Replication concerns in sports and exercise science: a narrative review of selected methodological issues in the field,2022,"Known methodological issues such as publication bias, questionable research practices and studies with underpowered designs are known to decrease the replicability of study findings. The presence of such issues has been widely established across different research fields, especially in psychology. Their presence raised the first concerns that the replicability of study findings could be low and led researchers to conduct large replication projects. These replication projects revealed that a significant portion of original study findings could not be replicated, giving rise to the conceptualization of the replication crisis. Although previous research in the field of sports and exercise science has identified the first warning signs, such as an overwhelming proportion of significant findings, small sample sizes and lack of data availability, their possible consequences for the replicability of our field have been overlooked. We discuss the consequences of the above issues on the replicability of our field and offer potential solutions to improve replicability.",10.1098/rsos.220946
"mostafa salari rad, alison jane martingano, jeremy ginges,",Toward a psychology of Homo sapiens: Making psychological science more representative of the human population,2018,"Two primary goals of psychological science should be to understand what aspects of human psychology are universal and the way that context and culture produce variability. This requires that we take into account the importance of culture and context in the way that we write our papers and in the types of populations that we sample. However, most research published in our leading journals has relied on sampling WEIRD (Western, educated, industrialized, rich, and democratic) populations. One might expect that our scholarly work and editorial choices would by now reflect the knowledge that Western populations may not be representative of humans generally with respect to any given psychological phenomenon. However, as we show here, almost all research published by one of our leading journals, Psychological Science, relies on Western samples and uses these data in an unreflective way to make inferences about humans in general. To take us forward, we offer a set of concrete proposals for authors, journal editors, and reviewers that may lead to a psychological science that is more representative of the human condition.",data/metaanalysis.pdf
"františek bartoš, maximilian maier, daniel s. quintana, e. wagenmakers,","Adjusting for Publication Bias in JASP and R: Selection Models, PET-PEESE, and Robust Bayesian Meta-Analysis",2022,"Meta-analyses are essential for cumulative science, but their validity can be compromised by publication bias. To mitigate the impact of publication bias, one may apply publication-bias-adjustment techniques such as precision-effect test and precision-effect estimate with standard errors (PET-PEESE) and selection models. These methods, implemented in JASP and R, allow researchers without programming experience to conduct state-of-the-art publication-bias-adjusted meta-analysis. In this tutorial, we demonstrate how to conduct a publication-bias-adjusted meta-analysis in JASP and R and interpret the results. First, we explain two frequentist bias-correction methods: PET-PEESE and selection models. Second, we introduce robust Bayesian meta-analysis, a Bayesian approach that simultaneously considers both PET-PEESE and selection models. We illustrate the methodology on an example data set, provide an instructional video (https://bit.ly/pubbias) and an R-markdown script (https://osf.io/uhaew/), and discuss the interpretation of the results. Finally, we include concrete guidance on reporting the meta-analytic results in an academic article.",data/metaanalysis.pdf
"j. flake, e. fried,",Measurement Schmeasurement: Questionable Measurement Practices and How to Avoid Them,2020,"In this article, we define questionable measurement practices (QMPs) as decisions researchers make that raise doubts about the validity of the measures, and ultimately the validity of study conclusions. Doubts arise for a host of reasons, including a lack of transparency, ignorance, negligence, or misrepresentation of the evidence. We describe the scope of the problem and focus on how transparency is a part of the solution. A lack of measurement transparency makes it impossible to evaluate potential threats to internal, external, statistical-conclusion, and construct validity. We demonstrate that psychology is plagued by a measurement schmeasurement attitude: QMPs are common, hide a stunning source of researcher degrees of freedom, and pose a serious threat to cumulative psychological science, but are largely ignored. We address these challenges by providing a set of questions that researchers and consumers of scientific research can consider to identify and avoid QMPs. Transparent answers to these measurement questions promote rigorous research, allow for thorough evaluations of a study’s inferences, and are necessary for meaningful replication studies.",data/metaanalysis.pdf
j. ioannidis,Why Most Published Research Findings Are False,2005,"Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.",10.1080/09332480.2005.10722754
"d. quintana,",From pre-registration to publication: a non-technical primer for conducting a meta-analysis to synthesize correlational data,2015,"Meta-analysis synthesizes a body of research investigating a common research question. Outcomes from meta-analyses provide a more objective and transparent summary of a research area than traditional narrative reviews. Moreover, they are often used to support research grant applications, guide clinical practice, and direct health policy. The aim of this article is to provide a practical and non-technical guide for psychological scientists that outlines the steps involved in planning and performing a meta-analysis of correlational datasets. I provide a supplementary R script to demonstrate each analytical step described in the paper, which is readily adaptable for researchers to use for their analyses. While the worked example is the analysis of a correlational dataset, the general meta-analytic process described in this paper is applicable for all types of effect sizes. I also emphasize the importance of meta-analysis protocols and pre-registration to improve transparency and help avoid unintended duplication. An improved understanding this tool will not only help scientists to conduct their own meta-analyses but also improve their evaluation of published meta-analyses.",data/metaanalysis.pdf
"maximilian maier, d. lakens,",Justify Your Alpha: A Primer on Two Practical Approaches,2021,"The default use of an alpha level of .05 is suboptimal for two reasons. First, decisions based on data can be made more efficiently by choosing an alpha level that minimizes the combined Type 1 and Type 2 error rate. Second, it is possible that in studies with very high statistical power, p values lower than the alpha level can be more likely when the null hypothesis is true than when the alternative hypothesis is true (i.e., Lindley’s paradox). In this article, we explain two approaches that can be used to justify a better choice of an alpha level than relying on the default threshold of .05. The first approach is based on the idea to either minimize or balance Type 1 and Type 2 error rates. The second approach lowers the alpha level as a function of the sample size to prevent Lindley’s paradox. An R package and Shiny app are provided to perform the required calculations. Both approaches have their limitations (e.g., the challenge of specifying relative costs and priors) but can offer an improvement to current practices, especially when sample sizes are large. The use of alpha levels that are better justified should improve statistical inferences and can increase the efficiency and informativeness of scientific research.",data/metaanalysis.pdf
randall j. ellis,"Questionable Research Practices, Low Statistical Power, and Other Obstacles to Replicability: Why Preclinical Neuroscience Research Would Benefit from Registered Reports",2022,"Abstract Replicability, the degree to which a previous scientific finding can be repeated in a distinct set of data, has been considered an integral component of institutionalized scientific practice since its inception several hundred years ago. In the past decade, large-scale replication studies have demonstrated that replicability is far from favorable, across multiple scientific fields. Here, I evaluate this literature and describe contributing factors including the prevalence of questionable research practices (QRPs), misunderstanding of p-values, and low statistical power. I subsequently discuss how these issues manifest specifically in preclinical neuroscience research. I conclude that these problems are multifaceted and difficult to solve, relying on the actions of early and late career researchers, funding sources, academic publishers, and others. I assert that any viable solution to the problem of substandard replicability must include changing academic incentives, with adoption of registered reports being the most immediately impactful and pragmatic strategy. For animal research in particular, comprehensive reporting guidelines that document potential sources of sensitivity for experimental outcomes is an essential addition.",10.1523/ENEURO.0017-22.2022
michèle b. nuijten m. v. van assen hilde e. m. augusteijn elise a. v. crompvoets j. wicherts,"Effect Sizes, Power, and Biases in Intelligence Research: A Meta-Meta-Analysis",2020,"In this meta-study, we analyzed 2442 effect sizes from 131 meta-analyses in intelligence research, published from 1984 to 2014, to estimate the average effect size, median power, and evidence for bias. We found that the average effect size in intelligence research was a Pearson’s correlation of 0.26, and the median sample size was 60. Furthermore, across primary studies, we found a median power of 11.9% to detect a small effect, 54.5% to detect a medium effect, and 93.9% to detect a large effect. We documented differences in average effect size and median estimated power between different types of intelligence studies (correlational studies, studies of group differences, experiments, toxicology, and behavior genetics). On average, across all meta-analyses (but not in every meta-analysis), we found evidence for small-study effects, potentially indicating publication bias and overestimated effects. We found no differences in small-study effects between different study types. We also found no convincing evidence for the decline effect, US effect, or citation bias across meta-analyses. We concluded that intelligence research does show signs of low power and publication bias, but that these problems seem less severe than in many other scientific fields.",10.3390/jintelligence8040036
"perke jacobs, w. viechtbauer,",Estimation of the biserial correlation and its sampling variance for use in meta‐analysis,2017,"Meta‐analyses are often used to synthesize the findings of studies examining the correlational relationship between two continuous variables. When only dichotomous measurements are available for one of the two variables, the biserial correlation coefficient can be used to estimate the product–moment correlation between the two underlying continuous variables. Unlike the point‐biserial correlation coefficient, biserial correlation coefficients can therefore be integrated with product–moment correlation coefficients in the same meta‐analysis. The present article describes the estimation of the biserial correlation coefficient for meta‐analytic purposes and reports simulation results comparing different methods for estimating the coefficient's sampling variance. The findings indicate that commonly employed methods yield inconsistent estimates of the sampling variance across a broad range of research situations. In contrast, consistent estimates can be obtained using two methods that appear to be unknown in the meta‐analytic literature. A variance‐stabilizing transformation for the biserial correlation coefficient is described that allows for the construction of confidence intervals for individual coefficients with close to nominal coverage probabilities in most of the examined conditions. Copyright © 2016 John Wiley & Sons, Ltd.",data/metaanalysis.pdf
"thomas schäfer, marcus a. schwarz,",The Meaningfulness of Effect Sizes in Psychological Research: Differences Between Sub-Disciplines and the Impact of Potential Biases,2019,"Effect sizes are the currency of psychological research. They quantify the results of a study to answer the research question and are used to calculate statistical power. The interpretation of effect sizes—when is an effect small, medium, or large?—has been guided by the recommendations Jacob Cohen gave in his pioneering writings starting in 1962: Either compare an effect with the effects found in past research or use certain conventional benchmarks. The present analysis shows that neither of these recommendations is currently applicable. From past publications without pre-registration, 900 effects were randomly drawn and compared with 93 effects from publications with pre-registration, revealing a large difference: Effects from the former (median r = 0.36) were much larger than effects from the latter (median r = 0.16). That is, certain biases, such as publication bias or questionable research practices, have caused a dramatic inflation in published effects, making it difficult to compare an actual effect with the real population effects (as these are unknown). In addition, there were very large differences in the mean effects between psychological sub-disciplines and between different study designs, making it impossible to apply any global benchmarks. Many more pre-registered studies are needed in the future to derive a reliable picture of real population effects.",data/metaanalysis.pdf
"j. brunner, u. schimmack,",Estimating Population Mean Power Under Conditions of Heterogeneity and Selection for Significance,2020,"In scientific fields that use significance tests, statistical power is important for successful replications of significant results because it is the long-run success rate in a series of exact replication studies. For any population of published results, there is a population of power values of the statistical tests on which conclusions are based. We give exact theoretical results showing how selection for significance affects the distribution of statistical power in a heterogeneous population of significance tests. In a set of large-scale simulation studies, we compare four methods for estimating population mean power of a set of studies selected for significance (a maximum likelihood model, extensions of p-curve and p-uniform, & z-curve). The p-uniform and p-curve methods performed well with a fixed effects size and varying sample sizes. However, when there was substantial variability in effect sizes as well as sample sizes, both methods systematically overestimate mean power. When the assumptions of the maximum likelihood were satisfied, it produced the most accurate estimates for heterogeneity in effect sizes, but z-curve produced more accurate estimates when the assumptions of the maximum likelihood model were not met. We recommend the use of zcurve to estimate the typical power of significant results, which has implications for the replicability of significant results in psychology journals.",data/metaanalysis.pdf
m. v. van assen robbie c. m. van aert j. wicherts,Meta-analysis using effect size distributions of only statistically significant studies.,2015,"Publication bias threatens the validity of meta-analytic results and leads to overestimation of the effect size in traditional meta-analysis. This particularly applies to meta-analyses that feature small studies, which are ubiquitous in psychology. Here we develop a new method for meta-analysis that deals with publication bias. This method, p-uniform, enables (a) testing of publication bias, (b) effect size estimation, and (c) testing of the null-hypothesis of no effect. No current method for meta-analysis possesses all 3 qualities. Application of p-uniform is straightforward because no additional data on missing studies are needed and no sophisticated assumptions or choices need to be made before applying it. Simulations show that p-uniform generally outperforms the trim-and-fill method and the test of excess significance (TES; Ioannidis & Trikalinos, 2007b) if publication bias exists and population effect size is homogenous or heterogeneity is slight. For illustration, p-uniform and other publication bias analyses are applied to the meta-analysis of McCall and Carriger (1993) examining the association between infants' habituation to a stimulus and their later cognitive ability (IQ). We conclude that p-uniform is a valuable technique for examining publication bias and estimating population effects in fixed-effect meta-analyses, and as sensitivity analysis to draw inferences about publication bias.",10.1037/met0000025
"j. higgins, s. thompson,",Quantifying heterogeneity in a meta‐analysis,2002,"The extent of heterogeneity in a meta‐analysis partly determines the difficulty in drawing overall conclusions. This extent may be measured by estimating a between‐study variance, but interpretation is then specific to a particular treatment effect metric. A test for the existence of heterogeneity exists, but depends on the number of studies in the meta‐analysis. We develop measures of the impact of heterogeneity on a meta‐analysis, from mathematical criteria, that are independent of the number of studies and the treatment effect metric. We derive and propose three suitable statistics: H is the square root of the χ2 heterogeneity statistic divided by its degrees of freedom; R is the ratio of the standard error of the underlying mean from a random effects meta‐analysis to the standard error of a fixed effect meta‐analytic estimate, and I2 is a transformation of H that describes the proportion of total variation in study estimates that is due to heterogeneity. We discuss interpretation, interval estimates and other properties of these measures and examine them in five example data sets showing different amounts of heterogeneity. We conclude that H and I2, which can usually be calculated for published meta‐analyses, are particularly useful summaries of the impact of heterogeneity. One or both should be presented in published meta‐analyses in preference to the test for heterogeneity. Copyright © 2002 John Wiley & Sons, Ltd.",data/metaanalysis.pdf
m. hagger,Meta-analysis,2022,"ABSTRACT The sheer volume of available research and shifts toward evidence-based practice has led researchers and practitioners in sport and exercise psychology to rely increasingly on meta-analyses to summarize current knowledge, provide future research directions, and inform policy and practice. These issues highlight the imperative of precision and integrity in the conduct of meta-analyses in the discipline. This review provides a summary of meta-analytic methods relevant to sport and exercise psychology, identifies important issues and advances in meta-analytic methods, and provides best practice guidelines for meta-analysts to consider when synthesizing research in the discipline. In Part I, I provide an overview of the basic principles of meta-analysis and direct readers to accessible, non-technical treatments of the topic. In Part II, I introduce several key issues in meta-analysis and summarize the latest advances in each: effective assessment of heterogeneity; testing for moderators; dealing with dependency; evaluating publication bias and tracking down ‘fugitive literature’; and assessing sample size in meta-analysis. I also cover two emerging topics: testing theories using meta-analysis and open science and transparency practices in meta-analysis. I conclude the discussion of each issue by providing best practice guidelines, and refer the reader to further accessible texts to augment knowledge and understanding.",10.1080/1750984X.2021.1966824
"a. linden, johannes hönekopp,",Heterogeneity of Research Results: A New Perspective From Which to Assess and Promote Progress in Psychological Science,2021,"Heterogeneity emerges when multiple close or conceptual replications on the same subject produce results that vary more than expected from the sampling error. Here we argue that unexplained heterogeneity reflects a lack of coherence between the concepts applied and data observed and therefore a lack of understanding of the subject matter. Typical levels of heterogeneity thus offer a useful but neglected perspective on the levels of understanding achieved in psychological science. Focusing on continuous outcome variables, we surveyed heterogeneity in 150 meta-analyses from cognitive, organizational, and social psychology and 57 multiple close replications. Heterogeneity proved to be very high in meta-analyses, with powerful moderators being conspicuously absent. Population effects in the average meta-analysis vary from small to very large for reasons that are typically not understood. In contrast, heterogeneity was moderate in close replications. A newly identified relationship between heterogeneity and effect size allowed us to make predictions about expected heterogeneity levels. We discuss important implications for the formulation and evaluation of theories in psychology. On the basis of insights from the history and philosophy of science, we argue that the reduction of heterogeneity is important for progress in psychology and its practical applications, and we suggest changes to our collective research practice toward this end.",data/metaanalysis.pdf
evan c. carter felix d. schönbrodt will m. gervais j. hilgard,Correcting for Bias in Psychology: A Comparison of Meta-Analytic Methods,2019,"Publication bias and questionable research practices in primary research can lead to badly overestimated effects in meta-analysis. Methodologists have proposed a variety of statistical approaches to correct for such overestimation. However, it is not clear which methods work best for data typically seen in psychology. Here, we present a comprehensive simulation study in which we examined how some of the most promising meta-analytic methods perform on data that might realistically be produced by research in psychology. We simulated several levels of questionable research practices, publication bias, and heterogeneity, and used study sample sizes empirically derived from the literature. Our results clearly indicated that no single meta-analytic method consistently outperformed all the others. Therefore, we recommend that meta-analysts in psychology focus on sensitivity analyses—that is, report on a variety of methods, consider the conditions under which these methods fail (as indicated by simulation studies such as ours), and then report how conclusions might change depending on which conditions are most plausible. Moreover, given the dependence of meta-analytic methods on untestable assumptions, we strongly recommend that researchers in psychology continue their efforts to improve the primary literature and conduct large-scale, preregistered replications. We provide detailed results and simulation code at https://osf.io/rf3ys and interactive figures at http://www.shinyapps.org/apps/metaExplorer/.",10.1177/2515245919847196
daniel s. quintana,A Guide for Calculating Study-Level Statistical Power for Meta-Analyses,2023,"Meta-analysis is a popular approach in the psychological sciences for synthesizing data across studies. However, the credibility of meta-analysis outcomes depends on the evidential value of studies included in the body of evidence used for data synthesis. One important consideration for determining a study’s evidential value is the statistical power of the study’s design/statistical test combination for detecting hypothetical effect sizes of interest. Studies with a design/test combination that cannot reliably detect a wide range of effect sizes are more susceptible to questionable research practices and exaggerated effect sizes. Therefore, determining the statistical power for design/test combinations for studies included in meta-analyses can help researchers make decisions regarding confidence in the body of evidence. Because the one true population effect size is unknown when hypothesis testing, an alternative approach is to determine statistical power for a range of hypothetical effect sizes. This tutorial introduces the metameta R package and web app, which facilitates the straightforward calculation and visualization of study-level statistical power in meta-analyses for a range of hypothetical effect sizes. Readers will be shown how to reanalyze data using information typically presented in meta-analysis forest plots or tables and how to integrate the metameta package when reporting novel meta-analyses. A step-by-step companion screencast video tutorial is also provided to assist readers using the R package.",10.1177/25152459221147260
c. brydges,"Effect Size Guidelines, Sample Size Calculations, and Statistical Power in Gerontology",2019,"Abstract Background and Objectives Researchers typically use Cohen’s guidelines of Pearson’s r = .10, .30, and .50, and Cohen’s d = 0.20, 0.50, and 0.80 to interpret observed effect sizes as small, medium, or large, respectively. However, these guidelines were not based on quantitative estimates and are only recommended if field-specific estimates are unknown. This study investigated the distribution of effect sizes in both individual differences research and group differences research in gerontology to provide estimates of effect sizes in the field. Research Design and Methods Effect sizes (Pearson’s r, Cohen’s d, and Hedges’ g) were extracted from meta-analyses published in 10 top-ranked gerontology journals. The 25th, 50th, and 75th percentile ranks were calculated for Pearson’s r (individual differences) and Cohen’s d or Hedges’ g (group differences) values as indicators of small, medium, and large effects. A priori power analyses were conducted for sample size calculations given the observed effect size estimates. Results Effect sizes of Pearson’s r = .12, .20, and .32 for individual differences research and Hedges’ g = 0.16, 0.38, and 0.76 for group differences research were interpreted as small, medium, and large effects in gerontology. Discussion and Implications Cohen’s guidelines appear to overestimate effect sizes in gerontology. Researchers are encouraged to use Pearson’s r = .10, .20, and .30, and Cohen’s d or Hedges’ g = 0.15, 0.40, and 0.75 to interpret small, medium, and large effects in gerontology, and recruit larger samples.",10.1093/geroni/igz036
d. lakens,Sample Size Justification,2021,"An important step when designing a study is to justify the sample size that will be collected. The key aim of a sample size justification is to explain how the collected data is expected to provide valuable information given the inferential goals of the researcher. In this overview article six approaches are discussed to justify the sample size in a quantitative empirical study: 1) collecting data from (an)almost) the entire population, 2) choosing a sample size based on resource constraints, 3) performing an a-priori power analysis, 4) planning for a desired accuracy, 5) using heuristics, or 6) explicitly acknowledging the absence of a justification. An important question to consider when justifying sample sizes is which effect sizes are deemed interesting, and the extent to which the data that is collected informs inferences about these effect sizes. Depending on the sample size justification chosen, researchers could consider 1) what the smallest effect size of interest is, 2) which minimal effect size will be statistically significant, 3) which effect sizes they expect (and what they base these expectations on), 4) which effect sizes would be rejected based on a confidence interval around the effect size, 5) which ranges of effects a study has sufficient power to detect based on a sensitivity power analysis, and 6) which effect sizes are plausible in a specific research area. Researchers can use the guidelines presented in this article to improve their sample size justification, and hopefully, align the informational value of a study with their inferential goals.",10.31234/osf.io/9d3yf
"maximilian maier, t. vanderweele, maya b. mathur,",Using selection models to assess sensitivity to publication bias: A tutorial and call for more routine use,2022,"Abstract In meta‐analyses, it is critical to assess the extent to which publication bias might have compromised the results. Classical methods based on the funnel plot, including Egger's test and Trim‐and‐Fill, have become the de facto default methods to do so, with a large majority of recent meta‐analyses in top medical journals (85%) assessing for publication bias exclusively using these methods. However, these classical funnel plot methods have important limitations when used as the sole means of assessing publication bias: they essentially assume that the publication process favors large point estimates for small studies and does not affect the largest studies, and they can perform poorly when effects are heterogeneous. In light of these limitations, we recommend that meta‐analyses routinely apply other publication bias methods in addition to or instead of classical funnel plot methods. To this end, we describe how to use and interpret selection models. These methods make the often more realistic assumption that publication bias favors “statistically significant” results, and the methods also directly accommodate effect heterogeneity. Selection models have been established for decades in the statistics literature and are supported by user‐friendly software, yet remain rarely reported in many disciplines. We use a previously published meta‐analysis to demonstrate that selection models can yield insights that extend beyond those provided by funnel plot methods, suggesting the importance of establishing more comprehensive reporting practices for publication bias assessment.",data/metaanalysis.pdf
"rune boen, d. quintana, c. ladouceur, c. k. tamnes,",Age‐related differences in the error‐related negativity and error positivity in children and adolescents are moderated by sample and methodological characteristics: A meta‐analysis,2021,"Abstract The error‐related negativity (ERN) and the error positivity (Pe) are electrophysiological components associated with error processing that are thought to exhibit distinctive developmental trajectories from childhood to adulthood. To investigate the age and age moderation effects on the ERN and the Pe strength during development, we conducted a preregistered three‐level meta‐analysis synthesizing 120 and 41 effect sizes across 18 group comparison studies and 19 correlational studies, respectively. The meta‐analysis included studies with mean age between 3.6 and 28.7 (min‐max age range: 3.5 and 49.8) years for age‐group comparisons and 6.1 to 18.7 (min‐max age range: 4.0–35.7) years for age correlations. Results showed that age was associated with a more negative ERN (SMD = −.433, r = −.230). No statistically significant association between age and the Pe was found (SMD = .059, r = −.091), except for in a group comparison between younger and older adolescents. The age effects were not significantly moderated by whether a Flanker or a Go/No‐Go task was used, whereas a probabilistic learning task moderated the age effect on the Pe. Moreover, the Fz and Cz electrode sites yielded stronger negative associations between age and the ERN and the Pe, respectively. The results confirm that the ERN and the Pe show differential development courses and suggest that sample and methodological characteristics influence the age effects, and lay the foundation for investigations of developmental patterns of the ERN and the Pe in relation to psychopathology and early genetic and environmental risk factors.",data/metaanalysis.pdf
"d. quintana,",Statistical considerations for reporting and planning heart rate variability case-control studies.,2017,"The calculation of heart rate variability (HRV) is a popular tool used to investigate differences in cardiac autonomic control between population samples. When interpreting effect sizes to quantify the magnitude of group differences, researchers typically use Cohen's guidelines of small (0.2), medium (0.5), and large (0.8) effects. However, these guidelines were originally proposed as a fallback for when the effect size distribution (ESD) was unknown. Despite the availability of effect sizes from hundreds of HRV studies, researchers still largely rely on Cohen's guidelines to interpret effect sizes and to perform power analyses to calculate required sample sizes for future research. This article describes an ESD analysis of 297 HRV effect sizes from between-group/case-control studies, revealing that the 25th, 50th, and 75th effect size percentiles correspond with effect sizes of 0.26, 0.51, and 0.88, respectively. The analyses suggest that Cohen's guidelines may underestimate the magnitude of small and large effect sizes and that HRV studies are generally underpowered. Therefore, to better reflect the observed ESD, effect sizes of 0.25, 0.5, and 0.9 should be interpreted as small, medium, and large effects (after rounding to the closest 0.05). Based on power calculations using the ESD, suggested sample sizes are also provided for planning suitably powered studies that are more likely to replicate. Researchers are encouraged to use the ESD data set or their own collected data sets in tandem with the provided analysis script to perform custom ESD and power analyses relevant to their specific research area.",data/metaanalysis.pdf
"g. alvares, d. quintana, a. whitehouse,",Beyond the hype and hope: Critical considerations for intranasal oxytocin research in autism spectrum disorder,2017,"Extensive research efforts in the last decade have been expended into understanding whether intranasal oxytocin may be an effective therapeutic in treating social communication impairments in individuals with autism spectrum disorder (ASD). After much hyped early findings, subsequent clinical trials of longer‐term administration have yielded more conservative and mixed evidence. However, it is still unclear at this stage whether these more disappointing findings reflect a true null effect or are mitigated by methodological differences masking true effects. In this review, we comprehensively evaluate the rationale for oxytocin as a therapeutic, evaluating evidence from randomized controlled trials, case reports, and open‐label studies of oxytocin administration in individuals with ASD. The evidence to date, including reviews of preregistered trials, suggests a number of critical considerations for the design and interpretation of research in this area. These include considering the choice of ASD outcome measures, dosing and nasal spray device issues, and participant selection. Despite these limitations in the field to date, there remains significant potential for oxytocin to ameliorate aspects of the persistent and debilitating social impairments in individuals with ASD. Given the considerable media hype around new treatments for ASD, as well as the needs of eager families, there is an urgent need for researchers to prioritise considering such factors when conducting well‐designed and controlled studies to further advance this field. Autism Res 2017, 10: 25–41. © 2016 International Society for Autism Research, Wiley Periodicals, Inc.",data/metaanalysis.pdf
j. ioannidis,Why Most Discovered True Associations Are Inflated,2008,"Newly discovered true (non-null) associations often have inflated effects compared with the true effect sizes. I discuss here the main reasons for this inflation. First, theoretical considerations prove that when true discovery is claimed based on crossing a threshold of statistical significance and the discovery study is underpowered, the observed effects are expected to be inflated. This has been demonstrated in various fields ranging from early stopped clinical trials to genome-wide associations. Second, flexible analyses coupled with selective reporting may inflate the published discovered effects. The vibration ratio (the ratio of the largest vs. smallest effect on the same association approached with different analytic choices) can be very large. Third, effects may be inflated at the stage of interpretation due to diverse conflicts of interest. Discovered effects are not always inflated, and under some circumstances may be deflated—for example, in the setting of late discovery of associations in sequentially accumulated overpowered evidence, in some types of misclassification from measurement error, and in conflicts causing reverse biases. Finally, I discuss potential approaches to this problem. These include being cautious about newly discovered effect sizes, considering some rational down-adjustment, using analytical methods that correct for the anticipated inflation, ignoring the magnitude of the effect (if not necessary), conducting large studies in the discovery phase, using strict protocols for analyses, pursuing complete and transparent reporting of all results, placing emphasis on replication, and being fair with interpretation of results.",10.1097/EDE.0b013e31818131e7
"s. halpern, j. karlawish, j. berlin,",The continuing unethical conduct of underpowered clinical trials.,2002,"Despite long-standing critiques of the conduct of underpowered clinical trials, the practice not only remains widespread, but also has garnered increasing support. Patients and healthy volunteers continue to participate in research that may be of limited clinical value, and authors recently have offered 2 related arguments to support the validity and value of underpowered clinical trials: that meta-analysis may ""save"" small studies by providing a means to combine the results with those of other similar studies to enable estimates of an intervention's efficacy, and that although small studies may not provide a good basis for testing hypotheses, they may provide valuable estimates of treatment effects using confidence intervals. In this article, we examine these arguments in light of the distinctive moral issues associated with the conduct of underpowered trials, the disclosures that are owed to potential participants in underpowered trials so they may make autonomous enrollment decisions, and the circumstances in which the prospects for future meta-analyses may justify individually underpowered trials. We conclude that underpowered trials are ethical in only 2 situations: small trials of interventions for rare diseases in which investigators document explicit plans for including their results with those of similar trials in a prospective meta-analysis, and early-phase trials in the development of drugs or devices, provided they are adequately powered for defined purposes other than randomized treatment comparisons. In both cases, investigators must inform prospective subjects that their participation may only indirectly contribute to future health care benefits.",data/metaanalysis.pdf
"a. gallyer, sean p. dougherty, kreshnik burani, b. albanese, t. joiner, g. hajcak,","Suicidal thoughts, behaviors, and event-related potentials: A systematic review and meta-analysis.",2021,"Suicidal thoughts and behaviors (STBs) are thought to result from, at least in part, abnormalities in various neural systems. Event-related potentials (ERPs) are a useful method for studying neural activity and can be leveraged to study neural deficits related to STBs; however, it is unknown how effective ERPs are at differentiating various STB groups. The present meta-analysis examined how well ERPs can differentiate (a) those with and without suicidal ideation, (b) those with and without suicide attempts, (c) those with different levels of suicide risk, and (d) differences between those with suicide attempts versus those with suicidal ideation only. This meta-analysis included 208 effect sizes from 2,517 participants from 27 studies. We used a random-effects meta-analysis using a restricted maximum likelihood estimator with robust variance estimation. We meta-analyzed ERP-STB combinations that had at least three effect sizes across two or more studies. A qualitative review found that for each ERP and STB combination, the literature is highly mixed. Our meta-analyses largely did not find significant relationships between STBs and ERPs. We also found that the literature is likely severely underpowered, with most studies only being sufficiently powered to detect unrealistically large effect sizes. Our results provided little-to-no support for a reliable relationship between the ERPs assessed and STBs. However, the current literature is severely underpowered, and there are many methodological weaknesses that must be resolved before making this determination. We recommend large-scale collaboration and improvements in measurement practices to combat the issues in this literature.",data/metaanalysis.pdf
"r. keefe, h. kraemer, r. epstein, e. frank, ginger haynes, t. laughren, james mcnulty, s. reed, juan sanchez, a. leon,",Defining a clinically meaningful effect for the design and interpretation of randomized controlled trials.,2013,"OBJECTIVE
This article captures the proceedings of a meeting aimed at defining clinically meaningful effects for use in randomized controlled trials for psychopharmacological agents.


DESIGN
Experts from a variety of disciplines defined clinically meaningful effects from their perspectives along with viewpoints about how to design and interpret randomized controlled trials.


SETTING
The article offers relevant, practical, and sometimes anecdotal information about clinically meaningful effects and how to interpret them.


PARTICIPANTS
The concept for this session was the work of co-chairs Richard Keefe and the late Andy Leon. Faculty included Richard Keefe, PhD; James McNulty, AbScB; Robert S. Epstein, MD, MS; Shelby D. Reed, PhD; Juan Sanchez, MD; Ginger Haynes, PhD; Andrew C. Leon, PhD; Helena Chmura Kraemer, PhD; Ellen Frank, PhD, and Kenneth L. Davis, MD.


RESULTS
The term clinically meaningful effect is an important aspect of designing and interpreting randomized controlled trials but can be particularly difficult in the setting of psychopharmacology where effect size may be modest, particularly over the short term, because of a strong response to placebo. Payers, regulators, patients, and clinicians have different concerns about clinically meaningful effects and may describe these terms differently. The use of moderators in success rate differences may help better delineate clinically meaningful effects.


CONCLUSION
There is no clear consensus on a single definition for clinically meaningful differences in randomized controlled trials, and investigators must be sensitive to specific concerns of stakeholders in psychopharmacology in order to design and execute appropriate clinical trials.",data/metaanalysis.pdf
h. campbell,"If journals embraced conditional equivalence testing, would research be better?",2019,"We consider the reliability of published science: the probability that scientific claims put forth are true. Low reliability within many scientific fields is of major concern for researchers, scientific journals and the public at large. In the first part of this thesis, we introduce a publication policy that incorporates “conditional equivalence testing” (CET), a two-stage testing scheme in which standard nullhypothesis significance testing is followed, if the null hypothesis is not rejected, by testing for equivalence. The idea of CET has the potential to address recent concerns about reproducibility and the limited publication of null results. We detail the implementation of CET, investigate similarities with a Bayesian testing scheme, and outline the basis for how a scientific journal could proceed to reduce publication bias while remaining relevant. In the second part of this thesis, we consider proposals to adopt measures of “greater statistical stringency,” including suggestions to require larger sample sizes and to lower the highly criticized “p < 0.05” significance threshold. While pros and cons are vigorously debated, there has been little to no modeling of how adopting these measures might affect what type of science is published. We develop a novel model that, given current incentives to publish, predicts a researcher’s most rational use of resources in terms of the number of studies to undertake, the statistical power to devote to each study, and the desirable pre-study odds to pursue. Using this model, we investigate the merits of adopting measures of “greater statistical stringency” with the goal of informing the ongoing debate. We also use this model to investigate the merits of alternative publication policies, including the registered reports policy and our novel CET publication policy.",10.14288/1.0379722
"benjamin jurek, i. neumann,",The Oxytocin Receptor: From Intracellular Signaling to Behavior.,2018,"The many facets of the oxytocin (OXT) system of the brain and periphery elicited nearly 25,000 publications since 1930 (see FIGURE 1 , as listed in PubMed), which revealed central roles for OXT and its receptor (OXTR) in reproduction, and social and emotional behaviors in animal and human studies focusing on mental and physical health and disease. In this review, we discuss the mechanisms of OXT expression and release, expression and binding of the OXTR in brain and periphery, OXTR-coupled signaling cascades, and their involvement in behavioral outcomes to assemble a comprehensive picture of the central and peripheral OXT system. Traditionally known for its role in milk let-down and uterine contraction during labor, OXT also has implications in physiological, and also behavioral, aspects of reproduction, such as sexual and maternal behaviors and pair bonding, but also anxiety, trust, sociability, food intake, or even drug abuse. The many facets of OXT are, on a molecular basis, brought about by a single receptor. The OXTR, a 7-transmembrane G protein-coupled receptor capable of binding to either Gαi or Gαq proteins, activates a set of signaling cascades, such as the MAPK, PKC, PLC, or CaMK pathways, which converge on transcription factors like CREB or MEF-2. The cellular response to OXT includes regulation of neurite outgrowth, cellular viability, and increased survival. OXTergic projections in the brain represent anxiety and stress-regulating circuits connecting the paraventricular nucleus of the hypothalamus, amygdala, bed nucleus of the stria terminalis, or the medial prefrontal cortex. Which OXT-induced patterns finally alter the behavior of an animal or a human being is still poorly understood, and studying those OXTR-coupled signaling cascades is one initial step toward a better understanding of the molecular background of those behavioral effects.",data/metaanalysis.pdf
"k. dwan, d. altman, j. -. arnaiz, j. bloom, a. chan, eugenia cronin, e. decullier, p. easterbrook, e. von elm, c. gamble, d. ghersi, j. ioannidis, j. simes, p. williamson,",Systematic Review of the Empirical Evidence of Study Publication Bias and Outcome Reporting Bias,2008,"Background The increased use of meta-analysis in systematic reviews of healthcare interventions has highlighted several types of bias that can arise during the completion of a randomised controlled trial. Study publication bias has been recognised as a potential threat to the validity of meta-analysis and can make the readily available evidence unreliable for decision making. Until recently, outcome reporting bias has received less attention. Methodology/Principal Findings We review and summarise the evidence from a series of cohort studies that have assessed study publication bias and outcome reporting bias in randomised controlled trials. Sixteen studies were eligible of which only two followed the cohort all the way through from protocol approval to information regarding publication of outcomes. Eleven of the studies investigated study publication bias and five investigated outcome reporting bias. Three studies have found that statistically significant outcomes had a higher odds of being fully reported compared to non-significant outcomes (range of odds ratios: 2.2 to 4.7). In comparing trial publications to protocols, we found that 40–62% of studies had at least one primary outcome that was changed, introduced, or omitted. We decided not to undertake meta-analysis due to the differences between studies. Conclusions Recent work provides direct empirical evidence for the existence of study publication bias and outcome reporting bias. There is strong evidence of an association between significant results and publication; studies that report positive or significant results are more likely to be published and outcomes that are statistically significant have higher odds of being fully reported. Publications have been found to be inconsistent with their protocols. Researchers need to be aware of the problems of both types of bias and efforts should be concentrated on improving the reporting of trials.",data/metaanalysis.pdf
"robbie c. m. van aert, j. wicherts, m. v. van assen,",Publication bias examined in meta-analyses from psychology and medicine: A meta-meta-analysis,2019,"Publication bias is a substantial problem for the credibility of research in general and of meta-analyses in particular, as it yields overestimated effects and may suggest the existence of non-existing effects. Although there is consensus that publication bias exists, how strongly it affects different scientific literatures is currently less well-known. We examined evidence of publication bias in a large-scale data set of primary studies that were included in 83 meta-analyses published in Psychological Bulletin (representing meta-analyses from psychology) and 499 systematic reviews from the Cochrane Database of Systematic Reviews (CDSR; representing meta-analyses from medicine). Publication bias was assessed on all homogeneous subsets (3.8% of all subsets of meta-analyses published in Psychological Bulletin) of primary studies included in meta-analyses, because publication bias methods do not have good statistical properties if the true effect size is heterogeneous. Publication bias tests did not reveal evidence for bias in the homogeneous subsets. Overestimation was minimal but statistically significant, providing evidence of publication bias that appeared to be similar in both fields. However, a Monte-Carlo simulation study revealed that the creation of homogeneous subsets resulted in challenging conditions for publication bias methods since the number of effect sizes in a subset was rather small (median number of effect sizes equaled 6). Our findings are in line with, in its most extreme case, publication bias ranging from no bias until only 5% statistically nonsignificant effect sizes being published. These and other findings, in combination with the small percentages of statistically significant primary effect sizes (28.9% and 18.9% for subsets published in Psychological Bulletin and CDSR), led to the conclusion that evidence for publication bias in the studied homogeneous subsets is weak, but suggestive of mild publication bias in both psychology and medicine.",data/metaanalysis.pdf
m. egger g. smith martin schneider c. minder,"Bias in meta-analysis detected by a simple, graphical test",1997,"Abstract Objective: Funnel plots (plots of effect estimates against sample size) may be useful to detect bias in meta-analyses that were later contradicted by large trials. We examined whether a simple test of asymmetry of funnel plots predicts discordance of results when meta-analyses are compared to large trials, and we assessed the prevalence of bias in published meta-analyses. Design: Medline search to identify pairs consisting of a meta-analysis and a single large trial (concordance of results was assumed if effects were in the same direction and the meta-analytic estimate was within 30% of the trial); analysis of funnel plots from 37 meta-analyses identified from a hand search of four leading general medicine journals 1993-6 and 38 meta-analyses from the second 1996 issue of the Cochrane Database of Systematic Reviews. Main outcome measure: Degree of funnel plot asymmetry as measured by the intercept from regression of standard normal deviates against precision. Results: In the eight pairs of meta-analysis and large trial that were identified (five from cardiovascular medicine, one from diabetic medicine, one from geriatric medicine, one from perinatal medicine) there were four concordant and four discordant pairs. In all cases discordance was due to meta-analyses showing larger effects. Funnel plot asymmetry was present in three out of four discordant pairs but in none of concordant pairs. In 14 (38%) journal meta-analyses and 5 (13%) Cochrane reviews, funnel plot asymmetry indicated that there was bias. Conclusions: A simple analysis of funnel plots provides a useful test for the likely presence of bias in meta-analyses, but as the capacity to detect bias will be limited when meta-analyses are based on a limited number of small trials the results from such analyses should be treated with considerable caution. Key messages Systematic reviews of randomised trials are the best strategy for appraising evidence; however, the findings of some meta-analyses were later contradicted by large trials Funnel plots, plots of the trials' effect estimates against sample size, are skewed and asymmetrical in the presence of publication bias and other biases Funnel plot asymmetry, measured by regression analysis, predicts discordance of results when meta-analyses are compared with single large trials Funnel plot asymmetry was found in 38% of meta-analyses published in leading general medicine journals and in 13% of reviews from the Cochrane Database of Systematic Reviews Critical examination of systematic reviews for publication and related biases should be considered a routine procedure",10.1136/bmj.315.7109.629
alexander a. aarts joanna e. anderson christopher j. anderson peter raymond attridge a. attwood jordan r. axt molly babel š. bahník e. baranski m. barnett-cowan e. bartmess jennifer s. beer raoul bell heather bentley leah beyan grace binion d. borsboom annick bosch frank bosco sara d. bowman m. brandt e. l. braswell hilmar brohmer b. t. brown kristina brown jovita brüning ann calhoun-sauls shannon p. callahan e. chagnon jesse j. chandler christopher r. chartier f. cheung cody daniel christopherson linda cillessen r. clay hayley m. d. cleary m. cloud michael conn j. cohoon simon columbus a. cordes g. costantini leslie d. cramblet alvarez e. cremata j. crusius j. decoster m. degaetano nicolás delia penna bobby den bezemer m. deserno olivia devitt l. dewitte david g. dobolyi geneva t. dodson m. donnellan ryan donohue rebecca a. dore a. dorrough anna dreber m. dugas elizabeth w. dunn kayleigh e. easey sylvia eboigbe c. eggleston jo embley s. epskamp timothy m. errington vivien estel frank j. farach j. feather a. fedor b. fernández-castilla s. fiedler james g. field stanka a. fitneva t. flagan amanda l. forest eskil forsell j. d. foster michael c. frank rebecca s. frazier heather m. fuchs p. gable jeff galak e. galliani anup gampa sara garcia douglas gazarian e. gilbert roger giner-sorolla a. glöckner lars goellner jin x. goh r. goldberg patrick t. goodbourn s. gordon-mckeon bryan h. gorges jessie gorges j. goss j. graham j. grange j. gray c.h.j. hartgerink joshua k. hartshorne f. hasselman timothy hayes emma heikensten felix henninger j. hodsoll taylor holubar g. hoogendoorn d. humphries c. o. hung n. immelman vanessa c. irsik g. jahn f. jäkel marc jekel m. johannesson l. johnson david j. johnson kate m. johnson william johnston k. jonas jennifer a. joy-gaba h. kappes kim kelso mallory c. kidwell seung k. kim m. kirkhart bennett kleinberg g. knežević f. kolorz j. kossakowski r. krause j.m.t. krijnen t. kuhlmann y. kunkels megan m. kyc calvin k. lai aamir laique d. lakens kristin a. lane bethany lassetter l. lazarević e. bel key jung lee minha lee k. lemm c. levitan melissa lewis lin lin stephanie c. lin matthias lippold darren loureiro ilse luteijn s. mackinnon heather n. mainard denise c. marigold d. p. martin tylar martinez e. masicampo joshua j. matacotta maya b. mathur michael may nicole c mechin p. mehta johannes m. meixner alissa melinger jeremy k. miller mallorie miller k. moore marcus möschl matt motyl s. muller m. munafo k. neijenhuijs taylor nervi gandalf nicolas g. nilsonne brian a. nosek michèle b. nuijten catherine olsson c. osborne lutz ostkamp m. pavel i. penton-voak o. perna c. pernet m. perugini r. pipitone m. pitts f. plessow j. prenoveau r. rahal kate a. ratliff david a. reinhard frank renkewitz ashley a. ricker a. rigney andrew m rivers mark a. roebke abraham m. rutchick robert s. ryan o. şahin anondah saide gillian m. sandstrom david santos r. saxe r. schlegelmilch kathleen schmidt sabine scholz l. seibel dylan selterman s. shaki w. b. simpson h. sinclair jeanine l. m. skorinko a. slowik j. snyder c. soderberg carina m. sonnleitner n. spencer jeffrey r. spies s. steegen s. stieger nina strohminger g. sullivan t. talhelm m. tapia a. t. dorsthorst m. thomae sarah l. thomas pia tio frits traets steve tsang f. tuerlinckx paul turchan milan valášek a. v. veer r. v. aert m. v. assen r. v. bork mathijs van de ven d. v. d. bergh m. hulst r. v. dooren j. doorn d. v. renswoude h. rijn w. vanpaemel alejandro echeverría melissa vazquez natalia vélez marieke vermue m. verschoor m. vianello m. voracek gina vuu e. wagenmakers joanneke weerdmeester a. welsh erin c. westgate joeri wissink m. wood a. woods emily m. wright sining wu m. zeelenberg kellylynn zuni,Estimating the reproducibility of psychological science,2015,"Empirically analyzing empirical evidence One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study. Science, this issue 10.1126/science.aac4716 A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired. INTRODUCTION Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error. RATIONALE There is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science. RESULTS We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P < .05). Thirty-six percent of replications had significant results; 47% of original effect sizes were in the 95% confidence interval of the replication effect size; 39% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams. CONCLUSION No single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here. Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that “we already know this” belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know. Original study effect size versus replication effect size (correlation coefficients). Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects. Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47% of original effect sizes were in the 95% confidence interval of the replication effect size; 39% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.",10.1126/science.aac4716
