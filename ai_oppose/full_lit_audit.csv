Author,Title,Publication Year,Abstract Note,SOURCE
"Gaddis, S. Michael, and Raj Ghoshal",A Racial Stratification System for the 21st Century: The Causes and Consequences of Discrimination among Millennials in Two Experiments,2017,,data/audit.pdf --> https://soc.ucla.edu/wp-content/uploads/2020/12/smig_c.v._7-19.pdf
"David J. Deming, Noam Yuchtman, Amira Abulaﬁ, Claudia Goldin, Lawrence F. Katz",The Value of Postsecondary Credentials in the Labor Market: An Experimental,2016,,data/audit.pdf --> https://www.aeaweb.org/articles?id=10.1257/aer.20141757
"Colin S. Campbell, S. Michael Gaddis",‘I Don’t Agree with Giving Cash’: A Survey Experiment Examining Support for Public Assistance,Forthcoming,,data/audit.pdf --> https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2599513
"j. heckman, peter siegelman,",The Urban Institute Audit Studies: Their Methods and Findings,1993,,data/audit.pdf --> https://www.semanticscholar.org/paper/The-Urban-Institute-Audit-Studies%3A-Their-Methods-Heckman-Siegelman/bb523775490d252f7a84fe8030748f9faad6773a
scott a. wright j. goodman,Mechanical Turk in Consumer Research,2019,,10.4324/9781351137713-18 --> https://www.taylorfrancis.com/chapters/edit/10.4324/9781351137713-18/mechanical-turk-consumer-research-scott-wright-joseph-goodman
"Steven D. Levitt, Stephen J. Dubner",Freakonomics: A Rogue Economist Explores the Hidden Side of Everything,2005,,data/audit.pdf --> https://en.wikipedia.org/wiki/Freakonomics
"Gaddis, S. Michael","A Field Experiment on Associate Degrees and Certificates: Statistical Discrimination, Stigma, Signal Boost, and Signal Saturation",2016,,data/audit.pdf --> https://psu-us.academia.edu/SMichaelGaddis/CurriculumVitae
"Heckman, James J.",Detecting Discrimination,1998,,data/audit.pdf --> https://www.aeaweb.org/articles?id=10.1257/jep.12.2.101
"rajiv sharma, a. mitra, m. stano,","Insurance, race/ethnicity, and sex in the search for a new physician",2015,,data/audit.pdf --> https://pdxscholar.library.pdx.edu/cgi/viewcontent.cgi?article=1038&context=econ_fac
"Dowling, Conor, and Michael G. Miller",Experimental Evidence on the Relationship Between Interest Group Funding and Candidate Vote Share,2016,,data/audit.pdf --> https://www.cambridge.org/core/journals/journal-of-experimental-political-science/article/experimental-evidence-on-the-relationship-between-candidate-funding-sources-and-voter-evaluations/1406498692CAB3AB3F913D0D176CBFBC
"a. hanson, zackary b. hawley, hal martin, bo liu,",Discrimination in mortgage lending: Evidence from a correspondence experiment☆,2016,,data/audit.pdf --> https://epublications.marquette.edu/econ_fac/555/
arne weigold ingrid k. weigold mi-seon jang emily m. thornton,College students’ and Mechanical Turk workers’ environmental factors while completing online surveys,2021,,10.1007/s11135-021-01237-0 --> https://www.researchgate.net/publication/354610998_College_students'_and_Mechanical_Turk_workers'_environmental_factors_while_completing_online_surveys
"Joleen Kirschenman, Kathryn M. Neckerman",‘We’d Love to Hire Them But...’: The Meaning of Race to Employers,1991,,data/audit.pdf --> https://www.taylorfrancis.com/chapters/edit/10.4324/9780429497896-16/love-hire-joleen-kirshenman-kathryn-neckerman
d. chandler a. kapelner,Breaking Monotony with Meaning: Motivation in Crowdsourcing Markets,2012,,10.1016/j.jebo.2013.03.003 --> https://arxiv.org/abs/1210.0962
kyle a. thomas scott clifford,Validity and Mechanical Turk: An assessment of exclusion methods and interactive experiments,2017,Highlights • Online participant recruitment has led to persistent concerns about data quality. • Online participants are just as attentive as participants recruited offline. • Online participants buy into experimental social interactions as much as in the lab. • Rigorous exclusion methods can be used to improve data quality online and offline.,10.1016/j.chb.2017.08.038 --> https://www.sciencedirect.com/science/article/pii/S074756321730506X
"rajeev darolia, c. koedel, paco martorell, katie wilson, f. perez-arce,",Race and gender effects on employer interest in job applicants: new evidence from a resume field experiment,2016,ABSTRACT We sent nearly 9000 fictitious resumes to advertisements for job openings in seven major cities in the United States across six occupational categories. We randomly assigned names to the resumes that convey race and gender but for which a strong socio-economic connotation is not implicated. We find little evidence of systematic employer preferences for applicants from particular race and gender groups.,data/audit.pdf
neil stewart jesse j. chandler gabriele paolacci,Crowdsourcing Samples in Cognitive Science,2017,"Crowdsourcing data collection from research participants recruited from online labor markets is now common in cognitive science. We review who is in the crowd and who can be reached by the average laboratory. We discuss reproducibility and review some recent methodological innovations for online experiments. We consider the design of research studies and arising ethical issues. We review how to code experiments for the web, what is known about video and audio presentation, and the measurement of reaction times. We close with comments about the high levels of experience of many participants and an emerging tragedy of the commons.",10.1016/j.tics.2017.06.007 --> https://pubmed.ncbi.nlm.nih.gov/28803699/
"a. hanson, zackary b. hawley,",Do landlords discriminate in the rental housing market? Evidence from an internet field experiment in US cities,2010,"This paper tests for racial discrimination in the rental housing market using matched-pair audits conducted via e-mail for rental units advertised on-line. We reveal home-seekers’ race to landlords by sending e-mails from names with a high likelihood of association with either whites or African Americans. Generally, discrimination occurs against African American names; however, when the content of the e-mail messages insinuates home-seekers with high social class, discrimination is non-existent. Racial discrimination is more severe in neighborhoods that are near “tipping points” in racial composition, and for units that are part of a larger building.",data/audit.pdf --> https://www.sciencedirect.com/science/article/pii/S0094119011000179
gabriele paolacci jesse j. chandler,Inside the Turk,2014,"Mechanical Turk (MTurk), an online labor market created by Amazon, has recently become popular among social scientists as a source of survey and experimental data. The workers who populate this market have been assessed on dimensions that are universally relevant to understanding whether, why, and when they should be recruited as research participants. We discuss the characteristics of MTurk as a participant pool for psychology and other social sciences, highlighting the traits of the MTurk samples, why people become MTurk workers and research participants, and how data quality on MTurk compares to that from other pools and depends on controllable and uncontrollable factors.",10.1177/0963721414531598
s. e. woo melissa g. keith meghan a. thornton,"Amazon Mechanical Turk for Industrial and Organizational Psychology: Advantages, Challenges, and Practical Recommendations",2015,"We are in almost full agreement with Landers and Behrend's (2015) thoughtful and balanced critiques of various convenience sampling strategies focusing on the four most frequently used data sources in our field. In this commentary, we expand on Landers and Behrend's discussions specifically around Mechanical Turk (MTurk) by providing further supporting voice and/or clarity to the four potential concerns and relative advantages associated with MTurk. We also raise a few additional concerns and challenges to which the current literature does not yet offer definitive answers. We conclude with some practical guidelines summarizing the relative advantages and unique challenges of using MTurk.",10.1017/iop.2015.21
matt lovett saleh bajaba myra lovett marcia j. simmering,Data Quality from Crowdsourced Surveys: A Mixed Method Inquiry into Perceptions of Amazon's Mechanical Turk Masters,2018,"Researchers in the social sciences are increasingly turning to online data collection panels for research purposes. While there is evidence that crowdsourcing platforms such as Amazon s Mechanical Turk can produce data as reliable as more traditional survey collection methods, little is known about Amazon s Mechanical Turk s most experienced respondents, their perceptions of crowdsourced data, and the degree to which these affect data quality. The current study utilises both quantitative and qualitative data to investigate Amazon s Mechanical Turk Masters perceptions and attitudes related to the data quality (e.g. inattention). Recommendations for researchers using crowdsourcing data are provided.",10.1111/apps.12124
s. m. gaddis,An Introduction to Audit Studies in the Social Sciences,2017,"An audit study is a specific type of field experiment primarily used to test for discriminatory behavior when survey and interview questions induce social desirability bas. In this chapter, I first review the language and definitions related to audit studies and encourage adoption of a common language. I then discuss why researchers use the audit method as well as when researchers can and should use this method. Next, I give an overview of the history of audit studies, focusing on major developments and changes in the overall body of work. Finally, I discuss the limitations of correspondence audits and provide some thoughts on future directions. Keywords Audit studies Correspondence audits Discrimination Field experiments",10.1007/978-3-319-71153-9_1 --> https://link.springer.com/chapter/10.1007/978-3-319-71153-9_1
"gabriele paolacci, jesse j. chandler, panagiotis g. ipeirotis,",Running Experiments on Amazon Mechanical Turk,2010,"Although Mechanical Turk has recently become popular among social scientists as a source of experimental data, doubts may linger about the quality of data provided by subjects recruited from online labor markets. We address these potential concerns by presenting new demographic data about the Mechanical Turk subject population, reviewing the strengths of Mechanical Turk relative to other online and offline methods of recruiting subjects, and comparing the magnitude of effects obtained using Mechanical Turk and traditional subject pools. We further discuss some additional benefits such as the possibility of longitudinal, cross cultural and prescreening designs, and offer some advice on how to best manage a common subject pool.",data/audit.pdf
jesse j. chandler pam mueller gabriele paolacci,Nonnaïveté among Amazon Mechanical Turk workers: Consequences and solutions for behavioral researchers,2013,"Crowdsourcing services—particularly Amazon Mechanical Turk—have made it easy for behavioral scientists to recruit research participants. However, researchers have overlooked crucial differences between crowdsourcing and traditional recruitment methods that provide unique opportunities and challenges. We show that crowdsourced workers are likely to participate across multiple related experiments and that researchers are overzealous in the exclusion of research participants. We describe how both of these problems can be avoided using advanced interface features that also allow prescreening and longitudinal data collection. Using these techniques can minimize the effects of previously ignored drawbacks and expand the scope of crowdsourcing as a tool for psychological research.",10.3758/s13428-013-0365-7 --> https://link.springer.com/article/10.3758/s13428-013-0365-7
david johnson j. ryan,Amazon Mechanical Turk workers can provide consistent and economically meaningful data,2018,"We explore the consistency of the characteristics of individuals who participate in studies posted on Amazon Mechanical Turk (AMT). The primary individuals analyzed in this study are subjects who participated in at least two of eleven experiments that were run on AMT between September of 2012 to January of 2018. We demonstrate subjects consistently report a series of demographic and personality characteristics. Further, subjective willingness to take risk is found to be significantly correlated with decisions made in a simple lottery experiment with real stakes - even when the subjective risk measure is reported months, sometimes years, in the past. This suggests the quality of data obtained via AMT is not significantly harmed by the lack of control over the conditions under which the responses are recorded.",10.1002/SOEJ.12451
"panagiotis g. ipeirotis,",Demographics of Mechanical Turk,2010,"We present the results of a survey that collected information about the demographics of participants on Amazon Mechanical Turk, together with information about their level of activity and motivation for working on Amazon Mechanical Turk. We find that approximately 50% of the workers come from the United States and 40% come from India. Country of origin tends to change the motivating reasons for workers to participate in the marketplace. Significantly more workers from India participate on Mechanical Turk because the online marketplace is a primary source of income, while in the US most workers consider Mechanical Turk a secondary source of income. While money is a primary motivating reason for workers to participate in the marketplace, workers also cite a variety of other motivating reasons, including entertainment and education.",data/audit.pdf
ronnie jia zachary r. steelman b. reich,"Using Mechanical Turk Data in IS Research: Risks, Rewards, and Recommendations",2017,": With the increasing use of crowdsourced data in behavioral research fields, it is important to examine their appropriateness and desirability for IS research. Extending recent work in the IS literature, this tutorial discusses the risks and rewards of using data gathered on Amazon’s Mechanical Turk. We examine the characteristics of MTurk workers and the resulting method biases that may be exacerbated in MTurk data. Based on this analysis, we present a 2x2 matrix to illustrate the categories of IS research questions that are and are not amenable to MTurk data. We suggest that MTurk data is more appropriate for generalizing studies that examine diverse cognition than for contextualizing studies or those involving shared cognition. Finally, we offer a set of practical recommendations for researchers who wish to collect data on MTurk.",10.17705/1cais.04114
"a. berinsky, g. huber, gabriel s. lenz,",Evaluating Online Labor Markets for Experimental Research: Amazon.com's Mechanical Turk,2012,"We examine the trade-offs associated with using Amazon.com's Mechanical Turk (MTurk) interface for subject recruitment. We first describe MTurk and its promise as a vehicle for performing low-cost and easy-to-field experiments. We then assess the internal and external validity of experiments performed using MTurk, employing a framework that can be used to evaluate other subject pools. We first investigate the characteristics of samples drawn from the MTurk population. We show that respondents recruited in this manner are often more representative of the U.S. population than in-person convenience samples—the modal sample in published experimental political science—but less representative than subjects in Internet-based panels or national probability samples. Finally, we replicate important published experimental work using MTurk samples.",data/audit.pdf
y. lee yong-won seo enno siemsen,Running Behavioral Operations Experiments Using Amazon's Mechanical Turk,2017,"Mechanical Turk (MTurk), an online labor market run by Amazon, provides a platform for conducting behavioral experiments; the site offers immediate and inexpensive access to a large global subject pool. In this paper, we review recent research about MTurk and test the validity of using MTurk for experiments in behavioral operations management. We used subjects from MTurk to replicate the experiments in Bolton and Katok (2008), Engelbrecht-Wiggans and Katok (2008), Loch and Wu (2008), and Bolton, Ockenfels and Thonemann (2012). Our results are similar to the originals, but we also document some important differences. MTurk appears to be an important and relevant tool for researchers in behavioral operations, but we caution researchers to restrict use of this subject pool to experiments involving short-lived stimuli and behavioral manipulations.",10.2139/ssrn.2972406
"n. jacquemet, constantine yannelis,",Indiscriminate Discrimination: A Correspondence Test for Ethnic Homophily in the Chicago Labor Market,2011,"Numerous field experiments have demonstrated the existence of discrimination in labor markets against specific minority groups. This paper uses a correspondence test to determine whether this discrimination is due to prejudice against specific groups, or a general preference for the majority group. Three groups of identical fabricated resumes are sent to help-wanted advertisements in Chicago newspapers: one with Anglo-Saxon names, one with African-American names, and one with fictitious foreign names whose ethnic origin is unidentifiable to most Americans. Resumes with Anglo-Saxon names generate nearly one third more call-backs than identical resumes with non Anglo-Saxon ones, either African-American or Foreign. We take this as evidence that discriminatory behavior is part of a larger pattern of unequal treatment of any member of non-majority groups, ethnic homophily.",data/audit.pdf
"corrado giulietti, m. tonin, michael vlassopoulos,",Racial Discrimination in Local Public Services: A Field Experiment in the US,2015,"Discrimination in access to public services can act as a major obstacle towards addressing racial inequality. We examine whether racial discrimination exists in access to a wide spectrum of public services in the US. We carry out an email correspondence study in which we pose simple queries to more than 19,000 local public service providers. We find that emails are less likely to receive a response if signed by a black-sounding name compared to a white-sounding name. Given a response rate of 72% for white senders, emails from putatively black senders are almost 4 percentage points less likely to receive an answer. We also find that responses to queries coming from black names are less likely to have a cordial tone. Further tests suggest that the differential in the likelihood of answering is due to animus towards blacks rather than inferring socioeconomic status from race.",data/audit.pdf
roland g. fryer s. levitt,The Causes and Consequences of Distinctively Black Names,2003,"In the 1960's, Blacks and Whites chose relatively similar first names for their children. Over a short period of time in the early 1970's, that pattern changed dramatically with most Blacks (particularly those living in racially isolated neighborhoods) adopting increasingly distinctive names, but a subset of Blacks actually moving toward more assimilating names. The patterns in the data appear most consistent with a model in which the rise of the Black Power movement influenced how Blacks perceived their identities. Among Blacks born in the last two decades, names provide a strong signal of socio-economic status, which was not previously the case. We find, however, no negative causal impact of having a distinctively Black name on life outcomes. Although that result is seemingly in conflict with previous audit studies involving resumes, we argue that the two sets of findings can be reconciled.",10.1162/0033553041502180
s. m. gaddis,Understanding the 'How' and 'Why' Aspects of Racial/Ethnic Discrimination: A Multi-Method Approach to Audit Studies,2019,"Researchers have used audit studies to provide causal evidence of racial discrimination for nearly sixty years. While audits are excellent methodological tools to investigate the “what,” “where,” and “when” aspects of racial/ethnic discrimination, they are less appropriate, by themselves, to investigate the “how,” and “why” aspects of racial/ethnic discrimination. In this article, I review why audit studies are necessary to study racial/ethnic discrimination, the evidence from audit studies, and their limitations. I then argue that scholars should adopt a multi-method approach to audit studies to move from documenting the existence of racial/ethnic discrimination to examining how and why racial/ethnic discrimination occurs. Adoption of this multi-method approach will result in a deeper understanding of racial/ethnic discrimination with the potential to shape both opinions and policy surrounding discrimination.",10.2139/ssrn.3426846
"eyal péer, j. vosgerau, a. acquisti,",Reputation as a sufficient condition for data quality on Amazon Mechanical Turk,2014,"Data quality is one of the major concerns of using crowdsourcing websites such as Amazon Mechanical Turk (MTurk) to recruit participants for online behavioral studies. We compared two methods for ensuring data quality on MTurk: attention check questions (ACQs) and restricting participation to MTurk workers with high reputation (above 95% approval ratings). In Experiment 1, we found that high-reputation workers rarely failed ACQs and provided higher-quality data than did low-reputation workers; ACQs improved data quality only for low-reputation workers, and only in some cases. Experiment 2 corroborated these findings and also showed that more productive high-reputation workers produce the highest-quality data. We concluded that sampling high-reputation workers can ensure high-quality data without having to resort to using ACQs, which may lead to selection bias if participants who fail ACQs are excluded post-hoc.",data/audit.pdf --> https://link.springer.com/article/10.3758/s13428-013-0434-y
a. arechar gordon t. kraft-todd david g. rand,Turking overtime: how participant characteristics and behavior vary over time and day on Amazon Mechanical Turk,2017,"Online experiments allow researchers to collect datasets at times not typical of laboratory studies. We recruit 2,336 participants from Amazon Mechanical Turk to examine if participant characteristics and behaviors differ depending on whether the experiment is conducted during the day versus night, and on weekdays versus weekends. Participants make incentivized decisions involving prosociality, punishment, and discounting, and complete a demographic and personality survey. We find no time or day differences in behavior, but do find that participants at nights and on weekends are less experienced with online studies; on weekends are less reflective; and at night are less conscientious and more neurotic. These results are largely robust to finer grained measures of time and day. We also find that those who participated earlier in the course of the study are more experienced, reflective, and agreeable, but less charitable than later participants.",10.1007/s40881-017-0035-0 --> https://pubmed.ncbi.nlm.nih.gov/29130029/
s. m. gaddis,How Black Are Lakisha and Jamal? Racial Perceptions from Names Used in Correspondence Audit Studies,2017,"Online correspondence audit studies have emerged as the primary method to examine racial discrimination. Although audits use distinctive names to signal race, few studies scientifically examine data regarding the perception of race from names. Different names treated as black or white may be perceived in heterogeneous ways. I conduct a survey experiment that asks respondents to identify the race they associate with a series of names. I alter the first names given to each respondent and inclusion of last names. Names more commonly given by highly educated black mothers (e.g., Jalen and Nia) are less likely to be perceived as black than names given by less educated black mothers (e.g., DaShawn and Tanisha). The results suggest that a large body of social science evidence on racial discrimination operates under a misguided assumption that all black names are alike, and the findings from correspondence audits are likely sensitive to name selection.",10.15195/V4.A19
"Michael Buhrmester, Tracy Kwang, Samuel D. Gosling","Amazon’s Mechanical Turk: A New Source of Inexpensive, Yet High-Quality, Data?",2011,"Amazon's Mechanical Turk (MTurk) is a relatively new website that contains the major elements required to conduct research: an integrated participant compensation system; a large participant pool; and a streamlined process of study design, participant recruitment, and data collection. In this article, we describe and evaluate the potential contributions of MTurk to psychology and other social sciences. Findings indicate that (a) MTurk participants are slightly more demographically diverse than are standard Internet samples and are significantly more diverse than typical American college samples; (b) participation is affected by compensation rate and task length, but participants can still be recruited rapidly and inexpensively; (c) realistic compensation rates do not affect data quality; and (d) the data obtained are at least as reliable as those obtained via traditional methods. Overall, MTurk can be used to obtain high-quality data inexpensively and rapidly.",data/audit.pdf --> https://pubmed.ncbi.nlm.nih.gov/26162106/
jesse j. chandler d. shapiro,Conducting Clinical Research Using Crowdsourced Convenience Samples.,2016,"Crowdsourcing has had a dramatic impact on the speed and scale at which scientific research can be conducted. Clinical scientists have particularly benefited from readily available research study participants and streamlined recruiting and payment systems afforded by Amazon Mechanical Turk (MTurk), a popular labor market for crowdsourcing workers. MTurk has been used in this capacity for more than five years. The popularity and novelty of the platform have spurred numerous methodological investigations, making it the most studied nonprobability sample available to researchers. This article summarizes what is known about MTurk sample composition and data quality with an emphasis on findings relevant to clinical psychological research. It then addresses methodological issues with using MTurk--many of which are common to other nonprobability samples but unfamiliar to clinical science researchers--and suggests concrete steps to avoid these issues or minimize their impact.",10.1146/annurev-clinpsy-021815-093623
jake haselswerdt,"Who Benefits? Race, Immigration, and Assumptions About Policy",2020,"Existing scholarship suggests that attitudes about the real or imagined beneficiaries or targets of public policies shape public opinion about those policies, with racial and ethnic stereotypes driving policy evaluations for many Americans. Despite the importance of these assumptions, we lack strong evidence about how and why people form such assumptions in the first place. In a pre-registered survey experiment, I demonstrate that elements of policy design (e.g., a work requirement) significantly affect the assumptions that individuals make about policy beneficiaries (their race and national origin). These assumptions shape individuals’ evaluations of the policy, conditional on existing attitudes (e.g., racial resentment). Importantly, existing attitudes do not condition the effects at the assumption stage: even those who profess not to believe in racial stereotypes about work ethic still assume that the absence of a work requirement makes a policy more likely to benefit blacks and immigrants.",10.1007/s11109-020-09608-3 --> https://link.springer.com/article/10.1007/s11109-020-09608-3
"k. einstein, d. glick,",Does Race Affect Access to Government Services? An Experiment Exploring Street-Level Bureaucrats and Access to Public Housing,2017,"While experimental studies of local election officials have found evidence of racial discrimination, we know little about whether these biases manifest in bureaucracies that provide access to valuable government programs and are less tied to politics. We address these issues in the context of affordable housing programs using a randomized field experiment. We explore responsiveness to putative white, black, and Hispanic requests for aid in the housing application process. In contrast to prior findings, public housing officials respond at equal rates to black and white email requests. We do, however, find limited evidence of responsiveness discrimination toward Hispanics. Moreover, we observe substantial differences in email tone. Hispanic housing applicants were 20 percentage points less likely to be greeted by name than were their black and white counterparts. This disparity in tone is somewhat more muted in more diverse locations, but it does not depend on whether a housing official is Hispanic.",data/audit.pdf
"christine horne, brice darras, e. bean, anurag srivastava, s. frickel,","Privacy, technology, and norms: the case of Smart Meters.",2015,"Norms shift and emerge in response to technological innovation. One such innovation is Smart Meters - components of Smart Grid energy systems capable of minute-to-minute transmission of consumer electricity use information. We integrate theory from sociological research on social norms and privacy to examine how privacy threats affect the demand for and expectations of norms that emerge in response to new technologies, using Smart Meters as a test case. Results from three vignette experiments suggest that increased threats to privacy created by Smart Meters are likely to provoke strong demand for and expectations of norms opposing the technology and that the strength of these normative rules is at least partly conditional on the context. Privacy concerns vary little with actors' demographic characteristics. These findings contribute to theoretical understanding of norm emergence and have practical implications for implementing privacy protections that effectively address concerns of electricity users.",data/audit.pdf --> https://pubmed.ncbi.nlm.nih.gov/25769852/
s. m. gaddis,Racial/Ethnic Perceptions from Hispanic Names: Selecting Names to Test for Discrimination,2017,"Researchers increasingly use correspondence audit studies to study racial/ethnic discrimination in employment, housing, and other domains. Although this method provides strong causal evidence of racial/ethnic discrimination, these claims depend on the signal being clearly conveyed through names. Few studies have pretested individual racial and ethnic perceptions of the names used to examine discrimination. The author conducts a survey experiment in which respondents are asked to identify the races or ethnicities they associate with a series of names. Respondents are provided with combinations of Hispanic and Anglo first and last names. Hispanic first names paired with Anglo last names are least likely to be recognized as Hispanic, while all versions of Hispanic first and last names are highly recognized (≥90 percent). The results suggest that researchers must use caution when trying to signal Hispanic ethnicity in experiments, and prior findings from correspondence audits may be biased from poor signals.",10.1177/2378023117737193
"daniel m. butler, david e. broockman,",Do Politicians Racially Discriminate Against Constituents? A Field Experiment on State Legislators,2011,"We use a field experiment to investigate whether race affects how responsive state legislators are to requests for help with registering to vote. In an email sent to each legislator, we randomized whether a putatively black or white alias was used and whether the email signaled the sender’s partisan preference. Overall, we find that putatively black requests receive fewer replies. We explore two potential explanations for this discrimination: strategic partisan behavior and the legislators’ own race. We find that the putatively black alias continues to be differentially treated even when the emails signal partisanship, indicating that strategic considerations cannot completely explain the observed differential treatment. Further analysis reveals that white legislators of both parties exhibit similar levels of discrimination against the black alias. Minority legislators do the opposite, responding more frequently to the black alias. Implications for the study of race and politics in the United States are discussed.",data/audit.pdf
patrick button brigham walker,Employment Discrimination Against Indigenous Peoples in the United States: Evidence from a Field Experiment,2019,"We conducted an audit study - a resume correspondence experiment - to measure discrimination in hiring faced by Indigenous Peoples in the United States (Native Americans, Alaska Natives, and Native Hawaiians). We sent employers 13,516 realistic resumes of Indigenous or white applications for common jobs in 11 cities. We signalled Indigenous status in one of four different ways. Interview offer rates do not differ by race, which holds after an extensive battery of robustness checks. We discuss multiple concerns such as the saliency of signals, selection of cities and occupations, and labour market tightness that could affect the results of our audit study and those of others. We also conduct decompositions of wages, unemployment rates, unemployment durations, and employment durations to explore if discrimination might exist in contexts outside our experiment. We conclude by highlighting the essential tests and considerations that are important for future audit studies, regardless of if they find discrimination or not.",10.1016/J.LABECO.2020.101851 --> https://pubmed.ncbi.nlm.nih.gov/32655210/
j. goodman gabriele paolacci,Crowdsourcing Consumer Research,2017,"Data collection in consumer research has progressively moved away from traditional samples (e.g., university undergraduates) and toward Internet samples. In the last complete volume of the Journal of Consumer Research (June 2015–April 2016), 43% of behavioral studies were conducted on the crowdsourcing website Amazon Mechanical Turk (MTurk). The option to crowdsource empirical investigations has great efficiency benefits for both individual researchers and the field, but it also poses new challenges and questions for how research should be designed, conducted, analyzed, and evaluated. We assess the evidence on the reliability of crowdsourced populations and the conditions under which crowdsourcing is a valid strategy for data collection. Based on this evidence, we propose specific guidelines for researchers to conduct high-quality research via crowdsourcing. We hope this tutorial will strengthen the community’s scrutiny on data collection practices and move the field toward better and more valid crowdsourcing of consumer research.",10.1093/JCR/UCX047
"tara s. behrend, d. sharek, a. meade, e. wiebe,",The viability of crowdsourcing for survey research,2011,"Online contract labor portals (i.e., crowdsourcing) have recently emerged as attractive alternatives to university participant pools for the purposes of collecting survey data for behavioral research. However, prior research has not provided a thorough examination of crowdsourced data for organizational psychology research. We found that, as compared with a traditional university participant pool, crowdsourcing respondents were older, were more ethnically diverse, and had more work experience. Additionally, the reliability of the data from the crowdsourcing sample was as good as or better than the corresponding university sample. Moreover, measurement invariance generally held across these groups. We conclude that the use of these labor portals is an efficient and appropriate alternative to a university participant pool, despite small differences in personality and socially desirable responding across the samples. The risks and advantages of crowdsourcing are outlined, and an overview of practical and ethical guidelines is provided.",data/audit.pdf --> https://pubmed.ncbi.nlm.nih.gov/21437749/
s. m. gaddis r. ghoshal,Searching for a Roommate: A Correspondence Audit Examining Racial/Ethnic and Immigrant Discrimination among Millennials,2020,"Survey research finds that millennials have less prejudiced views of racial/ethnic minorities than other generations, leading some to label millennials as postracial. However, attitudinal survey research may be subject to social desirability bias because it documents statements or beliefs instead of actions. Moreover, most audit studies focus on people who make hiring decisions or own rental property and are therefore often older than millennials. This study uses a correspondence audit to investigate discrimination among millennials via “roommate wanted” advertisements. We sent over 4,000 emails and found a tiered pattern of discrimination against Asian (Indian and Chinese), Hispanic, and Black room-seekers. However, whether Asian and Hispanic room-seekers face significant discrimination varies based on whether they use predominantly White first names or traditional first names. Our findings shed light on the future of our racial system, expand our knowledge of discrimination beyond the traditional Black/White binary, and illustrate the persistence of anti-Blackness.",10.1177/2378023120972287
"david n. figlio,","Names, Expectations and the Black-White Test Score Gap",2005,"This paper investigates the question of whether teachers treat children differentially on the basis of factors other than observed ability, and whether this differential treatment in turn translates into differences in student outcomes. I suggest that teachers may use a child's name as a signal of unobserved parental contributions to that child's education, and expect less from children with names that ""sound"" like they were given by uneducated parents. These names, empirically, are given most frequently by Blacks, but they are also given by White and Hispanic parents as well. I utilize a detailed dataset from a large Florida school district to directly test the hypothesis that teachers and school administrators expect less on average of children with names associated with low socio-economic status, and these diminished expectations in turn lead to reduced student cognitive performance. Comparing pairs of siblings, I find that teachers tend to treat children differently depending on their names, and that these same patterns apparently translate into large differences in test scores.",data/audit.pdf
"katherine l. milkman, m. akinola, d. chugh,",Temporal Distance and Discrimination,2012,"Through a field experiment set in academia (with a sample of 6,548 professors), we found that decisions about distant-future events were more likely to generate discrimination against women and minorities (relative to Caucasian males) than were decisions about near-future events. In our study, faculty members received e-mails from fictional prospective doctoral students seeking to schedule a meeting either that day or in 1 week; students’ names signaled their race (Caucasian, African American, Hispanic, Indian, or Chinese) and gender. When the requests were to meet in 1 week, Caucasian males were granted access to faculty members 26% more often than were women and minorities; also, compared with women and minorities, Caucasian males received more and faster responses. However, these patterns were essentially eliminated when prospective students requested a meeting that same day. Our identification of a temporal discrimination effect is consistent with the predictions of construal-level theory and implies that subtle contextual shifts can alter patterns of race- and gender-based discrimination.",data/audit.pdf
melissa g. keith l. tay p. harms,Systems Perspective of Amazon Mechanical Turk for Organizational Research: Review and Recommendations,2017,"Amazon Mechanical Turk (MTurk) is becoming a prevalent source of quick and cost effective data for organizational research, but there are questions about the appropriateness of the platform for organizational research. To answer these questions, we conducted an integrative review based on 75 papers evaluating the MTurk platform and 250 MTurk samples used in organizational research. This integrative review provides four contributions: (1) we analyze the trends associated with the use of MTurk samples in organizational research; (2) we develop a systems perspective (recruitment system, selection system, and work management system) to synthesize and organize the key factors influencing data collected on MTurk that may affect generalizability and data quality; (3) within each factor, we also use available MTurk samples from the organizational literature to analyze key issues (e.g., sample characteristics, use of attention checks, payment); and (4) based on our review, we provide specific recommendations and a checklist for data reporting in order to improve data transparency and enable further research on this issue.",10.3389/fpsyg.2017.01359
miranda a. galvin m. logan daniel w. snook,Assessing the validity of white-collar crime definitions using experimental survey data,2021,"Objectives We examine the extent to which the characteristics of offenders, the circumstance of offending, and offense characteristics affect public willingness to label an offense a “white-collar” crime. Methods We conducted a multidimensional factorial vignette survey hosted onAmazon’s Mechanical Turk. Participants (N = 2696) were randomly assigned to receive information about three of eighteen scenarios that could be considered white-collar crimes. Analyses are conducted at the scenario level with respondent-level fixed effects. Results Scenarios in which offenders had high status were rated more highly on a scale of “white-collarness.” Occupational access was also associated with higher ratings for both middle-status and upper-status offenders. Scenarios in which the means and consequences of the crime were financial were more likely to be considered white-collar crime. Conclusions In order to maximize generalizability and to support evidence-based policies, white-collar crime research should rely on a definition that incorporates practically relevant dimensions of offender status, occupational access, and financial means.",10.1007/s11292-020-09455-6 --> https://link.springer.com/article/10.1007/s11292-020-09455-6
c. campbell s. m. gaddis,'I Don't Agree with Giving Cash': A Survey Experiment Examining Support for Public Assistance,2015,"Existing sociological research on support for anti-poverty programs largely focuses on broad categories of welfare or assistance to the poor rather than particular types of transfers. Using an experimental survey design and mixed methods research, we examine whether support for anti-poverty programs is consistent across different types of anti-poverty programs. We find that programs that offer benefits in-kind are more popular than cash transfers. Food stamps and child care subsidies, in particular, garner greater support than cash welfare while housing assistance falls in a middle ground between food stamps/childcare subsidies and cash welfare. However, when public assistance comes through an increase in personal tax obligation, only food stamps remain more popular than cash welfare. The qualitative findings show that when evaluating anti-poverty programs, respondents adopt one of two perspectives: (1) cash assistance is problematic but other forms of assistance are acceptable or (2) any assistance is problematic. We conclude with a discussion of policy implications and how these findings may inform future sociological research.",10.2139/ssrn.2599513
"jill d. weinberg, j. freese, david mcelhattan,",Comparing data characteristics and results of an online factorial survey between a population-based and a crowdsource-recruited sample,2014,"Compared to older kinds of sample surveys, online platforms provide a fast and low-cost platform for factorial surveys, as well as a more demographically diverse alternative to student samples. Two distinct strategies have emerged for recruitment: using panels based on population-based samples versus recruiting people actively seeking to complete online tasks for money. The latter is much cheaper but prompts various concerns about data quality and generalizability. We compare results of three vignette experiments conducted using the leading online panel that uses a population-based paradigm (Knowledge Networks, now GfK) and the leading platform for crowdsource recruitment (Amazon Mechanical Turk). Our data show that, while demographic differences exist, most notably in age, the actual results of our experiments are very similar, especially once these demographic differences have been taken into account. Indicators of data quality were actually slightly better among the crowdsource subjects. Although more evidence is plainly needed, our results support the accumulating evidence for the promise of crowdsource recruitment for online experiments, including factorial surveys.",data/audit.pdf
"marianne bertrand, s. mullainathan,",Are Emily and Greg More Employable than Lakisha and Jamal? A Field Experiment on Labor Market Discrimination,2003,"We perform a field experiment to measure racial discrimination in the labor market. We respond with fictitious resumes to help-wanted ads in Boston and Chicago newspapers. To manipulate perception of race, each resume is randomly assigned either a very African American sounding name or a very White sounding name. The results show significant discrimination against African-American names: White names receive 50 percent more callbacks for interviews. We also find that race affects the benefits of a better resume. For White names, a higher quality resume elicits 30 percent more callbacks whereas for African Americans, it elicits a far smaller increase. Applicants living in better neighborhoods receive more callbacks but, interestingly, this effect does not differ by race. The amount of discrimination is uniform across occupations and industries. Federal contractors and employers who list Equal Opportunity Employer in their ad discriminate as much as other employers. We find little evidence that our results are driven by employers inferring something other than race, such as social class, from the names. These results suggest that racial discrimination is still a prominent feature of the labor market.",data/audit.pdf
"kevin j. mullinix, thomas j. leeper, j. druckman, j. freese,",The Generalizability of Survey Experiments*,2015,"Abstract Survey experiments have become a central methodology across the social sciences. Researchers can combine experiments’ causal power with the generalizability of population-based samples. Yet, due to the expense of population-based samples, much research relies on convenience samples (e.g. students, online opt-in samples). The emergence of affordable, but non-representative online samples has reinvigorated debates about the external validity of experiments. We conduct two studies of how experimental treatment effects obtained from convenience samples compare to effects produced by population samples. In Study 1, we compare effect estimates from four different types of convenience samples and a population-based sample. In Study 2, we analyze treatment effects obtained from 20 experiments implemented on a population-based sample and Amazon's Mechanical Turk (MTurk). The results reveal considerable similarity between many treatment effects obtained from convenience and nationally representative population-based samples. While the results thus bolster confidence in the utility of convenience samples, we conclude with guidance for the use of a multitude of samples for advancing scientific knowledge.",data/audit.pdf
elizabeth m. briones grant benham,An examination of the equivalency of self-report measures obtained from crowdsourced versus undergraduate student samples,2016,"Increasingly, researchers have begun to explore the potential of the Internet to reach beyond the traditional undergraduate sample. In the present study, we sought to compare the data obtained from a conventional undergraduate college-student sample to data collected via two online survey recruitment platforms. In order to examine whether the data sampled from the three populations were equivalent, we conducted a test of equivalency using inferential confidence intervals-an approach that differs from the more traditional null hypothesis significance testing. The results showed that the data obtained via the two online recruitment platforms, the Amazon Mechanical Turk crowdsourcing site and the virtual environment of Second Life, were statistically equivalent to the data obtained from the college sample, on the basis of means of standardized measures of psychological stress and sleep quality. Additionally, correlations between the sleep and stress measures were not statistically different between the groups. These results, along with practical considerations for the use of these recruitment platforms, are discussed, and recommendations for other researchers who may be considering the use of these platforms are provided.",10.3758/s13428-016-0710-8 --> https://pubmed.ncbi.nlm.nih.gov/26907746/
"s. lieberson, kelly s. mikelson,","DISTINCTIVE AFRICAN AMERICAN NAMES: AN EXPERIMENTAL, HISTORICAL, AND LINGUISTIC ANALYSIS OF INNOVATION*",1995,"Many African American parents create unique names for their children. Although in the United States there are no formal ""rules"" limiting the sounds parents may combine in creating a child's name, innovative names are not simply free-floating imaginative acts; they actually incorporate certain implicit practices found in the culture of both Whites and African Americans. Consequently, on hearing an innovative name, a stranger usually can guess the sex of the child. We are able to infer the linguistic features that influence innovations because these features appear more appropriate or less appropriate, depending on the sex of the child. We interpret our observations in terms of a cultural perspective on innovation which argues that the existing culture operates as an independent force to set bounds on creativity and imagination, independent of the influence of organizations or institutions. We also evaluate an alternative perspective. We analyze innovative naming patterns in the past 75 years and then consider both the influence of African heritage in America and the thrust toward African roots in recent decades. Here too we find a naming mechanism whereby adopted African names are modified by American linguistic conventions.",data/audit.pdf
samantha j kellar erika v. hall,Measuring Racial Discrimination Remotely: A Contemporary Review of Unobtrusive Measures,2022,"Social-science researchers have increasingly moved from conducting their studies in a face-to-face format to an online format. Although new and innovative remote platforms afford researchers generalizability and scale, many of these platforms also tend to solicit socially desirable responses. This pattern of socially desirable responding is evident in examinations of racial discrimination, in which participants are particularly determined to present themselves as ethical and moral. In the current article, we rectify the concern between remote platforms and inauthentic participant responses by reviewing unobtrusive measures of racial discrimination. First, we conceptualize unobtrusive measures as measurements that assess a participant’s discriminatory action without the participant’s knowledge that the specific discriminatory action is under observation. Next, we review the landscape of unobtrusive studies conducted within three broad categories—audit, helping, and friendly interaction—and discuss how these measures have changed over time. Finally, we discuss how to adapt classic face-to-face measures to remote platforms and provide recommendations for implementing unobtrusive measures into remote examinations of discrimination.",10.1177/17456916211045691
a. arechar s. gächter lucas molleman,Conducting interactive experiments online,2017,"Online labor markets provide new opportunities for behavioral research, but conducting economic experiments online raises important methodological challenges. This particularly holds for interactive designs. In this paper, we provide a methodological discussion of the similarities and differences between interactive experiments conducted in the laboratory and online. To this end, we conduct a repeated public goods experiment with and without punishment using samples from the laboratory and the online platform Amazon Mechanical Turk. We chose to replicate this experiment because it is long and logistically complex. It therefore provides a good case study for discussing the methodological and practical challenges of online interactive experimentation. We find that basic behavioral patterns of cooperation and punishment in the laboratory are replicable online. The most important challenge of online interactive experiments is participant dropout. We discuss measures for reducing dropout and show that, for our case study, dropouts are exogenous to the experiment. We conclude that data quality for interactive experiments via the Internet is adequate and reliable, making online interactive experimentation a potentially valuable complement to laboratory studies.",10.2139/ssrn.2884409
i. gleibs,Are all “research fields” equal? Rethinking practice for the use of data from crowdsourcing market places,2016,"New technologies like large-scale social media sites (e.g., Facebook and Twitter) and crowdsourcing services (e.g., Amazon Mechanical Turk, Crowdflower, Clickworker) are impacting social science research and providing many new and interesting avenues for research. The use of these new technologies for research has not been without challenges, and a recently published psychological study on Facebook has led to a widespread discussion of the ethics of conducting large-scale experiments online. Surprisingly little has been said about the ethics of conducting research using commercial crowdsourcing marketplaces. In this article, I focus on the question of which ethical questions are raised by data collection with crowdsourcing tools. I briefly draw on the implications of Internet research more generally, and then focus on the specific challenges that research with crowdsourcing tools faces. I identify fair pay and the related issue of respect for autonomy, as well as problems with the power dynamic between researcher and participant, which has implications for withdrawal without prejudice, as the major ethical challenges of crowdsourced data. Furthermore, I wish to draw attention to how we can develop a “best practice” for researchers using crowdsourcing tools.",10.3758/s13428-016-0789-y --> https://link.springer.com/article/10.3758/s13428-016-0789-y
jacob young k. young,Don’t Get Lost in the Crowd: Best Practices for Using Amazon’s Mechanical Turk in Behavioral Research,2019,"The use of Amazon’s Mechanical Turk (MTurk) to conduct academic research has steadily grown since its inception in 2005. The ability to control every aspect of a study, from sampling to collection, is extremely appealing to researchers. Unfortunately, the additional control offered through MTurk can also lead to poor data quality if researchers are not careful. Despite research on various aspects of data quality, participant compensation, and participant demographics, the academic literature still lacks a practical guide to the effective use of settings and features in MTurk for survey and experimental research. Therefore, the purpose of this tutorial is to provide researchers with a recommended set of best practices to follow before, during, and after collecting data via MTurk to ensure that responses are of the highest possible quality. We also recommend that editors and reviewers place more emphasis on the collection methods employed by researchers, rather than assume that all samples collected using a given online platform are of equal quality. We also recommend that editors and reviewers place more emphasis on the collection methods employed by researchers, rather than assuming that all samples collected using a given online platform are of equal quality.",10.17705/3jmwa.000050
s. m. gaddis,Signaling Class: An Experiment Examining Social Class Perceptions from Names Used in Correspondence Audit Studies,2019,"Field and survey experiments examining racial discrimination and inequality commonly use names to signal race and ethnicity. However, little work has been done to understand how individuals interpret these signals. Despite strong concerns that racialized names simultaneously signal social class, no work has scientifically examined the social class signaling power of names used in previous research. In this article, I conduct a survey experiment to test individual perceptions of social class from names. Respondents are presented with a series of first and last names and asked to state the social class category they associate with each name. In total, 7,936 respondents provide their social class perceptions on 600 different combinations of first and last names. I find that black and Hispanic names are much more likely to be perceived as lower or working class than white names, which are overwhelmingly perceived as middle or upper class. These perceptions are independent of the effect of population-based socioeconomic naming patterns. Overall, this research suggests that scholars must not assume that population patterns in naming are indicative of individual perceptions of names. Instead, scholars should pretest names ethnicity before conducting a field or survey experiment to increase internal validity.",10.2139/ssrn.3350739
s. m. gaddis,Assessing Immigrant Generational Status from Names: Evidence for Experiments Examining Racial/Ethnic and Immigrant Discrimination,2019,"Evidence of racial and ethnic discrimination stems mostly from experiments in the U.S., Europe, and elsewhere that use names to signal race/ethnicity. Although recent work has examined individual racial perceptions of names in the U.S., no research has examined how names might convey immigrant generational status – an important signal for discrimination experiments across the world. I conduct a survey experiment that presents respondents with a series of first and last names to examine perceptions of immigrant generational status in the U.S. In total, 1,659 respondents provide information on 56 different names. I find that when presented with both traditional first and last Hispanic, Indian, or Chinese names, respondents most often believe that person was not born in the U.S. When presented with traditional white or Anglo first names combined with Hispanic or Asian last names, respondents most often believe that person was born in the U.S. but their parents were not. Individual names provide some variation within these results and some groups have stronger results than others. These findings have important implications for discrimination experiments in the U.S. and open the door for future research to distinguish between discrimination based on race/ethnicity and discrimination based on immigration status.",10.2139/ssrn.3022217
"david g. rand,",The promise of Mechanical Turk: how online labor markets can help theorists run behavioral experiments.,2012,"Combining evolutionary models with behavioral experiments can generate powerful insights into the evolution of human behavior. The emergence of online labor markets such as Amazon Mechanical Turk (AMT) allows theorists to conduct behavioral experiments very quickly and cheaply. The process occurs entirely over the computer, and the experience is quite similar to performing a set of computer simulations. Thus AMT opens the world of experimentation to evolutionary theorists. In this paper, I review previous work combining theory and experiments, and I introduce online labor markets as a tool for behavioral experimentation. I review numerous replication studies indicating that AMT data is reliable. I also present two new experiments on the reliability of self-reported demographics. In the first, I use IP address logging to verify AMT subjects' self-reported country of residence, and find that 97% of responses are accurate. In the second, I compare the consistency of a range of demographic variables reported by the same subjects across two different studies, and find between 81% and 98% agreement, depending on the variable. Finally, I discuss limitations of AMT and point out potential pitfalls. I hope this paper will encourage evolutionary modelers to enter the world of experimentation, and help to strengthen the bond between theoretical and empirical analyses of the evolution of human behavior.",data/audit.pdf --> https://pubmed.ncbi.nlm.nih.gov/21402081/
joshua d. miller michael l. crowe brandon m. weiss j. maples-keller d. lynam,"Using Online, Crowdsourcing Platforms for Data Collection in Personality Disorder Research: The Example of Amazon’s Mechanical Turk",2017,"The use of crowdsourcing platforms such as Amazon’s Mechanical Turk (MTurk) for data collection in the behavioral sciences has increased substantially in the past several years due in large part to (a) the ability to recruit large samples, (b) the inexpensiveness of data collection, (c) the speed of data collection, and (d) evidence that the data collected are, for the most part, of equal or better quality to that collected in undergraduate research pools. In this review, we first evaluate the strengths and potential limitations of this approach to data collection. Second, we examine how MTurk has been used to date in personality disorder (PD) research and compare the characteristics of such research to PD research conducted in other settings. Third, we compare PD trait data from the Section III trait model of the DSM–5 collected via MTurk to data collected using undergraduate and clinical samples with regard to internal consistency, mean-level differences, and factor structure. Overall, we conclude that platforms such as MTurk have much to offer PD researchers, especially for certain kinds of research (e.g., where large samples are required and there is a need for iterative sampling). Whether MTurk itself remains the predominant model of such platforms is unclear, however, and will largely depend on decisions related to cost effectiveness and the development of alternatives that offer even greater flexibility.",10.1037/per0000191
"j. horton, david g. rand, r. zeckhauser,",The online laboratory: conducting experiments in a real labor market,2010,"Online labor markets have great potential as platforms for conducting experiments. They provide immediate access to a large and diverse subject pool, and allow researchers to control the experimental context. Online experiments, we show, can be just as valid—both internally and externally—as laboratory and field experiments, while often requiring far less money and time to design and conduct. To demonstrate their value, we use an online labor market to replicate three classic experiments. The first finds quantitative agreement between levels of cooperation in a prisoner’s dilemma played online and in the physical laboratory. The second shows—consistent with behavior in the traditional laboratory—that online subjects respond to priming by altering their choices. The third demonstrates that when an identical decision is framed differently, individuals reverse their choice, thus replicating a famed Tversky-Kahneman result. Then we conduct a field experiment showing that workers have upward-sloping labor supply curves. Finally, we analyze the challenges to online experiments, proposing methods to cope with the unique threats to validity in an online setting, and examining the conceptual issues surrounding the external validity of online results. We conclude by presenting our views on the potential role that online experiments can play within the social sciences, and then recommend software development priorities and best practices.",data/audit.pdf --> https://link.springer.com/article/10.1007/s10683-011-9273-9
travis simcox j. fiez,Collecting response times using Amazon Mechanical Turk and Adobe Flash,2013,"Crowdsourcing systems like Amazon's Mechanical Turk (AMT) allow data to be collected from a large sample of people in a short amount of time. This use has garnered considerable interest from behavioral scientists. So far, most experiments conducted on AMT have focused on survey-type instruments because of difficulties inherent in running many experimental paradigms over the Internet. This study investigated the viability of presenting stimuli and collecting response times using Adobe Flash to run ActionScript 3 code in conjunction with AMT. First, the timing properties of Adobe Flash were investigated using a phototransistor and two desktop computers running under several conditions mimicking those that may be present in research using AMT. This experiment revealed some strengths and weaknesses of the timing capabilities of this method. Next, a flanker task and a lexical decision task implemented in Adobe Flash were administered to participants recruited with AMT. The expected effects in these tasks were replicated. Power analyses were conducted to describe the number of participants needed to replicate these effects. A questionnaire was used to investigate previously undescribed computer use habits of 100 participants on AMT. We conclude that a Flash program in conjunction with AMT can be successfully used for running many experimental paradigms that rely on response times, although experimenters must understand the limitations of the method.",10.3758/s13428-013-0345-y --> https://pubmed.ncbi.nlm.nih.gov/23670340/
zachary c merz j. lace a. eisenstein,Examining broad intellectual abilities obtained within an mTurk internet sample,2020,"Widely used in social science research, samples of participants obtained via Amazon’s Mechanical Turk (mTurk) tend to be representative across many sociodemographic variables. However, to date, no research has investigated and reported the global cognitive ability level (i.e., intelligence) of samples obtained via mTurk. The present study contributes to the literature by investigating a previously well-validated, public domain measure of cognitive ability in a sample of American adults recruited via mTurk. As part of a larger cross-sectional, survey-based study, four hundred thirty-four (434) Americans (M age = 37.86; 35.7% men) completed a demographic questionnaire and the 16-item International Cognitive Ability Resource, Sample Test (ICAR-16). Results revealed a normal distribution of ICAR-16 scores across the current sample. Additionally, total scores were positively correlated with participants’ level of education, income, and self-estimated intelligence, but did not significantly correlate with participant age. No gender differences were identified on ICAR-16 total scores. Finally, ICAR-16 scores did not significantly differ from normative data derived from its validation study. These results suggested that American mTurk samples may be representative of the broader population in terms of global cognitive ability, and that the ICAR-16 is likely a reasonable, psychometrically sound, and inexpensive measure of global cognitive ability appropriate for use in mTurk samples.",10.1007/s12144-020-00741-0 --> https://link.springer.com/article/10.1007/s12144-020-00741-0
"winter a. mason, siddharth suri,",Conducting behavioral research on Amazon’s Mechanical Turk,2011,"Amazon's Mechanical Turk is an online labor market where requesters post jobs and workers choose which jobs to do for pay. The central purpose of this article is to demonstrate how to use this Web site for conducting behavioral research and to lower the barrier to entry for researchers who could benefit from this platform. We describe general techniques that apply to a variety of types of research and experiments across disciplines. We begin by discussing some of the advantages of doing experiments on Mechanical Turk, such as easy access to a large, stable, and diverse subject pool, the low cost of doing experiments, and faster iteration between developing theory and executing experiments. While other methods of conducting behavioral research may be comparable to or even better than Mechanical Turk on one or more of the axes outlined above, we will show that when taken as a whole Mechanical Turk can be a useful tool for many researchers. We will discuss how the behavior of workers compares with that of experts and laboratory subjects. Then we will illustrate the mechanics of putting a task on Mechanical Turk, including recruiting subjects, executing the task, and reviewing the work that was submitted. We also provide solutions to common problems that a researcher might face when executing their research on this platform, including techniques for conducting synchronous experiments, methods for ensuring high-quality work, how to keep data private, and how to maintain code security.",data/audit.pdf --> https://pubmed.ncbi.nlm.nih.gov/21717266/
fabiana silva,The Strength of Whites’ Ties: How Employers Reward the Referrals of Black and White Jobseekers,2018,"Abstract:Sociologists commonly point to jobseekers’ racially segregated networks and employers’ discriminatory behavior to explain racial inequality in employment. Network scholars argue that, given segregated networks and black and white employees’ unequal position in the labor market, employers’ reliance on employee referrals reproduces black disadvantage. Scholars of discrimination focus instead on employers’ unequal treatment of equally qualified black and white jobseekers. Drawing on an original experiment with a sample of white individuals with hiring responsibilities, I seek to bridge these literatures by examining whether respondents’ racial prejudice affects how they reward employee referrals of black and white applicants from black and white employees. I use a measure of implicit prejudice that is resistant to social desirability and that can capture biases among people who genuinely believe they are unbiased. Whether evaluated by low-prejudiced or high-prejudiced respondents, white applicants benefit greatly from same-race referrals. In contrast, black applicants do not benefit from same-race referrals, even when they are evaluated by low-prejudiced respondents. In fact, black applicants only benefit from having a referral when two conditions are met: the referring employee is white and they are evaluated by a relatively low-prejudiced respondent. These findings suggest that in addition to their disadvantage in access to employee referrals, black jobseekers suffer from a disadvantage in returns to these referrals.",10.1093/SF/SOY051
"s. m. gaddis,",Discrimination in the Credential Society: An Audit Study of Race and College Selectivity in the Labor Market,2014,"Racial inequality in economic outcomes, particularly among the college educated, persists throughout US society. Scholars debate whether this inequality stems from racial differences in human capital (e.g., college selectivity, GPA, college major) or employer discrimination against black job candidates. However, limited measures of human capital and the inherent difficulties in measuring discrimination using observational data make determining the cause of racial differences in labor-market outcomes a difficult endeavor. In this research, I examine employment opportunities for white and black graduates of elite top-ranked universities versus high-ranked but less selective institutions. Using an audit design, I create matched candidate pairs and apply for 1,008 jobs on a national job-search website. I also exploit existing birth-record data in selecting names to control for differences across social class within racialized names. The results show that although a credential from an elite university results in more employer responses for all candidates, black candidates from elite universities only do as well as white candidates from less selective universities. Moreover, race results in a double penalty: When employers respond to black candidates, it is for jobs with lower starting salaries and lower prestige than those of white peers. These racial differences suggest that a bachelor’s degree, even one from an elite institution, cannot fully counteract the importance of race in the labor market. Thus, both discrimination and differences in human capital contribute to racial economic inequality.",data/audit.pdf
s. m. gaddis r. ghoshal,Dynamic Racial Triangulation: Examining the Racial Order Using an Experiment on Discrimination by White Millennials,2020,"Scholars argue that the traditional binary racial order model of the U.S. is outdated and acknowledge that racial systems can shift in response to demographic, political, and economic changes. In the coming years, White Millennials will exert ever-greater political and economic power in shaping the racial order. However, we have limited empirical evidence on the racial attitudes and race-based actions of this group. Our overarching goals in this paper are to uncover the racial order currently taking shape among White Millennials and to understand links between racial valorization, cultural inclusion/exclusion, and discriminatory actions among this group. To do so, we simulate a previous housing correspondence audit and conduct an online survey experiment with 4,462 White Millennials gauging respondents’ actions toward and views of White, Black, Hispanic, Indian, and Chinese-origin individuals. We find that White Millennial respondents express less acceptance of Blacks than members of other groups. Second, Blacks and Latinx are penalized on dimensions of valorization (i.e. responsibility, courteousness, and financial stability). Third, all groups other than Whites and “Americanized” Latinx are penalized on cultural inclusion, though the extent varies by signals of assimilation. Our findings indicate some support for racial triangulation theory but also show that insights from this perspective should be synthesized with those of racial formation theory. We suggest that rather than conform to any existing theory, White Millennials enact an emerging racial order rooted in what we call dynamic racial triangulation.",10.2139/ssrn.3022208
janelle h. cheung d. burns r. sinclair michael t. sliter,Amazon Mechanical Turk in Organizational Psychology: An Evaluation and Practical Recommendations,2016,"Purpose Amazon Mechanical Turk is an increasingly popular data source in the organizational psychology research community. This paper presents an evaluation of MTurk and provides a set of practical recommendations for researchers using MTurk. Design/Methodology/Approach We present an evaluation of methodological concerns related to the use of MTurk and potential threats to validity inferences. Based on our evaluation, we also provide a set of recommendations to strengthen validity inferences using MTurk samples. Findings Although MTurk samples can overcome some important validity concerns, there are other limitations researchers must consider in light of their research objectives. Researchers should carefully evaluate the appropriateness and quality of MTurk samples based on the different issues we discuss in our evaluation. Implications There is not a one-size-fits-all answer to whether MTurk is appropriate for a research study. The answer depends on the research questions and the data collection and analytic procedures adopted. The quality of the data is not defined by the data source per se, but rather the decisions researchers make during the stages of study design, data collection, and data analysis. Originality/Value The current paper extends the literature by evaluating MTurk in a more comprehensive manner than in prior reviews. Past review papers focused primarily on internal and external validity, with less attention paid to statistical conclusion and construct validity—which are equally important in making accurate inferences about research findings. This paper also provides a set of practical recommendations in addressing validity concerns when using MTurk.",10.1007/s10869-016-9458-5 --> https://link.springer.com/article/10.1007/s10869-016-9458-5
justin t. pickett s. p. roche,"Questionable, Objectionable or Criminal? Public Opinion on Data Fraud and Selective Reporting in Science",2017,"Data fraud and selective reporting both present serious threats to the credibility of science. However, there remains considerable disagreement among scientists about how best to sanction data fraud, and about the ethicality of selective reporting. The public is arguably the largest stakeholder in the reproducibility of science; research is primarily paid for with public funds, and flawed science threatens the public's welfare. Members of the public are able to make meaningful judgments about the morality of different behaviors using moral intuitions. Legal scholars emphasize that to maintain legitimacy, social control policies must be developed with some consideration given to the public's moral intuitions. Although there is a large literature on popular attitudes toward science, there is no existing evidence about public opinion on data fraud or selective reporting. We conducted two studies-a survey experiment with a nationwide convenience sample (N = 821), and a follow-up survey with a representative sample of US adults (N = 964)-to explore community members' judgments about the morality of data fraud and selective reporting in science. The findings show that community members make a moral distinction between data fraud and selective reporting, but overwhelmingly judge both behaviors to be immoral and deserving of punishment. Community members believe that scientists who commit data fraud or selective reporting should be fired and banned from receiving funding. For data fraud, most Americans support criminal penalties. Results from an ordered logistic regression analysis reveal few demographic and no significant partisan differences in punitiveness toward data fraud.",10.1007/s11948-017-9886-2 --> https://pubmed.ncbi.nlm.nih.gov/28281156/
"d. crane, s. lieberson,","A Matter of Taste: How Names, Fashions, and Culture Change",2000,"What accounts for our tastes? Why and how do they change over time? In this innovative book Stanley Lieberson analyzes children's first names to develop an original theory of fashion. Children's names provide an opportunity to view the pure mechanisms of fashion, unaffected by commercial interests that influence many fashions and tastes, says Lieberson. He disputes the commonly held notion that tastes in names (and other fashions) simply reflect societal shifts. There exist also ""internal taste mechanisms"" that drive changes in fashion even in the absence of social change, Lieberson contends. He explores the intricate and subtle ways in which internal mechanisms operate in concert with social forces to determine our choices of names. And he applies these conclusions to classical music, the decline of the fedora, women's garments, and other examples of change in fashion. Examining extensive data on names over long periods of time, Lieberson discovers an orderly regularity to the process of change. He considers an array of naming practices-how Rebecca became a popular name, why the names of certain important and attractive biblical characters are rarely chosen, and the influence of movie stars and characters in movies and novels. The book also inquires into name selection by specific ethnic and racial groups-Mexicans' choices of names for their sons and daughters, African-American naming tastes from the time of slavery, changing names among American Jews throughout the twentieth century, and ethnic influences on naming in assimilated white groups. Lieberson concludes with a discussion of broader applications of internal mechanisms, suggesting that they operate widely in culture, across the entire ""cultural surface.""",data/audit.pdf
"d. o. sears,",College sophomores in the laboratory: Influences of a narrow data base on social psychology's view of human nature.,1986,"For the 2 decades prior to 1960, published research in social psychology was based on a wide variety of subjects and research sites. Content analyses show that since then such research has overwhelmingly been based on college students tested in academic laboratories on academiclike tasks. How might this heavy dependence on one narrow data base have biased the main substantive conclusions of sociopsychological research in this era? Research on the full life span suggests that, compared with older adults, college students are likely to have less-crystallized attitudes, less-formulated senses of self, stronger cognitive skills, stronger tendencies to comply with authority, and more unstable peer group relationships. The laboratory setting is likely to exaggerate all these differences. These peculiarities of social psychology's predominant data base may have contributed to central elements of its portrait of human nature. According to this view people (a) are quite compliant and their behavior is easily socially influenced, (b) readily change their attitudes and (c) behave inconsistently with them, and (d) do not rest their self-perceptions on introspection. The narrow data base may also contribute to this portrait of human nature's (e) strong emphasis on cognitive processes and to its lack of emphasis on (f) personality dispositions, (g) material self-interest, (h) emotionally based irrationalities, (i) group norms, and (j) stage-specific phenomena. The analysis implies the need both for more careful examination of sociopsychological propositions for systematic biases introduced by dependence on this narrow data base and for increased reliance on adults tested in their natural habitats with materials drawn from ordinary life.",data/audit.pdf
joseph k. goodman cynthia cryder amar cheema,Data collection in a flat world: the strengths and weaknesses of mechanical turk samples,2013,"Mechanical Turk (MTurk), an online labor system run by Amazon.com, provides quick, easy, and inexpensive access to online research participants. As use of MTurk has grown, so have questions from behavioral researchers about its participants, reliability, and low compensation. In this article, we review recent research about MTurk and compare MTurk participants with community and student samples on a set of personality dimensions and classic decision-making biases. Across two studies, we find many similarities between MTurk participants and traditional samples, but we also find important differences. For instance, MTurk participants are less likely to pay attention to experimental materials, reducing statistical power. They are more likely to use the Internet to find answers, even with no incentive for correct responses. MTurk participants have attitudes about money that are different from a community sample’s attitudes but similar to students’ attitudes. Finally, MTurk participants are less extraverted and have lower self-esteem than other participants, presenting challenges for some research domains. Despite these differences, MTurk participants produce reliable results consistent with standard decision-making biases: they are present biased, risk-averse for gains, risk-seeking for losses, show delay/expedite asymmetries, and show the certainty effect—with almost no significant differences in effect sizes from other samples. We conclude that MTurk offers a highly valuable opportunity for data collection and recommend that researchers using MTurk (1) include screening questions that gauge attention and language comprehension; (2) avoid questions with factual answers; and (3) consider how individual differences in financial and social domains may influence results. Copyright © 2012 John Wiley & Sons, Ltd.",10.1002/BDM.1753
j. strickland,EXAMINING THE UTILITY OF BEHAVIORAL ECONOMIC DEMAND IN ADDICTION SCIENCE,2019,"OF DISSERTATION EXAMINING THE UTILITY OF BEHAVIORAL ECONOMIC DEMAND IN ADDICTION SCIENCE The marriage of perspectives from behavioral economic theory and learning theory has the potential to advance an understanding of substance use and substance use disorder. Behavioral economic demand is a central concept to this interdisciplinary approach. Evaluating demand in the laboratory and clinic can improve previous research on the relative reinforcing effects of drugs by accounting for the multi-dimensional nature of reinforcement rather than viewing reinforcement as a unitary construct. Recent advances in the commodity purchase task methodology have further simplified the measurement of demand values in human participants. This dissertation project presents a programmatic series of studies designed to demonstrate the utility of using a behavioral economic demand framework and the purchase task methodology for understanding substance use disorder through basic and applied science research. Experiments are presented spanning a continuum from theoretical and methodological development to longitudinal work and clinical application. These experiments demonstrate three key conclusions regarding behavioral economic demand. First, behavioral economic demand provides a reliable and valid measure of drug valuation that is applicable to varied drug types and participant populations. Second, behavioral economic demand is a stimulus-selective measure specifically reflecting valuation for the commodity under study. Third, behavioral economic demand provides incremental information about substance use in the laboratory and clinical setting above and beyond traditional measures of reinforcer valuation and other behavioral economic variables. These findings collectively highlight the benefits of behavioral economic demand and provide an important platform for future work in addiction science.",10.13023/ETD.2019.030
"b. hogan, b. berry,",Racial and Ethnic Biases in Rental Housing: An Audit Study of Online Apartment Listings,2011,"As rental markets move online, techniques to assess racial/ethnic rental housing discrimination should keep pace. We demonstrate an audit method for assessing discrimination in Toronto's online rental market. As a multicultural city with less segregation and more diverse visible minorities than most US cities, Toronto lends itself to multiname audit studies. We sent 5,620 fictitious email inquiries to landlords offering apartments on Craigslist, a popular Internet classifieds service. Each landlord received one inquiry each from five racialized groups—Caucasian, Black, E/SE Asian, Muslim/Arabic, and Jewish. In our experiments, “opportunity denying” discrimination (exclusion through nonresponse) was 10 times as common as “opportunity diminishing” discrimination (e.g., additional rental conditions). We estimate Muslim/Arabic–racialized men face the greatest resistance, with discrimination occurring in 12 percent of experiments. The level of discrimination is modest but significant for Asian men (7 percent), Blacks (5 percent), and Muslim/Arabic women (5 percent). Discrimination was evenly spread throughout the city. Sesgos raciales y étnicos en el alquiler de viviendas: Una auditoría de los anuncios de apartamentos en línea (Bernie Hogan y Brent Berry) Resumen A medida que los mercados de alquiler tienen más presencia en Internet, es preciso actualizar las técnicas para identificar la discriminación racial y étnica en dichos mercados. En este artículo demostramos cómo utilizar un método de auditoría para evaluar el nivel de discriminación en el mercado de alquiler de Toronto en el Internet. En tanto ciudad multicultural con un menor nivel de segregación y minorías más visibles que la mayoría de las ciudades estadounidenses, Toronto constituye un buen ejemplo para llevar a cabo estudios tipo auditoría usando diferentes nombres. Enviamos 5,620 mensajes electrónicos ficticios solicitando información a caseros con anuncios en Craigslist, un popular servicio de clasificados por Internet. Cada casero recibió un email de uno de cinco grupos raciales–personas de raza blanca, negra, asiáticas, musulmanes/árabes y judías. En nuestros experimentos, la discriminación tipo “negación de oportunidades” (exclusión vía la ausencia de respuestas) resultó ser 10 veces más común que la discriminación tipo “reducción de oportunidades” (por ejemplo, agregando condiciones adicionales). Según nuestros estimados, los hombres de origen musulmán/árabe son los que encuentran mayor resistencia dado que se dieron instancias de discriminación en un 12% de los experimentos usando nombres árabes masculinos. Los niveles de discriminación son más modestos pero significativos con respecto a los hombres asiáticos (7 por ciento), las personas de raza negra (5 por ciento) y las mujeres musulmanas/árabes (5 por ciento). Los patrones de discriminación estuvieron distribuidos de manera equitativa en toda la ciudad.",data/audit.pdf
